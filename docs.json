[
  {
    "content": "* * *\n\nIntroduction to Modal\n=====================\n\nModal lets you run code in the cloud without having to think about infrastructure.\n\nFeatures\n--------\n\n*   Run any code remotely within seconds.\n*   Define container environments in code (or use one of our pre-built backends).\n*   Scale up horizontally to thousands of containers.\n*   Deploy and monitor persistent cron jobs.\n*   Attach GPUs with a single line of code.\n*   Serve your functions as web endpoints.\n*   Use powerful primitives like distributed dictionaries and queues.\n\nGetting started\n---------------\n\nThe nicest thing about all of this is that **you don’t have to set up any infrastructure.** Just:\n\n1.  Create an account at [modal.com](https://modal.com)\n    \n2.  Install the `modal` Python package\n3.  Set up a token\n\n…and you can start running jobs right away.\n\nModal is currently Python-only, but we may support other languages in the future.\n\nHow does it work?\n-----------------\n\nModal takes your code, puts it in a container, and executes it in the cloud.\n\nWhere does it run? Modal runs it in its own cloud environment. The benefit is that we solve all the hard infrastructure problems for you, so you don’t have to do anything. You don’t need to mess with Kubernetes, Docker or even an AWS account.\n\n[Introduction to Modal](#introduction-to-modal)\n [Features](#features)\n [Getting started](#getting-started)\n [How does it work?](#how-does-it-work)\n\nSee it in action\n\n[Hello, world!](/docs/examples/hello_world)\n\n[A simple web scraper](/docs/examples/web-scraper)",
    "markdown": "* * *\n\nIntroduction to Modal\n=====================\n\nModal lets you run code in the cloud without having to think about infrastructure.\n\nFeatures\n--------\n\n*   Run any code remotely within seconds.\n*   Define container environments in code (or use one of our pre-built backends).\n*   Scale up horizontally to thousands of containers.\n*   Deploy and monitor persistent cron jobs.\n*   Attach GPUs with a single line of code.\n*   Serve your functions as web endpoints.\n*   Use powerful primitives like distributed dictionaries and queues.\n\nGetting started\n---------------\n\nThe nicest thing about all of this is that **you don’t have to set up any infrastructure.** Just:\n\n1.  Create an account at [modal.com](https://modal.com)\n    \n2.  Install the `modal` Python package\n3.  Set up a token\n\n…and you can start running jobs right away.\n\nModal is currently Python-only, but we may support other languages in the future.\n\nHow does it work?\n-----------------\n\nModal takes your code, puts it in a container, and executes it in the cloud.\n\nWhere does it run? Modal runs it in its own cloud environment. The benefit is that we solve all the hard infrastructure problems for you, so you don’t have to do anything. You don’t need to mess with Kubernetes, Docker or even an AWS account.\n\n[Introduction to Modal](#introduction-to-modal)\n [Features](#features)\n [Getting started](#getting-started)\n [How does it work?](#how-does-it-work)\n\nSee it in action\n\n[Hello, world!](/docs/examples/hello_world)\n\n[A simple web scraper](/docs/examples/web-scraper)",
    "metadata": {
      "title": "Introduction to Modal | Modal Docs",
      "description": "Modal lets you run code in the cloud without having to think about infrastructure.",
      "ogTitle": "Introduction to Modal",
      "ogDescription": "Modal lets you run code in the cloud without having to think about infrastructure.",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nCustom containers\n=================\n\nThis guide walks you through how to define the environment your Modal functions and applications run within.\n\nThese environments are called _containers_. Containers are like light-weight virtual machines — container engines use [operating system tricks](https://earthly.dev/blog/chroot/)\n to isolate programs from each other (“containing” them), making them work as though they were running on their own hardware with their own filesystem. This makes execution environments more reproducible, for example by preventing accidental cross-contamination of environments on the same machine. For added security, Modal runs containers using the sandboxed [gVisor container runtime](https://cloud.google.com/blog/products/identity-security/open-sourcing-gvisor-a-sandboxed-container-runtime)\n.\n\nContainers are started up from a stored “snapshot” of their filesystem state called an _image_. Producing the image for a container is called _building_ the image.\n\nBy default, Modal functions are executed in a [Debian Linux](https://en.wikipedia.org/wiki/Debian)\n container with a basic Python installation of the same minor version `v3.x` as your local Python interpreter.\n\nCustomizing this environment is critical. To make your apps and functions useful, you will probably need some third party system packages or Python libraries. To make them start up faster, you can bake data like model weights into the container image, taking advantage of Modal’s optimized filesystem for serving containers.\n\nModal provides a number of options to customize your container images at different levels of abstraction and granularity, from high-level convenience methods like `pip_install` through wrappers of core container image build features like `RUN` and `ENV` to full on “bring-your-own-Dockerfile”. We’ll cover each of these in this guide, along with tips and tricks for building images effectively when using each tool.\n\nThe typical flow for defining an image in Modal is [method chaining](https://jugad2.blogspot.com/2016/02/examples-of-method-chaining-in-python.html)\n starting from a base image, like this:\n\n    from modal import Image\n    \n    image = (\n        Image.debian_slim(python_version=\"3.10\")\n        .apt_install(\"git\")\n        .pip_install(\"torch==2.2.1\")\n        .env({\"HALT_AND_CATCH_FIRE\": 0})\n        .run_commands(\"git clone https://github.com/modal-labs/agi && echo 'ready to go!'\")\n    )\n\nCopy\n\nIn addition to being Pythonic and clean, this also matches the onion-like [layerwise build process](https://docs.docker.com/build/guide/layers/)\n of container images.\n\nAdd Python packages with `pip_install`\n--------------------------------------\n\nThe simplest and most common container modification is to add some third party Python package, like [`pandas`](https://pandas.pydata.org/)\n.\n\nYou can add Python packages to the environment by passing all the packages you need to the [`pip_install`](/docs/reference/modal.Image/#pip_install)\n method of an image.\n\nYou can include [typical Python dependency version specifiers](https://peps.python.org/pep-0508/)\n, like `\"torch <= 2.0\"`, in the arguments. But we recommend pinning dependencies tightly, like `\"torch == 1.9.1\"`, to improve the reproducibility and robustness of your builds.\n\nOf course, that means you need to start from some image. Below, we use the recommended [`debian_slim`](/docs/reference/modal.Image#debian_slim)\n image as our base.\n\n    from modal import Image\n    \n    datascience_image = (\n        Image.debian_slim(python_version=\"3.10\")\n        .pip_install(\"pandas==2.2.0\", \"numpy\")\n    )\n    \n    \n    @app.function(image=datascience_image)\n    def my_function():\n        import pandas as pd\n        import numpy as np\n    \n        df = pd.DataFrame()\n        ...\n\nCopy\n\nNote that because you can define a different environment for each and every Modal function if you so choose, you don’t need to worry about virtual environment management. Containers make for much better separation of concerns!\n\nIf you want to run a specific version of Python remotely rather than just matching the one you’re running locally, provide the `python_version` as a string when constructing the base image, like we did above.\n\n### What if I have different Python packages locally and remotely?\n\nYou might want to use packages inside your Modal code that you don’t have on your local computer. In the example above, we build a container that uses `pandas`. But if we don’t have `pandas` locally, on the computer launching the Modal job, we can’t put `import pandas` at the top of the script, since it would cause an `ImportError`.\n\nThe easiest solution to this is to put `import pandas` in the function body instead, as you can see above. This means that `pandas` is only imported when running inside the remote Modal container, which has `pandas` installed.\n\nBe careful about what you return from Modal functions that have different packages installed than the ones you have locally! Modal functions return Python objects, like `pandas.DataFrame`s, and if your local machine doesn’t have `pandas` installed, it won’t be able to handle a `pandas` object (the error message you see will mention [serialization](https://hazelcast.com/glossary/serialization/)\n/[deserialization](https://hazelcast.com/glossary/deserialization/)\n).\n\nIf you have a lot of functions and a lot of Python packages, you might want to keep the imports in the global scope so that every function can use the same imports. In that case, you can use the [`imports()`](/docs/reference/modal.Image#imports)\n context manager:\n\n    from modal import Image\n    \n    pandas_image = Image.debian_slim().pip_install(\"pandas\", \"numpy\")\n    \n    \n    with pandas_image.imports():\n        import pandas as pd\n        import numpy as np\n    \n    \n    @app.function(image=pandas_image)\n    def my_function():\n        df = pd.DataFrame()\n\nCopy\n\nNote that this feature is still in beta.\n\nRun shell commands with `.run_commands`\n---------------------------------------\n\nYou can also supply shell commands that should be executed when building the container image.\n\nYou might use this to preload custom assets, like model parameters, so that they don’t need to be retrieved when functions start up:\n\n    from modal import Image\n    \n    image_with_model = (\n        Image.debian_slim().apt_install(\"curl\").run_commands(\n            \"curl -O https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalcatface.xml\",\n        )\n    )\n    \n    \n    @app.function(image=image_with_model)\n    def find_cats():\n        content = open(\"/haarcascade_frontalcatface.xml\").read()\n        ...\n\nCopy\n\nYou can also use this command to install Python packages. For example, some libraries require a complicated `pip` invocation that is not supported by `.pip_install`:\n\n    from modal import Image\n    \n    image = (\n        modal.Image.from_registry(\"pytorch/pytorch:2.3.1-cuda12.1-cudnn8-devel\", add_python=\"3.11\")\n        .apt_install(\"git\")\n        .run_commands(\"pip install flash-attn --no-build-isolation\")\n    )\n\nCopy\n\nOr you can install packages with [`uv`](https://github.com/astral-sh/uv)\n, which can be substantially faster than `pip`:\n\n    from modal import Image\n    \n    image = (\n        Image.debian_slim()\n        .pip_install(\"uv\")\n        .run_commands(\"uv pip install --system --compile-bytecode torch\")\n    )\n\nCopy\n\nNote that it is important to pass `--compile-bytecode` when using `uv`; its default behavior differs from that of `pip`, but it is important to compile the bytecode when you build the image so that it doesn’t happen on every container cold start.\n\nRun a Modal function during your build with `.run_function` (beta)\n------------------------------------------------------------------\n\nInstead of using shell commands, you can also run a Python function as an image build step using the [`Image.run_function`](/docs/reference/modal.Image#run_function)\n method. For example, you can use this to download model parameters from Hugging Face into your image, massively speeding up function starts:\n\n    from modal import Image, Secret\n    \n    def download_models():\n        import diffusers\n    \n        pipe = diffusers.StableDiffusionPipeline.from_pretrained(\n            model_id, use_auth_token=os.environ[\"HF_TOKEN\"]\n        )\n        pipe.save_pretrained(\"/model\")\n    \n    \n    image = (\n        Image.debian_slim()\n            .pip_install(\"diffusers[torch]\", \"transformers\", \"ftfy\", \"accelerate\")\n            .run_function(download_models, secrets=[Secret.from_name(\"huggingface-secret\")])\n    )\n\nCopy\n\nAny kwargs accepted by [`@app.function`](/docs/reference/modal.App#function)\n (such as [`Mount`s](/docs/guide/local-data#mounting-directories)\n, [`NetworkFileSystem`s](/docs/guide/network-file-systems)\n, and specifications of resources like [GPUs](/docs/guide/gpu)\n) can be supplied here.\n\nEssentially, this is equivalent to running a Modal function and snapshotting the resulting filesystem as an image.\n\nWhenever you change other features of your image, like the base image or the version of a Python package, the image will automatically be rebuilt the next time it is used. This is a bit more complicated when changing the contents of Modal functions. See the [reference documentation](/docs/reference/modal.Image#run_function)\n for details.\n\nAttach GPUs during setup\n------------------------\n\nIf a step in the setup of your container image should be run on an instance with a GPU (e.g., so that a package can be linked against CUDA libraries), pass a desired GPU type when defining that step:\n\n    from modal import Image\n    \n    image = (\n        Image.debian_slim()\n        .pip_install(\"bitsandbytes\", gpu=\"H100\")\n    )\n\nCopy\n\nUse `mamba` instead of `pip` with `micromamba_install`\n------------------------------------------------------\n\n`pip` installs Python packages, but some Python workloads require the coordinated installation of system packages as well. The `mamba` package manager can install both. Modal provides a pre-built [Micromamba](https://mamba.readthedocs.io/en/latest/user_guide/micromamba.html)\n base image that makes it easy to work with `micromamba`:\n\n    from modal import Image, App\n    \n    app = App(\"bayes-pgm\")\n    \n    numpyro_pymc_image = (\n        Image.micromamba()\n        .micromamba_install(\"pymc==5.10.4\", \"numpyro==0.13.2\", channels=[\"conda-forge\"])\n    )\n    \n    \n    @app.function(image=numpyro_pymc_image)\n    def sample():\n        import pymc as pm\n        import numpyro as np\n    \n        print(f\"Running on PyMC v{pm.__version__} with JAX/numpyro v{np.__version__} backend\")\n        ...\n\nCopy\n\nUse an existing container image with `.from_registry`\n-----------------------------------------------------\n\nYou don’t always need to start from scratch! Public registries like [Docker Hub](https://hub.docker.com/)\n have many pre-built container images for common software packages.\n\nYou can use any public image in your function using [`Image.from_registry`](/docs/reference/modal.Image#from_registry)\n, so long as:\n\n*   Python 3.8 or above is present, and is available as `python`\n*   `pip` is installed correctly\n*   The image is built for the [`linux/amd64` platform](https://unix.stackexchange.com/questions/53415/why-are-64-bit-distros-often-called-amd64)\n    \n*   The image has a [valid `ENTRYPOINT`](#entrypoint)\n    \n\n    from modal import Image\n    \n    sklearn_image = Image.from_registry(\"huanjason/scikit-learn\")\n    \n    \n    @app.function(image=sklearn_image)\n    def fit_knn():\n        from sklearn.neighbors import KNeighborsClassifier\n        ...\n\nCopy\n\nIf an existing image does not have either `python` or `pip` set up properly, you can still use it. Just provide a version number as the `add_python` argument to install a reproducible, [standalone build](https://github.com/indygreg/python-build-standalone)\n of Python:\n\n    from modal import Image\n    \n    image1 = Image.from_registry(\"ubuntu:22.04\", add_python=\"3.11\")\n    image2 = Image.from_registry(\"gisops/valhalla:latest\", add_python=\"3.11\")\n\nCopy\n\nThe `from_registry` method can load images from all public registries, such as [Nvidia’s `nvcr.io`](https://catalog.ngc.nvidia.com/containers)\n, [AWS ECR](https://aws.amazon.com/ecr/)\n, and [GitHub’s `ghcr.io`](https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry)\n.\n\nWe also support access to [private AWS ECR and GCP Artifact Registry images](/docs/guide/private-registries)\n.\n\nBring your own image definition with `.from_dockerfile`\n-------------------------------------------------------\n\nSometimes, you might be working in a setting where the environment is already defined as a container image in the form of a `Dockerfile`.\n\nModal supports defining a container image directly from a Dockerfile via the [`Image.from_dockerfile`](/docs/reference/modal.Image#from_dockerfile)\n function. It takes a path to an existing Dockerfile.\n\nFor instance, we might write a Dockerfile based on the official Python image and adding scikit-learn:\n\n    FROM python:3.9\n    RUN pip install sklearn\n\nCopy\n\nand then define an image for Modal based on it:\n\n    from modal import Image\n    \n    dockerfile_image = Image.from_dockerfile(\"Dockerfile\")\n    \n    \n    @app.function(image=dockerfile_image)\n    def fit():\n        import sklearn\n        ...\n\nCopy\n\nNote that you can still do method chaining to extend this image!\n\n### Dockerfile command compatibility\n\nSince Modal doesn’t use Docker to build containers, we have our own implementation of the [Dockerfile specification](https://docs.docker.com/engine/reference/builder/)\n. Most Dockerfiles should work out of the box, but there are some differences to be aware of.\n\nFirst, a few minor Dockerfile commands and flags have not been implemented yet. Please reach out to us if your use case requires any of these.\n\nNext, there are some command-specific things that may be useful when porting a Dockerfile to Modal.\n\n#### `ENTRYPOINT`\n\nWhile the [`ENTRYPOINT`](https://docs.docker.com/engine/reference/builder/#entrypoint)\n command is supported, there is an additional constraint to the entrypoint script provided: it must also `exec` the arguments passed to it at some point. This is so that Modal’s own Python entrypoint can run after your own. Most entrypoint scripts in Docker containers are wrappers over other scripts, so this is likely already the case.\n\nIf you wish to write your own entrypoint script, you can use the following as a template:\n\n    #!/usr/bin/env bash\n    \n    # Your custom startup commands here.\n    \n    exec \"$@\" # Runs the command passed to the entrypoint script.\n\nCopy\n\nIf the above file is saved as `/usr/bin/my_entrypoint.sh` in your container, then you can register it as an entrypoint with `ENTRYPOINT [\"/usr/bin/my_entrypoint.sh\"]` in your Dockerfile, or with [`dockerfile_commands`](/docs/reference/modal.Image#dockerfile_commands)\n as an Image build step.\n\n    from modal import Image\n    \n    image = (\n        Image.debian_slim()\n        .pip_install(\"foo\")\n        .dockerfile_commands('ENTRYPOINT [\"/usr/bin/my_entrypoint.sh\"]')\n    )\n\nCopy\n\n#### `ENV`\n\nWe currently don’t support Default value in [Interpolation](https://docs.docker.com/compose/compose-file/12-interpolation/)\n, such as `${VAR:-default}`\n\nImage caching and rebuilds\n--------------------------\n\nModal uses the definition of an image to determine whether it needs to be rebuilt. If the definition hasn’t changed since the last time you ran or deployed your App, the previous version will be pulled from the cache.\n\nImages are cached per layer (i.e., per `Image` method call), and breaking the cache on a single layer will cause cascading rebuilds for all subsequent layers. You can shorten iteration cycles by defining frequently-changing layers last so that the cached version of all other layers can be used.\n\nIn some cases, you may want to force an image to rebuild, even if the definition hasn’t changed. You can do this by adding the `force_build=True` argument to any of the image build steps.\n\n    from modal import Image\n    \n    image = (\n        Image.debian_slim()\n        .apt_install(\"git\")\n        .pip_install(\"slack-sdk\", force_build=True)\n        .run_commands(\"echo hi\")\n    )\n\nCopy\n\nAs in other cases where a layer’s definition changes, both the `pip_install` and `run_commands` layers will rebuild, but the `apt_install` will not. Remember to remove `force_build=True` after you’ve rebuilt the image, otherwise it will rebuild every time you run your code.\n\nAlternatively, you can set the `MODAL_FORCE_BUILD` environment variable (e.g. `MODAL_FORCE_BUILD=1 modal run ...`) to rebuild all images attached to your App. But note that, when you rebuild a base layer, the cache will be invalidated for _all_ images that depend on it, and they will rebuild the next time you run or deploy any App that uses that base.\n\n[Custom containers](#custom-containers)\n [Add Python packages with pip\\_install](#add-python-packages-with-pip_install)\n [What if I have different Python packages locally and remotely?](#what-if-i-have-different-python-packages-locally-and-remotely)\n [Run shell commands with .run\\_commands](#run-shell-commands-with-run_commands)\n [Run a Modal function during your build with .run\\_function (beta)](#run-a-modal-function-during-your-build-with-run_function-beta)\n [Attach GPUs during setup](#attach-gpus-during-setup)\n [Use mamba instead of pip with micromamba\\_install](#use-mamba-instead-of-pip-with-micromamba_install)\n [Use an existing container image with .from\\_registry](#use-an-existing-container-image-with-from_registry)\n [Bring your own image definition with .from\\_dockerfile](#bring-your-own-image-definition-with-from_dockerfile)\n [Dockerfile command compatibility](#dockerfile-command-compatibility)\n [ENTRYPOINT](#entrypoint)\n [ENV](#env)\n [Image caching and rebuilds](#image-caching-and-rebuilds)\n\nSee it in action\n\n[Registry image for Algolia indexing](/docs/examples/algolia_indexer)",
    "markdown": "* * *\n\nCustom containers\n=================\n\nThis guide walks you through how to define the environment your Modal functions and applications run within.\n\nThese environments are called _containers_. Containers are like light-weight virtual machines — container engines use [operating system tricks](https://earthly.dev/blog/chroot/)\n to isolate programs from each other (“containing” them), making them work as though they were running on their own hardware with their own filesystem. This makes execution environments more reproducible, for example by preventing accidental cross-contamination of environments on the same machine. For added security, Modal runs containers using the sandboxed [gVisor container runtime](https://cloud.google.com/blog/products/identity-security/open-sourcing-gvisor-a-sandboxed-container-runtime)\n.\n\nContainers are started up from a stored “snapshot” of their filesystem state called an _image_. Producing the image for a container is called _building_ the image.\n\nBy default, Modal functions are executed in a [Debian Linux](https://en.wikipedia.org/wiki/Debian)\n container with a basic Python installation of the same minor version `v3.x` as your local Python interpreter.\n\nCustomizing this environment is critical. To make your apps and functions useful, you will probably need some third party system packages or Python libraries. To make them start up faster, you can bake data like model weights into the container image, taking advantage of Modal’s optimized filesystem for serving containers.\n\nModal provides a number of options to customize your container images at different levels of abstraction and granularity, from high-level convenience methods like `pip_install` through wrappers of core container image build features like `RUN` and `ENV` to full on “bring-your-own-Dockerfile”. We’ll cover each of these in this guide, along with tips and tricks for building images effectively when using each tool.\n\nThe typical flow for defining an image in Modal is [method chaining](https://jugad2.blogspot.com/2016/02/examples-of-method-chaining-in-python.html)\n starting from a base image, like this:\n\n    from modal import Image\n    \n    image = (\n        Image.debian_slim(python_version=\"3.10\")\n        .apt_install(\"git\")\n        .pip_install(\"torch==2.2.1\")\n        .env({\"HALT_AND_CATCH_FIRE\": 0})\n        .run_commands(\"git clone https://github.com/modal-labs/agi && echo 'ready to go!'\")\n    )\n\nCopy\n\nIn addition to being Pythonic and clean, this also matches the onion-like [layerwise build process](https://docs.docker.com/build/guide/layers/)\n of container images.\n\nAdd Python packages with `pip_install`\n--------------------------------------\n\nThe simplest and most common container modification is to add some third party Python package, like [`pandas`](https://pandas.pydata.org/)\n.\n\nYou can add Python packages to the environment by passing all the packages you need to the [`pip_install`](/docs/reference/modal.Image/#pip_install)\n method of an image.\n\nYou can include [typical Python dependency version specifiers](https://peps.python.org/pep-0508/)\n, like `\"torch <= 2.0\"`, in the arguments. But we recommend pinning dependencies tightly, like `\"torch == 1.9.1\"`, to improve the reproducibility and robustness of your builds.\n\nOf course, that means you need to start from some image. Below, we use the recommended [`debian_slim`](/docs/reference/modal.Image#debian_slim)\n image as our base.\n\n    from modal import Image\n    \n    datascience_image = (\n        Image.debian_slim(python_version=\"3.10\")\n        .pip_install(\"pandas==2.2.0\", \"numpy\")\n    )\n    \n    \n    @app.function(image=datascience_image)\n    def my_function():\n        import pandas as pd\n        import numpy as np\n    \n        df = pd.DataFrame()\n        ...\n\nCopy\n\nNote that because you can define a different environment for each and every Modal function if you so choose, you don’t need to worry about virtual environment management. Containers make for much better separation of concerns!\n\nIf you want to run a specific version of Python remotely rather than just matching the one you’re running locally, provide the `python_version` as a string when constructing the base image, like we did above.\n\n### What if I have different Python packages locally and remotely?\n\nYou might want to use packages inside your Modal code that you don’t have on your local computer. In the example above, we build a container that uses `pandas`. But if we don’t have `pandas` locally, on the computer launching the Modal job, we can’t put `import pandas` at the top of the script, since it would cause an `ImportError`.\n\nThe easiest solution to this is to put `import pandas` in the function body instead, as you can see above. This means that `pandas` is only imported when running inside the remote Modal container, which has `pandas` installed.\n\nBe careful about what you return from Modal functions that have different packages installed than the ones you have locally! Modal functions return Python objects, like `pandas.DataFrame`s, and if your local machine doesn’t have `pandas` installed, it won’t be able to handle a `pandas` object (the error message you see will mention [serialization](https://hazelcast.com/glossary/serialization/)\n/[deserialization](https://hazelcast.com/glossary/deserialization/)\n).\n\nIf you have a lot of functions and a lot of Python packages, you might want to keep the imports in the global scope so that every function can use the same imports. In that case, you can use the [`imports()`](/docs/reference/modal.Image#imports)\n context manager:\n\n    from modal import Image\n    \n    pandas_image = Image.debian_slim().pip_install(\"pandas\", \"numpy\")\n    \n    \n    with pandas_image.imports():\n        import pandas as pd\n        import numpy as np\n    \n    \n    @app.function(image=pandas_image)\n    def my_function():\n        df = pd.DataFrame()\n\nCopy\n\nNote that this feature is still in beta.\n\nRun shell commands with `.run_commands`\n---------------------------------------\n\nYou can also supply shell commands that should be executed when building the container image.\n\nYou might use this to preload custom assets, like model parameters, so that they don’t need to be retrieved when functions start up:\n\n    from modal import Image\n    \n    image_with_model = (\n        Image.debian_slim().apt_install(\"curl\").run_commands(\n            \"curl -O https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalcatface.xml\",\n        )\n    )\n    \n    \n    @app.function(image=image_with_model)\n    def find_cats():\n        content = open(\"/haarcascade_frontalcatface.xml\").read()\n        ...\n\nCopy\n\nYou can also use this command to install Python packages. For example, some libraries require a complicated `pip` invocation that is not supported by `.pip_install`:\n\n    from modal import Image\n    \n    image = (\n        modal.Image.from_registry(\"pytorch/pytorch:2.3.1-cuda12.1-cudnn8-devel\", add_python=\"3.11\")\n        .apt_install(\"git\")\n        .run_commands(\"pip install flash-attn --no-build-isolation\")\n    )\n\nCopy\n\nOr you can install packages with [`uv`](https://github.com/astral-sh/uv)\n, which can be substantially faster than `pip`:\n\n    from modal import Image\n    \n    image = (\n        Image.debian_slim()\n        .pip_install(\"uv\")\n        .run_commands(\"uv pip install --system --compile-bytecode torch\")\n    )\n\nCopy\n\nNote that it is important to pass `--compile-bytecode` when using `uv`; its default behavior differs from that of `pip`, but it is important to compile the bytecode when you build the image so that it doesn’t happen on every container cold start.\n\nRun a Modal function during your build with `.run_function` (beta)\n------------------------------------------------------------------\n\nInstead of using shell commands, you can also run a Python function as an image build step using the [`Image.run_function`](/docs/reference/modal.Image#run_function)\n method. For example, you can use this to download model parameters from Hugging Face into your image, massively speeding up function starts:\n\n    from modal import Image, Secret\n    \n    def download_models():\n        import diffusers\n    \n        pipe = diffusers.StableDiffusionPipeline.from_pretrained(\n            model_id, use_auth_token=os.environ[\"HF_TOKEN\"]\n        )\n        pipe.save_pretrained(\"/model\")\n    \n    \n    image = (\n        Image.debian_slim()\n            .pip_install(\"diffusers[torch]\", \"transformers\", \"ftfy\", \"accelerate\")\n            .run_function(download_models, secrets=[Secret.from_name(\"huggingface-secret\")])\n    )\n\nCopy\n\nAny kwargs accepted by [`@app.function`](/docs/reference/modal.App#function)\n (such as [`Mount`s](/docs/guide/local-data#mounting-directories)\n, [`NetworkFileSystem`s](/docs/guide/network-file-systems)\n, and specifications of resources like [GPUs](/docs/guide/gpu)\n) can be supplied here.\n\nEssentially, this is equivalent to running a Modal function and snapshotting the resulting filesystem as an image.\n\nWhenever you change other features of your image, like the base image or the version of a Python package, the image will automatically be rebuilt the next time it is used. This is a bit more complicated when changing the contents of Modal functions. See the [reference documentation](/docs/reference/modal.Image#run_function)\n for details.\n\nAttach GPUs during setup\n------------------------\n\nIf a step in the setup of your container image should be run on an instance with a GPU (e.g., so that a package can be linked against CUDA libraries), pass a desired GPU type when defining that step:\n\n    from modal import Image\n    \n    image = (\n        Image.debian_slim()\n        .pip_install(\"bitsandbytes\", gpu=\"H100\")\n    )\n\nCopy\n\nUse `mamba` instead of `pip` with `micromamba_install`\n------------------------------------------------------\n\n`pip` installs Python packages, but some Python workloads require the coordinated installation of system packages as well. The `mamba` package manager can install both. Modal provides a pre-built [Micromamba](https://mamba.readthedocs.io/en/latest/user_guide/micromamba.html)\n base image that makes it easy to work with `micromamba`:\n\n    from modal import Image, App\n    \n    app = App(\"bayes-pgm\")\n    \n    numpyro_pymc_image = (\n        Image.micromamba()\n        .micromamba_install(\"pymc==5.10.4\", \"numpyro==0.13.2\", channels=[\"conda-forge\"])\n    )\n    \n    \n    @app.function(image=numpyro_pymc_image)\n    def sample():\n        import pymc as pm\n        import numpyro as np\n    \n        print(f\"Running on PyMC v{pm.__version__} with JAX/numpyro v{np.__version__} backend\")\n        ...\n\nCopy\n\nUse an existing container image with `.from_registry`\n-----------------------------------------------------\n\nYou don’t always need to start from scratch! Public registries like [Docker Hub](https://hub.docker.com/)\n have many pre-built container images for common software packages.\n\nYou can use any public image in your function using [`Image.from_registry`](/docs/reference/modal.Image#from_registry)\n, so long as:\n\n*   Python 3.8 or above is present, and is available as `python`\n*   `pip` is installed correctly\n*   The image is built for the [`linux/amd64` platform](https://unix.stackexchange.com/questions/53415/why-are-64-bit-distros-often-called-amd64)\n    \n*   The image has a [valid `ENTRYPOINT`](#entrypoint)\n    \n\n    from modal import Image\n    \n    sklearn_image = Image.from_registry(\"huanjason/scikit-learn\")\n    \n    \n    @app.function(image=sklearn_image)\n    def fit_knn():\n        from sklearn.neighbors import KNeighborsClassifier\n        ...\n\nCopy\n\nIf an existing image does not have either `python` or `pip` set up properly, you can still use it. Just provide a version number as the `add_python` argument to install a reproducible, [standalone build](https://github.com/indygreg/python-build-standalone)\n of Python:\n\n    from modal import Image\n    \n    image1 = Image.from_registry(\"ubuntu:22.04\", add_python=\"3.11\")\n    image2 = Image.from_registry(\"gisops/valhalla:latest\", add_python=\"3.11\")\n\nCopy\n\nThe `from_registry` method can load images from all public registries, such as [Nvidia’s `nvcr.io`](https://catalog.ngc.nvidia.com/containers)\n, [AWS ECR](https://aws.amazon.com/ecr/)\n, and [GitHub’s `ghcr.io`](https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry)\n.\n\nWe also support access to [private AWS ECR and GCP Artifact Registry images](/docs/guide/private-registries)\n.\n\nBring your own image definition with `.from_dockerfile`\n-------------------------------------------------------\n\nSometimes, you might be working in a setting where the environment is already defined as a container image in the form of a `Dockerfile`.\n\nModal supports defining a container image directly from a Dockerfile via the [`Image.from_dockerfile`](/docs/reference/modal.Image#from_dockerfile)\n function. It takes a path to an existing Dockerfile.\n\nFor instance, we might write a Dockerfile based on the official Python image and adding scikit-learn:\n\n    FROM python:3.9\n    RUN pip install sklearn\n\nCopy\n\nand then define an image for Modal based on it:\n\n    from modal import Image\n    \n    dockerfile_image = Image.from_dockerfile(\"Dockerfile\")\n    \n    \n    @app.function(image=dockerfile_image)\n    def fit():\n        import sklearn\n        ...\n\nCopy\n\nNote that you can still do method chaining to extend this image!\n\n### Dockerfile command compatibility\n\nSince Modal doesn’t use Docker to build containers, we have our own implementation of the [Dockerfile specification](https://docs.docker.com/engine/reference/builder/)\n. Most Dockerfiles should work out of the box, but there are some differences to be aware of.\n\nFirst, a few minor Dockerfile commands and flags have not been implemented yet. Please reach out to us if your use case requires any of these.\n\nNext, there are some command-specific things that may be useful when porting a Dockerfile to Modal.\n\n#### `ENTRYPOINT`\n\nWhile the [`ENTRYPOINT`](https://docs.docker.com/engine/reference/builder/#entrypoint)\n command is supported, there is an additional constraint to the entrypoint script provided: it must also `exec` the arguments passed to it at some point. This is so that Modal’s own Python entrypoint can run after your own. Most entrypoint scripts in Docker containers are wrappers over other scripts, so this is likely already the case.\n\nIf you wish to write your own entrypoint script, you can use the following as a template:\n\n    #!/usr/bin/env bash\n    \n    # Your custom startup commands here.\n    \n    exec \"$@\" # Runs the command passed to the entrypoint script.\n\nCopy\n\nIf the above file is saved as `/usr/bin/my_entrypoint.sh` in your container, then you can register it as an entrypoint with `ENTRYPOINT [\"/usr/bin/my_entrypoint.sh\"]` in your Dockerfile, or with [`dockerfile_commands`](/docs/reference/modal.Image#dockerfile_commands)\n as an Image build step.\n\n    from modal import Image\n    \n    image = (\n        Image.debian_slim()\n        .pip_install(\"foo\")\n        .dockerfile_commands('ENTRYPOINT [\"/usr/bin/my_entrypoint.sh\"]')\n    )\n\nCopy\n\n#### `ENV`\n\nWe currently don’t support Default value in [Interpolation](https://docs.docker.com/compose/compose-file/12-interpolation/)\n, such as `${VAR:-default}`\n\nImage caching and rebuilds\n--------------------------\n\nModal uses the definition of an image to determine whether it needs to be rebuilt. If the definition hasn’t changed since the last time you ran or deployed your App, the previous version will be pulled from the cache.\n\nImages are cached per layer (i.e., per `Image` method call), and breaking the cache on a single layer will cause cascading rebuilds for all subsequent layers. You can shorten iteration cycles by defining frequently-changing layers last so that the cached version of all other layers can be used.\n\nIn some cases, you may want to force an image to rebuild, even if the definition hasn’t changed. You can do this by adding the `force_build=True` argument to any of the image build steps.\n\n    from modal import Image\n    \n    image = (\n        Image.debian_slim()\n        .apt_install(\"git\")\n        .pip_install(\"slack-sdk\", force_build=True)\n        .run_commands(\"echo hi\")\n    )\n\nCopy\n\nAs in other cases where a layer’s definition changes, both the `pip_install` and `run_commands` layers will rebuild, but the `apt_install` will not. Remember to remove `force_build=True` after you’ve rebuilt the image, otherwise it will rebuild every time you run your code.\n\nAlternatively, you can set the `MODAL_FORCE_BUILD` environment variable (e.g. `MODAL_FORCE_BUILD=1 modal run ...`) to rebuild all images attached to your App. But note that, when you rebuild a base layer, the cache will be invalidated for _all_ images that depend on it, and they will rebuild the next time you run or deploy any App that uses that base.\n\n[Custom containers](#custom-containers)\n [Add Python packages with pip\\_install](#add-python-packages-with-pip_install)\n [What if I have different Python packages locally and remotely?](#what-if-i-have-different-python-packages-locally-and-remotely)\n [Run shell commands with .run\\_commands](#run-shell-commands-with-run_commands)\n [Run a Modal function during your build with .run\\_function (beta)](#run-a-modal-function-during-your-build-with-run_function-beta)\n [Attach GPUs during setup](#attach-gpus-during-setup)\n [Use mamba instead of pip with micromamba\\_install](#use-mamba-instead-of-pip-with-micromamba_install)\n [Use an existing container image with .from\\_registry](#use-an-existing-container-image-with-from_registry)\n [Bring your own image definition with .from\\_dockerfile](#bring-your-own-image-definition-with-from_dockerfile)\n [Dockerfile command compatibility](#dockerfile-command-compatibility)\n [ENTRYPOINT](#entrypoint)\n [ENV](#env)\n [Image caching and rebuilds](#image-caching-and-rebuilds)\n\nSee it in action\n\n[Registry image for Algolia indexing](/docs/examples/algolia_indexer)",
    "metadata": {
      "title": "Custom containers | Modal Docs",
      "description": "This guide walks you through how to define the environment your Modal functions and applications run within.",
      "ogTitle": "Custom containers",
      "ogDescription": "This guide walks you through how to define the environment your Modal functions and applications run within.",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/custom-container",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nPrivate registries\n==================\n\nModal provides the [`Image.from_registry`](/docs/guide/custom-container#use-an-existing-container-image-with-from_registry)\n function, which can pull public images available from registries such as Docker Hub and GitHub Container Registry, as well as private images from registries such as [AWS Elastic Container Registry (ECR)](https://aws.amazon.com/ecr/)\n, [GCP Artifact Registry](https://cloud.google.com/artifact-registry)\n, and Docker Hub.\n\nDocker Hub (Private)\n--------------------\n\nTo pull container images from private Docker Hub repositories, [create an access token](https://docs.docker.com/security/for-developers/access-tokens/)\n with “Read-Only” permissions and use this token value and your Docker Hub username to create a Modal [Secret](/docs/guide/secrets)\n.\n\n    REGISTRY_USERNAME=my-dockerhub-username\n    REGISTRY_PASSWORD=dckr_pat_TS012345aaa67890bbbb1234ccc\n\nCopy\n\nUse this Secret with the [`modal.Image.from_registry`](/docs/reference/modal.Image#from_registry)\n method.\n\nElastic Container Registry (ECR)\n--------------------------------\n\nYou can pull images from your AWS ECR account by specifying the full image URI as follows:\n\n    from modal import Image, Secret, App\n    \n    aws_secret = Secret.from_name(\"my-aws-secret\")\n    image = (\n        Image.from_aws_ecr(\n            \"000000000000.dkr.ecr.us-east-1.amazonaws.com/my-private-registry:latest\",\n            secret=aws_secret,\n        )\n        .pip_install(\"torch\", \"huggingface\")\n    )\n    \n    app = App(image=image)\n\nCopy\n\nAs shown above, you also need to use a [Modal Secret](/docs/guide/secrets)\n containing the environment variables `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, and `AWS_REGION`. The AWS IAM user account associated with those keys must have access to the private registry you want to access.\n\nThe user needs to have the following read-only policies:\n\n    {\n      \"Version\": \"2012-10-17\",\n      \"Statement\": [\\\n        {\\\n          \"Action\": [\"ecr:GetAuthorizationToken\"],\\\n          \"Effect\": \"Allow\",\\\n          \"Resource\": \"*\"\\\n        },\\\n        {\\\n          \"Effect\": \"Allow\",\\\n          \"Action\": [\\\n            \"ecr:BatchCheckLayerAvailability\",\\\n            \"ecr:GetDownloadUrlForLayer\",\\\n            \"ecr:GetRepositoryPolicy\",\\\n            \"ecr:DescribeRepositories\",\\\n            \"ecr:ListImages\",\\\n            \"ecr:DescribeImages\",\\\n            \"ecr:BatchGetImage\",\\\n            \"ecr:GetLifecyclePolicy\",\\\n            \"ecr:GetLifecyclePolicyPreview\",\\\n            \"ecr:ListTagsForResource\",\\\n            \"ecr:DescribeImageScanFindings\"\\\n          ],\\\n          \"Resource\": \"<MY-REGISTRY-ARN>\"\\\n        }\\\n      ]\n    }\n\nCopy\n\nYou can use the IAM configuration above as a template for creating an IAM user. You can then [generate an access key](https://aws.amazon.com/premiumsupport/knowledge-center/create-access-key/)\n and create a Modal Secret using the AWS integration option. Modal will use your access keys to generate an ephemeral ECR token. That token is only used to pull image layers at the time a new image is built. We don’t store this token but will cache the image once it has been pulled.\n\nImages on ECR must be private and follow [image configuration requirements](/docs/reference/modal.Image#from_aws_ecr)\n.\n\nGoogle Artifact Registry and Google Container Registry\n------------------------------------------------------\n\nFor further detail on how to pull images from Google’s image registries, see [`modal.Image.from_gcp_artifact_registry`](/docs/reference/modal.Image#from_gcp_artifact_registry)\n.\n\n[Private registries](#private-registries)\n [Docker Hub (Private)](#docker-hub-private)\n [Elastic Container Registry (ECR)](#elastic-container-registry-ecr)\n [Google Artifact Registry and Google Container Registry](#google-artifact-registry-and-google-container-registry)\n\nSee it in action\n\n[Registry image for Algolia indexing](/docs/examples/algolia_indexer)",
    "markdown": "* * *\n\nPrivate registries\n==================\n\nModal provides the [`Image.from_registry`](/docs/guide/custom-container#use-an-existing-container-image-with-from_registry)\n function, which can pull public images available from registries such as Docker Hub and GitHub Container Registry, as well as private images from registries such as [AWS Elastic Container Registry (ECR)](https://aws.amazon.com/ecr/)\n, [GCP Artifact Registry](https://cloud.google.com/artifact-registry)\n, and Docker Hub.\n\nDocker Hub (Private)\n--------------------\n\nTo pull container images from private Docker Hub repositories, [create an access token](https://docs.docker.com/security/for-developers/access-tokens/)\n with “Read-Only” permissions and use this token value and your Docker Hub username to create a Modal [Secret](/docs/guide/secrets)\n.\n\n    REGISTRY_USERNAME=my-dockerhub-username\n    REGISTRY_PASSWORD=dckr_pat_TS012345aaa67890bbbb1234ccc\n\nCopy\n\nUse this Secret with the [`modal.Image.from_registry`](/docs/reference/modal.Image#from_registry)\n method.\n\nElastic Container Registry (ECR)\n--------------------------------\n\nYou can pull images from your AWS ECR account by specifying the full image URI as follows:\n\n    from modal import Image, Secret, App\n    \n    aws_secret = Secret.from_name(\"my-aws-secret\")\n    image = (\n        Image.from_aws_ecr(\n            \"000000000000.dkr.ecr.us-east-1.amazonaws.com/my-private-registry:latest\",\n            secret=aws_secret,\n        )\n        .pip_install(\"torch\", \"huggingface\")\n    )\n    \n    app = App(image=image)\n\nCopy\n\nAs shown above, you also need to use a [Modal Secret](/docs/guide/secrets)\n containing the environment variables `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, and `AWS_REGION`. The AWS IAM user account associated with those keys must have access to the private registry you want to access.\n\nThe user needs to have the following read-only policies:\n\n    {\n      \"Version\": \"2012-10-17\",\n      \"Statement\": [\\\n        {\\\n          \"Action\": [\"ecr:GetAuthorizationToken\"],\\\n          \"Effect\": \"Allow\",\\\n          \"Resource\": \"*\"\\\n        },\\\n        {\\\n          \"Effect\": \"Allow\",\\\n          \"Action\": [\\\n            \"ecr:BatchCheckLayerAvailability\",\\\n            \"ecr:GetDownloadUrlForLayer\",\\\n            \"ecr:GetRepositoryPolicy\",\\\n            \"ecr:DescribeRepositories\",\\\n            \"ecr:ListImages\",\\\n            \"ecr:DescribeImages\",\\\n            \"ecr:BatchGetImage\",\\\n            \"ecr:GetLifecyclePolicy\",\\\n            \"ecr:GetLifecyclePolicyPreview\",\\\n            \"ecr:ListTagsForResource\",\\\n            \"ecr:DescribeImageScanFindings\"\\\n          ],\\\n          \"Resource\": \"<MY-REGISTRY-ARN>\"\\\n        }\\\n      ]\n    }\n\nCopy\n\nYou can use the IAM configuration above as a template for creating an IAM user. You can then [generate an access key](https://aws.amazon.com/premiumsupport/knowledge-center/create-access-key/)\n and create a Modal Secret using the AWS integration option. Modal will use your access keys to generate an ephemeral ECR token. That token is only used to pull image layers at the time a new image is built. We don’t store this token but will cache the image once it has been pulled.\n\nImages on ECR must be private and follow [image configuration requirements](/docs/reference/modal.Image#from_aws_ecr)\n.\n\nGoogle Artifact Registry and Google Container Registry\n------------------------------------------------------\n\nFor further detail on how to pull images from Google’s image registries, see [`modal.Image.from_gcp_artifact_registry`](/docs/reference/modal.Image#from_gcp_artifact_registry)\n.\n\n[Private registries](#private-registries)\n [Docker Hub (Private)](#docker-hub-private)\n [Elastic Container Registry (ECR)](#elastic-container-registry-ecr)\n [Google Artifact Registry and Google Container Registry](#google-artifact-registry-and-google-container-registry)\n\nSee it in action\n\n[Registry image for Algolia indexing](/docs/examples/algolia_indexer)",
    "metadata": {
      "title": "Private registries | Modal Docs",
      "description": "Modal provides the Image.from_registry function, which can pull public images available from registries such as Docker Hub and GitHub Container Registry, as well as private images from registries such as AWS Elastic Container Registry (ECR), GCP Artifact Registry, and Docker Hub.",
      "ogTitle": "Private registries",
      "ogDescription": "Modal provides the Image.from_registry function, which can pull public images available from registries such as Docker Hub and GitHub Container Registry, as well as private images from registries such as AWS Elastic Container Registry (ECR), GCP Artifact Registry, and Docker Hub.",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/private-registries",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nGPU acceleration\n================\n\nContemporary machine learning models are large linear algebra machines, and running them with reasonable latency and throughput requires specialized hardware for executing large linear algebra tasks. The weapon of choice here is the venerable Graphics Processing Unit, or GPU.\n\nModal is designed from the ground up to make running your ML-powered functions on GPUs as easy, cost-effective, and performant as possible. And Modal GPUs are great for [graphics too](/docs/examples/blender_video)\n!\n\nThis guide will walk you through all the options available for running your GPU-acclerated code on Modal and suggest techniques for choosing the right hardware for your problem.\n\nIf you have code or use libraries that benefit from GPUs, you can attach the first available GPU to your function by passing the `gpu=\"any\"` argument to the `@app.function` decorator:\n\n    import modal\n    \n    app = modal.App()\n    \n    @app.function(gpu=\"any\")\n    def render_toy_story():\n        # code here will be executed on a machine with an available GPU\n        ...\n\nCopy\n\nSpecifying GPU type\n-------------------\n\nWhen `gpu=\"any\"` is specified, your function runs in a container with access to a GPU. Currently this GPU will be either an NVIDIA [Tesla T4](https://www.nvidia.com/en-us/data-center/tesla-t4/)\n or [A10G](https://www.nvidia.com/en-us/data-center/products/a10-gpu/)\n instance, and [pricing](/pricing)\n is based on which one you land on.\n\nIf you need more control, you can pick a specific GPU type by changing this argument:\n\n    @app.function(gpu=\"A10G\")\n    def run_sdxl_turbo():\n        ...\n    \n    @app.function(gpu=\"A100\")\n    def run_sdxl_batch():\n        ...\n    \n    @app.function(gpu=\"H100\")\n    def finetune_sdxl():\n        ...\n\nCopy\n\nFor information on all valid values for the `gpu` parameter see [the reference docs](/docs/reference/modal.gpu)\n.\n\nFor running, rather than training, neural networks, we recommend starting off with the A10Gs, which offer an excellent trade-off of cost and performance.\n\nFor more on how to pick a GPU for use with neural networks like LLaMA or Stable Diffusion, and for tips on how to make that GPU go brrr, check out [Tim Dettemers’ blog post](https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/)\n or the [Full Stack Deep Learning page on Cloud GPUs](https://fullstackdeeplearning.com/cloud-gpus/)\n.\n\nSpecifying GPU count\n--------------------\n\nThe largest machine learning models are too large to fit in the memory of just one of even the most capacious GPUs. Rather than off-loading from GPU memory to CPU memory or disk, which leads to punishing drops in latency and throughput, the usual tactic is to parallelize the model across several GPUs on the same machine — or even to distribute it across several machines, each with several GPUs.\n\nYou can run your function on a Modal machine with more than one GPU by changing the `count` argument in the [object form](/docs/reference/modal.gpu)\n of the `gpu` parameter:\n\n    @app.function(gpu=modal.gpu.H100(count=8))\n    def train_sdxl():\n        ...\n\nCopy\n\nWe also support an equivalent string-based shorthand for specifying the count:\n\n    \n    @app.function(gpu=\"H100:8\")\n    def train_sdxl():\n        ...\n\nCopy\n\nCurrently H100, A100, and T4 instances support up to 8 GPUs (up to 640 GB VRAM), and A10G instances support up to 4 GPUs (up to 96 GB VRAM). Note that requesting more than 2 GPUs per container will usually result in larger wait times. These GPUs are always attached to the same physical machine.\n\nH100 GPUs\n---------\n\nModal’s fastest GPUs are the [H100s](https://www.nvidia.com/en-us/data-center/h100/)\n, which are NVIDIA’s flagship data center chip. Modal offers H100s in the SXM form factor, with Tensor Cores capable of nearly two petaFLOPS in 16-bit precision connected via >3 TB/s bandwidth connections to 80 GB of on-device RAM. Powerful!\n\nTo request an H100, replace the `gpu=\"any\"` argument with `gpu=\"H100\"`\n\n    @app.function(gpu=\"H100\")\n    def run_mixtral():\n        ...\n\nCopy\n\nCheck out [this example](/docs/examples/vllm_gemma)\n to see how you can run 7B parameter language models at thousands of tokens per second using an H100 on Modal.\n\nBefore you jump for the most powerful (and so most expensive) GPU, make sure you understand where the bottlenecks are in your computations. For example, running language models with small batch sizes (e.g. one prompt at a time) results in a [bottleneck on memory, not arithmetic](https://kipp.ly/transformer-inference-arithmetic/)\n. Since arithmetic throughput has risen faster than memory throughput in recent hardware generations, speedups for memory-bound GPU jobs are not as extreme and may not be worth the extra cost.\n\nAdditionally, it takes time for library providers to adapt to the latest hardware, so expect sharp edges, like missing CUDA kernels, when using H100s — through at least the first half of 2024.\n\nA100 GPUs\n---------\n\n[A100s](https://www.nvidia.com/en-us/data-center/a100/)\n are the previous generation of top-of-the-line data center chip from NVIDIA. Modal offers two versions of the A100: one with 40 GB of RAM and another with 80 GB of RAM.\n\nTo request an A100 with 40 GB of GPU memory, replace the `gpu=\"any\"` argument with `gpu=\"A100\"`:\n\n    @app.function(gpu=\"A100\")\n    def llama_7b():\n        ...\n\nCopy\n\nAt half precision, a 34B parameter language model like LLaMA 34B will require more than 40 GB of RAM (16 bits = 2 bytes and 34 × 2 > 40). To request an 80 GB A100 that can run those models, use the [object form](/docs/reference/modal.gpu)\n. of the `gpu` argument:\n\n    @app.function(gpu=modal.gpu.A100(size=\"80GB\"))\n    def llama_34b():\n        ...\n\nCopy\n\nFor the largest useful open source models, or when finetuning models that are of size 7B or higher, you may need mutliple A100s to have enough GPU RAM. Finetuning models can be particularly RAM intensive because optimizing neural networks requires [storing a lot of things in memory](https://fullstackdeeplearning.com/course/2022/lecture-2-development-infrastructure-and-tooling/#sharded-data-parallelism)\n: not only input data and weights, but also intermediate calculations, gradients, and optimizer parameters.\n\nTo use more than one GPU, set the `count` argument to an integer value between `2` and `8`.\n\n    @app.function(gpu=modal.gpu.A100(size=\"80GB\", count=8))\n    def finetune_llama_70b():\n        ...\n\nCopy\n\nExamples\n--------\n\nTake a look at some of our examples that use GPUs:\n\n*   [Fast multi-GPU inference with vLLM (Mixtral 8x7B)](/docs/examples/vllm_mixtral)\n    \n*   [Finetuning Stable Diffusion on pictures of your pet](/docs/examples/dreambooth_app)\n    \n*   [Real-time Stable Diffusion XL Turbo](/docs/examples/stable_diffusion_xl_turbo)\n    \n*   [Rendering Blender videos](/docs/examples/blender_video)\n    \n\n[GPU acceleration](#gpu-acceleration)\n [Specifying GPU type](#specifying-gpu-type)\n [Specifying GPU count](#specifying-gpu-count)\n [H100 GPUs](#h100-gpus)\n [A100 GPUs](#a100-gpus)\n [Examples](#examples)\n\nSee it in action\n\n[High-speed inference with vLLM](/docs/examples/vllm_inference)\n\n[Stable Diffusion XL Turbo](/docs/examples/stable_diffusion_xl_turbo)\n\n[Blender video renderer](/docs/examples/blender_video)",
    "markdown": "* * *\n\nGPU acceleration\n================\n\nContemporary machine learning models are large linear algebra machines, and running them with reasonable latency and throughput requires specialized hardware for executing large linear algebra tasks. The weapon of choice here is the venerable Graphics Processing Unit, or GPU.\n\nModal is designed from the ground up to make running your ML-powered functions on GPUs as easy, cost-effective, and performant as possible. And Modal GPUs are great for [graphics too](/docs/examples/blender_video)\n!\n\nThis guide will walk you through all the options available for running your GPU-acclerated code on Modal and suggest techniques for choosing the right hardware for your problem.\n\nIf you have code or use libraries that benefit from GPUs, you can attach the first available GPU to your function by passing the `gpu=\"any\"` argument to the `@app.function` decorator:\n\n    import modal\n    \n    app = modal.App()\n    \n    @app.function(gpu=\"any\")\n    def render_toy_story():\n        # code here will be executed on a machine with an available GPU\n        ...\n\nCopy\n\nSpecifying GPU type\n-------------------\n\nWhen `gpu=\"any\"` is specified, your function runs in a container with access to a GPU. Currently this GPU will be either an NVIDIA [Tesla T4](https://www.nvidia.com/en-us/data-center/tesla-t4/)\n or [A10G](https://www.nvidia.com/en-us/data-center/products/a10-gpu/)\n instance, and [pricing](/pricing)\n is based on which one you land on.\n\nIf you need more control, you can pick a specific GPU type by changing this argument:\n\n    @app.function(gpu=\"A10G\")\n    def run_sdxl_turbo():\n        ...\n    \n    @app.function(gpu=\"A100\")\n    def run_sdxl_batch():\n        ...\n    \n    @app.function(gpu=\"H100\")\n    def finetune_sdxl():\n        ...\n\nCopy\n\nFor information on all valid values for the `gpu` parameter see [the reference docs](/docs/reference/modal.gpu)\n.\n\nFor running, rather than training, neural networks, we recommend starting off with the A10Gs, which offer an excellent trade-off of cost and performance.\n\nFor more on how to pick a GPU for use with neural networks like LLaMA or Stable Diffusion, and for tips on how to make that GPU go brrr, check out [Tim Dettemers’ blog post](https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/)\n or the [Full Stack Deep Learning page on Cloud GPUs](https://fullstackdeeplearning.com/cloud-gpus/)\n.\n\nSpecifying GPU count\n--------------------\n\nThe largest machine learning models are too large to fit in the memory of just one of even the most capacious GPUs. Rather than off-loading from GPU memory to CPU memory or disk, which leads to punishing drops in latency and throughput, the usual tactic is to parallelize the model across several GPUs on the same machine — or even to distribute it across several machines, each with several GPUs.\n\nYou can run your function on a Modal machine with more than one GPU by changing the `count` argument in the [object form](/docs/reference/modal.gpu)\n of the `gpu` parameter:\n\n    @app.function(gpu=modal.gpu.H100(count=8))\n    def train_sdxl():\n        ...\n\nCopy\n\nWe also support an equivalent string-based shorthand for specifying the count:\n\n    \n    @app.function(gpu=\"H100:8\")\n    def train_sdxl():\n        ...\n\nCopy\n\nCurrently H100, A100, and T4 instances support up to 8 GPUs (up to 640 GB VRAM), and A10G instances support up to 4 GPUs (up to 96 GB VRAM). Note that requesting more than 2 GPUs per container will usually result in larger wait times. These GPUs are always attached to the same physical machine.\n\nH100 GPUs\n---------\n\nModal’s fastest GPUs are the [H100s](https://www.nvidia.com/en-us/data-center/h100/)\n, which are NVIDIA’s flagship data center chip. Modal offers H100s in the SXM form factor, with Tensor Cores capable of nearly two petaFLOPS in 16-bit precision connected via >3 TB/s bandwidth connections to 80 GB of on-device RAM. Powerful!\n\nTo request an H100, replace the `gpu=\"any\"` argument with `gpu=\"H100\"`\n\n    @app.function(gpu=\"H100\")\n    def run_mixtral():\n        ...\n\nCopy\n\nCheck out [this example](/docs/examples/vllm_gemma)\n to see how you can run 7B parameter language models at thousands of tokens per second using an H100 on Modal.\n\nBefore you jump for the most powerful (and so most expensive) GPU, make sure you understand where the bottlenecks are in your computations. For example, running language models with small batch sizes (e.g. one prompt at a time) results in a [bottleneck on memory, not arithmetic](https://kipp.ly/transformer-inference-arithmetic/)\n. Since arithmetic throughput has risen faster than memory throughput in recent hardware generations, speedups for memory-bound GPU jobs are not as extreme and may not be worth the extra cost.\n\nAdditionally, it takes time for library providers to adapt to the latest hardware, so expect sharp edges, like missing CUDA kernels, when using H100s — through at least the first half of 2024.\n\nA100 GPUs\n---------\n\n[A100s](https://www.nvidia.com/en-us/data-center/a100/)\n are the previous generation of top-of-the-line data center chip from NVIDIA. Modal offers two versions of the A100: one with 40 GB of RAM and another with 80 GB of RAM.\n\nTo request an A100 with 40 GB of GPU memory, replace the `gpu=\"any\"` argument with `gpu=\"A100\"`:\n\n    @app.function(gpu=\"A100\")\n    def llama_7b():\n        ...\n\nCopy\n\nAt half precision, a 34B parameter language model like LLaMA 34B will require more than 40 GB of RAM (16 bits = 2 bytes and 34 × 2 > 40). To request an 80 GB A100 that can run those models, use the [object form](/docs/reference/modal.gpu)\n. of the `gpu` argument:\n\n    @app.function(gpu=modal.gpu.A100(size=\"80GB\"))\n    def llama_34b():\n        ...\n\nCopy\n\nFor the largest useful open source models, or when finetuning models that are of size 7B or higher, you may need mutliple A100s to have enough GPU RAM. Finetuning models can be particularly RAM intensive because optimizing neural networks requires [storing a lot of things in memory](https://fullstackdeeplearning.com/course/2022/lecture-2-development-infrastructure-and-tooling/#sharded-data-parallelism)\n: not only input data and weights, but also intermediate calculations, gradients, and optimizer parameters.\n\nTo use more than one GPU, set the `count` argument to an integer value between `2` and `8`.\n\n    @app.function(gpu=modal.gpu.A100(size=\"80GB\", count=8))\n    def finetune_llama_70b():\n        ...\n\nCopy\n\nExamples\n--------\n\nTake a look at some of our examples that use GPUs:\n\n*   [Fast multi-GPU inference with vLLM (Mixtral 8x7B)](/docs/examples/vllm_mixtral)\n    \n*   [Finetuning Stable Diffusion on pictures of your pet](/docs/examples/dreambooth_app)\n    \n*   [Real-time Stable Diffusion XL Turbo](/docs/examples/stable_diffusion_xl_turbo)\n    \n*   [Rendering Blender videos](/docs/examples/blender_video)\n    \n\n[GPU acceleration](#gpu-acceleration)\n [Specifying GPU type](#specifying-gpu-type)\n [Specifying GPU count](#specifying-gpu-count)\n [H100 GPUs](#h100-gpus)\n [A100 GPUs](#a100-gpus)\n [Examples](#examples)\n\nSee it in action\n\n[High-speed inference with vLLM](/docs/examples/vllm_inference)\n\n[Stable Diffusion XL Turbo](/docs/examples/stable_diffusion_xl_turbo)\n\n[Blender video renderer](/docs/examples/blender_video)",
    "metadata": {
      "title": "GPU acceleration | Modal Docs",
      "description": "Contemporary machine learning models are large linear algebra machines, and running them with reasonable latency and throughput requires specialized hardware for executing large linear algebra tasks. The weapon of choice here is the venerable Graphics Processing Unit, or GPU.",
      "ogTitle": "GPU acceleration",
      "ogDescription": "Contemporary machine learning models are large linear algebra machines, and running them with reasonable latency and throughput requires specialized hardware for executing large linear algebra tasks. The weapon of choice here is the venerable Graphics Processing Unit, or GPU.",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/gpu",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nReserving CPU and memory\n========================\n\nModal jobs are reserved 128 MiB of memory and 0.1 CPU cores by default. However, if there is free memory or CPU capacity on a worker, containers are free to spike above these limits.\n\nKeep in mind that these reservations should be set to the _minimum_ value required to have your application function correctly.\n\nCPU cores\n---------\n\nIf you have code that must run on a larger number of cores, you can request that using the `cpu` argument. This allows you to specify a floating-point number of CPU cores:\n\n    import modal\n    \n    app = modal.App()\n    \n    @app.function(cpu=8.0)\n    def my_function():\n        # code here will have access to at least 8.0 cores\n        ...\n\nCopy\n\nMemory\n------\n\nIf you have code that needs more guaranteed memory, you can request it using the `memory` argument. This expects an integer number of megabytes:\n\n    import modal\n    \n    app = modal.App()\n    \n    @app.function(memory=32768)\n    def my_function():\n        # code here will have access to at least 32 GiB of RAM\n        ...\n\nCopy\n\nHow much can I request?\n-----------------------\n\nFor both CPU and memory, a maximum is enforced at function creation time to ensure your application can be scheduled for execution. Requests exceeding the maximum will be rejected with an [`InvalidError`](/docs/reference/modal.exception#modalexceptioninvaliderror)\n.\n\nAs the platform grows, we plan to support larger CPU and memory reservations.\n\nResource limits\n---------------\n\n### CPU limits\n\nModal containers have a soft CPU limit that is set at 4 physical cores above the CPU request. Given that the default CPU request is 0.1 cores the soft CPU limit is 4.1 cores. Above this limit the host will begin to throttle the CPU usage of the container.\n\n### Memory limits\n\nModal containers can have a hard memory limit which will ‘Out of Memory’ (OOM) kill containers which attempt to exceed the limit. This functionality is useful when a container has a serious memory leak. You can set the limit and have the container killed to avoid paying for the leaked GBs of memory.\n\n    mem_request = 1024\n    mem_limit = 2048\n    @app.function(\n        memory=(mem_request, mem_limit),\n    )\n\nCopy\n\nSpecify this limit using the [`memory` parameter](/docs/reference/modal.App#function)\n on Modal Functions.\n\n### Disk limits\n\nRunning Modal containers have access to many GBs of SSD disk, but the amount of writes is limited by:\n\n1.  The size of the underlying worker’s SSD disk capacity\n2.  A per-container disk quota that is set in the 100s of GBs.\n\nHitting either limit will cause the container’s disk writes to be rejected, which typically manifests as an `OSError`.\n\nIncreased disk sizes can be requested with the [`ephemeral_disk` parameter](/docs/reference/modal.App#function)\n. The maximum disk size is 3.0 TiB (3,145,728 MiB). Larger disks are intended to be used for [dataset processing](/docs/guide/dataset-ingestion)\n. Be aware that requesting larger disk size prevents also specifying other configuration requests (RAM, GPU, region).\n\n[Reserving CPU and memory](#reserving-cpu-and-memory)\n [CPU cores](#cpu-cores)\n [Memory](#memory)\n [How much can I request?](#how-much-can-i-request)\n [Resource limits](#resource-limits)\n [CPU limits](#cpu-limits)\n [Memory limits](#memory-limits)\n [Disk limits](#disk-limits)",
    "markdown": "* * *\n\nReserving CPU and memory\n========================\n\nModal jobs are reserved 128 MiB of memory and 0.1 CPU cores by default. However, if there is free memory or CPU capacity on a worker, containers are free to spike above these limits.\n\nKeep in mind that these reservations should be set to the _minimum_ value required to have your application function correctly.\n\nCPU cores\n---------\n\nIf you have code that must run on a larger number of cores, you can request that using the `cpu` argument. This allows you to specify a floating-point number of CPU cores:\n\n    import modal\n    \n    app = modal.App()\n    \n    @app.function(cpu=8.0)\n    def my_function():\n        # code here will have access to at least 8.0 cores\n        ...\n\nCopy\n\nMemory\n------\n\nIf you have code that needs more guaranteed memory, you can request it using the `memory` argument. This expects an integer number of megabytes:\n\n    import modal\n    \n    app = modal.App()\n    \n    @app.function(memory=32768)\n    def my_function():\n        # code here will have access to at least 32 GiB of RAM\n        ...\n\nCopy\n\nHow much can I request?\n-----------------------\n\nFor both CPU and memory, a maximum is enforced at function creation time to ensure your application can be scheduled for execution. Requests exceeding the maximum will be rejected with an [`InvalidError`](/docs/reference/modal.exception#modalexceptioninvaliderror)\n.\n\nAs the platform grows, we plan to support larger CPU and memory reservations.\n\nResource limits\n---------------\n\n### CPU limits\n\nModal containers have a soft CPU limit that is set at 4 physical cores above the CPU request. Given that the default CPU request is 0.1 cores the soft CPU limit is 4.1 cores. Above this limit the host will begin to throttle the CPU usage of the container.\n\n### Memory limits\n\nModal containers can have a hard memory limit which will ‘Out of Memory’ (OOM) kill containers which attempt to exceed the limit. This functionality is useful when a container has a serious memory leak. You can set the limit and have the container killed to avoid paying for the leaked GBs of memory.\n\n    mem_request = 1024\n    mem_limit = 2048\n    @app.function(\n        memory=(mem_request, mem_limit),\n    )\n\nCopy\n\nSpecify this limit using the [`memory` parameter](/docs/reference/modal.App#function)\n on Modal Functions.\n\n### Disk limits\n\nRunning Modal containers have access to many GBs of SSD disk, but the amount of writes is limited by:\n\n1.  The size of the underlying worker’s SSD disk capacity\n2.  A per-container disk quota that is set in the 100s of GBs.\n\nHitting either limit will cause the container’s disk writes to be rejected, which typically manifests as an `OSError`.\n\nIncreased disk sizes can be requested with the [`ephemeral_disk` parameter](/docs/reference/modal.App#function)\n. The maximum disk size is 3.0 TiB (3,145,728 MiB). Larger disks are intended to be used for [dataset processing](/docs/guide/dataset-ingestion)\n. Be aware that requesting larger disk size prevents also specifying other configuration requests (RAM, GPU, region).\n\n[Reserving CPU and memory](#reserving-cpu-and-memory)\n [CPU cores](#cpu-cores)\n [Memory](#memory)\n [How much can I request?](#how-much-can-i-request)\n [Resource limits](#resource-limits)\n [CPU limits](#cpu-limits)\n [Memory limits](#memory-limits)\n [Disk limits](#disk-limits)",
    "metadata": {
      "title": "Reserving CPU and memory | Modal Docs",
      "description": "Modal jobs are reserved 128 MiB of memory and 0.1 CPU cores by default. However, if there is free memory or CPU capacity on a worker, containers are free to spike above these limits.",
      "ogTitle": "Reserving CPU and memory",
      "ogDescription": "Modal jobs are reserved 128 MiB of memory and 0.1 CPU cores by default. However, if there is free memory or CPU capacity on a worker, containers are free to spike above these limits.",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/resources",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nScaling out\n===========\n\nModal has a few different tools that helps with increasing performance of your applications.\n\nParallel execution of inputs\n----------------------------\n\nIf your code is running the same function repeatedly with different independent inputs (e.g., a grid search), the easiest way to increase performance is to run those function calls in parallel using Modal’s [`Function.map()`](/docs/reference/modal.Function#map)\n method.\n\nHere is an example if we had a function evaluate\\_model that takes a single argument:\n\n    import modal\n    \n    app = modal.App()\n    \n    \n    @app.function()\n    def evaluate_model(x):\n        ...\n    \n    \n    @app.local_entrypoint()\n    def main():\n        inputs = list(range(100))\n        for result in evaluate_model.map(inputs):  # runs many inputs in parallel\n            ...\n\nCopy\n\nIn this example, `evaluate_model` will be called with each of the 100 inputs (the numbers 0 - 99 in this case) roughly in parallel and the results are returned as an iterable with the results ordered in the same way as the inputs.\n\n### Exceptions\n\nBy default, if any of the function calls raises an exception, the exception will be propagated. To treat exceptions as successful results and aggregate them in the results list, pass in [`return_exceptions=True`](/docs/reference/modal.Function#map)\n.\n\n    @app.function()\n    def my_func(a):\n        if a == 2:\n            raise Exception(\"ohno\")\n        return a ** 2\n    \n    @app.local_entrypoint()\n    def main():\n        print(list(my_func.map(range(3), return_exceptions=True)))\n        # [0, 1, UserCodeException(Exception('ohno'))]\n\nCopy\n\n### Starmap\n\nIf your function takes multiple variable arguments, you can either use [`Function.map()`](/docs/reference/modal.Function#map)\n with one input iterator per argument, or [`Function.starmap()`](/docs/reference/modal.Function#starmap)\n with a single input iterator containing sequences (like tuples) that can be spread over the arguments. This works similarly to Python’s built in `map` and `itertools.starmap`.\n\n    @app.function()\n    def my_func(a, b):\n        return a + b\n    \n    @app.local_entrypoint()\n    def main():\n        assert list(my_func.starmap([(1, 2), (3, 4)])) == [3, 7]\n\nCopy\n\n### Gotchas\n\nNote that `.map()` is a method on the modal function object itself, so you don’t explicitly _call_ the function.\n\nIncorrect usage:\n\n    results = evaluate_model(inputs).map()\n\nCopy\n\nModal’s map is also not the same as using Python’s builtin `map()`. While the following will technically work, it will execute all inputs in sequence rather than in parallel.\n\nIncorrect usage:\n\n    results = map(evaluate_model, inputs)\n\nCopy\n\nAsynchronous usage\n------------------\n\nAll Modal APIs are available in both blocking and asynchronous variants. If you are comfortable with asynchronous programming, you can use it to create arbitrary parallel execution patterns, with the added benefit that any Modal functions will be executed remotely. See the [async guide](/docs/guide/async)\n or the examples for more information about asynchronous usage.\n\nGPU acceleration\n----------------\n\nSometimes you can speed up your applications by utilizing GPU acceleration. See the [gpu section](/docs/guide/gpu)\n for more information.\n\nLimiting concurrency\n--------------------\n\nIf you want to limit concurrency, you can use the `concurrency_limit` argument to `app.function`. For instance:\n\n    app = modal.App()\n    \n    @app.function(concurrency_limit=5)\n    def f(x):\n        print(x)\n\nCopy\n\nWith this, Modal will spin up at most 5 containers at any point.\n\n[Scaling out](#scaling-out)\n [Parallel execution of inputs](#parallel-execution-of-inputs)\n [Exceptions](#exceptions)\n [Starmap](#starmap)\n [Gotchas](#gotchas)\n [Asynchronous usage](#asynchronous-usage)\n [GPU acceleration](#gpu-acceleration)\n [Limiting concurrency](#limiting-concurrency)\n\nSee it in action\n\n[Auto-scaling LLM inference endpoints](/docs/examples/text_generation_inference)\n\n[Job queue for OCR](/docs/examples/doc_ocr_jobs)\n\n[Parallel web scraping](/docs/examples/web-scraper#scaling-out)",
    "markdown": "* * *\n\nScaling out\n===========\n\nModal has a few different tools that helps with increasing performance of your applications.\n\nParallel execution of inputs\n----------------------------\n\nIf your code is running the same function repeatedly with different independent inputs (e.g., a grid search), the easiest way to increase performance is to run those function calls in parallel using Modal’s [`Function.map()`](/docs/reference/modal.Function#map)\n method.\n\nHere is an example if we had a function evaluate\\_model that takes a single argument:\n\n    import modal\n    \n    app = modal.App()\n    \n    \n    @app.function()\n    def evaluate_model(x):\n        ...\n    \n    \n    @app.local_entrypoint()\n    def main():\n        inputs = list(range(100))\n        for result in evaluate_model.map(inputs):  # runs many inputs in parallel\n            ...\n\nCopy\n\nIn this example, `evaluate_model` will be called with each of the 100 inputs (the numbers 0 - 99 in this case) roughly in parallel and the results are returned as an iterable with the results ordered in the same way as the inputs.\n\n### Exceptions\n\nBy default, if any of the function calls raises an exception, the exception will be propagated. To treat exceptions as successful results and aggregate them in the results list, pass in [`return_exceptions=True`](/docs/reference/modal.Function#map)\n.\n\n    @app.function()\n    def my_func(a):\n        if a == 2:\n            raise Exception(\"ohno\")\n        return a ** 2\n    \n    @app.local_entrypoint()\n    def main():\n        print(list(my_func.map(range(3), return_exceptions=True)))\n        # [0, 1, UserCodeException(Exception('ohno'))]\n\nCopy\n\n### Starmap\n\nIf your function takes multiple variable arguments, you can either use [`Function.map()`](/docs/reference/modal.Function#map)\n with one input iterator per argument, or [`Function.starmap()`](/docs/reference/modal.Function#starmap)\n with a single input iterator containing sequences (like tuples) that can be spread over the arguments. This works similarly to Python’s built in `map` and `itertools.starmap`.\n\n    @app.function()\n    def my_func(a, b):\n        return a + b\n    \n    @app.local_entrypoint()\n    def main():\n        assert list(my_func.starmap([(1, 2), (3, 4)])) == [3, 7]\n\nCopy\n\n### Gotchas\n\nNote that `.map()` is a method on the modal function object itself, so you don’t explicitly _call_ the function.\n\nIncorrect usage:\n\n    results = evaluate_model(inputs).map()\n\nCopy\n\nModal’s map is also not the same as using Python’s builtin `map()`. While the following will technically work, it will execute all inputs in sequence rather than in parallel.\n\nIncorrect usage:\n\n    results = map(evaluate_model, inputs)\n\nCopy\n\nAsynchronous usage\n------------------\n\nAll Modal APIs are available in both blocking and asynchronous variants. If you are comfortable with asynchronous programming, you can use it to create arbitrary parallel execution patterns, with the added benefit that any Modal functions will be executed remotely. See the [async guide](/docs/guide/async)\n or the examples for more information about asynchronous usage.\n\nGPU acceleration\n----------------\n\nSometimes you can speed up your applications by utilizing GPU acceleration. See the [gpu section](/docs/guide/gpu)\n for more information.\n\nLimiting concurrency\n--------------------\n\nIf you want to limit concurrency, you can use the `concurrency_limit` argument to `app.function`. For instance:\n\n    app = modal.App()\n    \n    @app.function(concurrency_limit=5)\n    def f(x):\n        print(x)\n\nCopy\n\nWith this, Modal will spin up at most 5 containers at any point.\n\n[Scaling out](#scaling-out)\n [Parallel execution of inputs](#parallel-execution-of-inputs)\n [Exceptions](#exceptions)\n [Starmap](#starmap)\n [Gotchas](#gotchas)\n [Asynchronous usage](#asynchronous-usage)\n [GPU acceleration](#gpu-acceleration)\n [Limiting concurrency](#limiting-concurrency)\n\nSee it in action\n\n[Auto-scaling LLM inference endpoints](/docs/examples/text_generation_inference)\n\n[Job queue for OCR](/docs/examples/doc_ocr_jobs)\n\n[Parallel web scraping](/docs/examples/web-scraper#scaling-out)",
    "metadata": {
      "title": "Scaling out | Modal Docs",
      "description": "Modal has a few different tools that helps with increasing performance of your applications.",
      "ogTitle": "Scaling out",
      "ogDescription": "Modal has a few different tools that helps with increasing performance of your applications.",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/scale",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nDicts and queues\n================\n\nModal provides a variety of distributed objects to enable seamless interactivity and data transfer across different components of a distributed system. Two key objects are dicts and queues, both of which serve specific roles in facilitating communication and data management in your applications.\n\nModal Dicts\n-----------\n\nA [Dict](/docs/reference/modal.Dict)\n in Modal provides distributed key-value storage. Much like a standard Python dictionary, it lets you store and retrieve values using keys. However, unlike a regular dictionary, a dict in Modal is shared across all containers of an application and can be accessed and manipulated concurrently from any of them.\n\n    import modal\n    \n    app = modal.App()\n    \n    # Create a persisted dict - the data gets retained between app runs\n    my_dict = modal.Dict.from_name(\"my-persisted-dict\", create_if_missing=True)\n    \n    \n    @app.local_entrypoint()\n    def main():\n        my_dict[\"key\"] = \"value\"  # setting a value\n        value = my_dict[\"key\"]    # getting a value\n\nCopy\n\nDicts in Modal are persisted, which means that the data in the dictionary is stored and can be retrieved later, even after the application is redeployed. They can also be accessed from other Modal functions.\n\nYou can store Python values of any type within Dicts, since they’re serialized using [cloudpickle](https://github.com/cloudpipe/cloudpickle)\n.\n\nCurrently, the ​overall​ size of a Dict is limited to 10 GiB. However, we intend to lower this limit as Dicts are not intended for caching large datasets. There is no per-object size limit, but the maximum number of entries per update request is 10,000.\n\nModal Queues\n------------\n\nA [Queue](/docs/reference/modal.Queue)\n in Modal is a distributed queue-like object. It allows you to add and retrieve items in a first-in-first-out (FIFO) manner. Queues are particularly useful when you want to handle tasks or process data asynchronously, or when you need to pass messages between different components of your distributed system.\n\n    import modal\n    \n    app = modal.App()\n    my_queue = modal.Queue.from_name(\"my-persisted-queue\", create_if_missing=True)\n    \n    \n    @app.local_entrypoint()\n    def main():\n        my_queue.put(\"some object\")  # adding a value\n        value = my_queue.get()  # retrieving a value\n\nCopy\n\nSimilar to Dicts, Queues are also persisted and support values of any type.\n\n### Queue partitions\n\nQueues are split into separate FIFO partitions via a string key. By default, one partition (corresponding to an empty key) is used.\n\nA single `Queue` can contain up to 100,000 partitions, each with up to 5,000 items. Each item can be up to 256 KiB. These limits also apply to the default partition.\n\n    from modal import Queue, App\n    \n    app = App()\n    my_queue = Queue.from_name(\"my-persisted-queue\", create_if_missing=True)\n    \n    @app.local_entrypoint()\n    def main():\n        my_queue.put(\"some value\")\n        my_queue.put(123)\n    \n        assert my_queue.get() == \"some value\"\n        assert my_queue.get() == 123\n    \n        my_queue.put(0)\n        my_queue.put(1, partition_key=\"foo\")\n        my_queue.put(2, partition_key=\"bar\")\n    \n        # Default and \"foo\" partition are ignored by the get operation.\n        assert my_queue.get(partition_key=\"bar\") == 2\n    \n        # Set custom 10s expiration time on \"foo\" partition.\n        my_queue.put(3, partition_key=\"foo\", partition_ttl=10)\n    \n        # (beta feature) Iterate through items in place (read immutably)\n        my_queue.put(1)\n        assert [v for v in my_queue.iterate()] == [0, 1]\n\nCopy\n\nBy default, each partition is cleared 24 hours after the last `put` operation. A lower TTL can be specified by the `partition_ttl` argument in the `put` or `put_many` methods. Each partition’s expiry is handled independently.\n\nAs such, `Queue`s are best used for communication between active functions and not relied on for persistent storage.\n\nAsynchronous calls\n------------------\n\nBoth Dicts and Queues are synchronous by default, but they support asynchronous interaction with the `.aio` function suffix.\n\n    @app.local_entrypoint()\n    async def main():\n        await my_queue.put.aio(100)\n        assert await my_queue.get.aio() == 100\n    \n        await my_dict.put.aio(\"hello\", 400)\n        assert await my_dict.get.aio(\"hello\") == 400\n\nCopy\n\nNote that `.put` and `.get` are aliases for the overloaded indexing operators on Dicts, and you need them name for asynchronous calls.\n\nPlease see the docs on [asynchronous functions](/docs/guide/async)\n for more information.\n\nExample: Dict and Queue Interaction\n-----------------------------------\n\nTo illustrate how dicts and queues can interact together in a simple distributed system, consider the following example program that crawls the web, starting from [wikipedia.org](https://www.wikipedia.org)\n and traversing links to many sites in breadth-first order. The Queue stores pages to crawl, while the Dict is used as a kill switch to stop execution of tasks immediately upon completion.\n\n    import queue\n    import sys\n    from datetime import datetime\n    \n    from modal import Dict, Image, Queue, App\n    \n    \n    app = App(image=Image.debian_slim().pip_install(\"requests\", \"beautifulsoup4\"))\n    \n    \n    def extract_links(url: str) -> list[str]:\n        \"\"\"Extract links from a given URL.\"\"\"\n        import requests\n        import urllib.parse\n        from bs4 import BeautifulSoup\n    \n        resp = requests.get(url, timeout=10)\n        resp.raise_for_status()\n        soup = BeautifulSoup(resp.text, \"html.parser\")\n        links = []\n        for link in soup.find_all(\"a\"):\n            links.append(urllib.parse.urljoin(url, link.get(\"href\")))\n        return links\n    \n    \n    @app.function()\n    def crawl_pages(q: Queue, d: Dict, urls: set[str]) -> None:\n        for url in urls:\n            if \"stop\" in d:\n                return\n            try:\n                s = datetime.now()\n                links = extract_links(url)\n                print(f\"Crawled: {url} in {datetime.now() - s}, with {len(links)} links\")\n                q.put_many(links)\n            except Exception as exc:\n                print(f\"Failed to crawl: {url} with error {exc}, skipping...\", file=sys.stderr)\n    \n    \n    @app.function()\n    def scrape(url: str):\n        start_time = datetime.now()\n    \n        # Create ephemeral dicts and queues\n        with Dict.ephemeral() as d, Queue.ephemeral() as q:\n            # The dict is used to signal the scraping to stop\n            # The queue contains the URLs that have been crawled\n    \n            # Initialize queue with a starting URL\n            q.put(url)\n    \n            # Crawl until the queue is empty, or reaching some number of URLs\n            visited = set()\n            max_urls = 50000\n            while True:\n                try:\n                    next_urls = q.get_many(2000, timeout=5)\n                except queue.Empty:\n                    break\n                new_urls = set(next_urls) - visited\n                visited |= new_urls\n                if len(visited) < max_urls:\n                    crawl_pages.spawn(q, d, new_urls)\n                else:\n                    d[\"stop\"] = True\n    \n            elapsed = (datetime.now() - start_time).total_seconds()\n            print(f\"Crawled {len(visited)} URLs in {elapsed:.2f} seconds\")\n    \n    \n    @app.local_entrypoint()\n    def main():\n        scrape.remote(\"https://www.wikipedia.org/\")\n\nCopy\n\nStarting from Wikipedia, this spawns several dozen containers (auto-scaled on demand) to crawl over 200,000 URLs in 40 seconds.\n\nData durability\n---------------\n\nDict and Queue objects are backed by an in-memory database, and thus are not resilient to database server restarts. Queues and Dicts are also subject to expiration, as described by the [modal.Dict](/docs/reference/modal.Dict#modaldictdict)\n and [modal.Queue](/docs/reference/modal.Queue#modalqueuequeue)\n reference pages.\n\n[Please get in touch](mailto:support@modal.com)\n if you need durability for Dict or Queue objects.\n\n[Dicts and queues](#dicts-and-queues)\n [Modal Dicts](#modal-dicts)\n [Modal Queues](#modal-queues)\n [Queue partitions](#queue-partitions)\n [Asynchronous calls](#asynchronous-calls)\n [Example: Dict and Queue Interaction](#example-dict-and-queue-interaction)\n [Data durability](#data-durability)",
    "markdown": "* * *\n\nDicts and queues\n================\n\nModal provides a variety of distributed objects to enable seamless interactivity and data transfer across different components of a distributed system. Two key objects are dicts and queues, both of which serve specific roles in facilitating communication and data management in your applications.\n\nModal Dicts\n-----------\n\nA [Dict](/docs/reference/modal.Dict)\n in Modal provides distributed key-value storage. Much like a standard Python dictionary, it lets you store and retrieve values using keys. However, unlike a regular dictionary, a dict in Modal is shared across all containers of an application and can be accessed and manipulated concurrently from any of them.\n\n    import modal\n    \n    app = modal.App()\n    \n    # Create a persisted dict - the data gets retained between app runs\n    my_dict = modal.Dict.from_name(\"my-persisted-dict\", create_if_missing=True)\n    \n    \n    @app.local_entrypoint()\n    def main():\n        my_dict[\"key\"] = \"value\"  # setting a value\n        value = my_dict[\"key\"]    # getting a value\n\nCopy\n\nDicts in Modal are persisted, which means that the data in the dictionary is stored and can be retrieved later, even after the application is redeployed. They can also be accessed from other Modal functions.\n\nYou can store Python values of any type within Dicts, since they’re serialized using [cloudpickle](https://github.com/cloudpipe/cloudpickle)\n.\n\nCurrently, the ​overall​ size of a Dict is limited to 10 GiB. However, we intend to lower this limit as Dicts are not intended for caching large datasets. There is no per-object size limit, but the maximum number of entries per update request is 10,000.\n\nModal Queues\n------------\n\nA [Queue](/docs/reference/modal.Queue)\n in Modal is a distributed queue-like object. It allows you to add and retrieve items in a first-in-first-out (FIFO) manner. Queues are particularly useful when you want to handle tasks or process data asynchronously, or when you need to pass messages between different components of your distributed system.\n\n    import modal\n    \n    app = modal.App()\n    my_queue = modal.Queue.from_name(\"my-persisted-queue\", create_if_missing=True)\n    \n    \n    @app.local_entrypoint()\n    def main():\n        my_queue.put(\"some object\")  # adding a value\n        value = my_queue.get()  # retrieving a value\n\nCopy\n\nSimilar to Dicts, Queues are also persisted and support values of any type.\n\n### Queue partitions\n\nQueues are split into separate FIFO partitions via a string key. By default, one partition (corresponding to an empty key) is used.\n\nA single `Queue` can contain up to 100,000 partitions, each with up to 5,000 items. Each item can be up to 256 KiB. These limits also apply to the default partition.\n\n    from modal import Queue, App\n    \n    app = App()\n    my_queue = Queue.from_name(\"my-persisted-queue\", create_if_missing=True)\n    \n    @app.local_entrypoint()\n    def main():\n        my_queue.put(\"some value\")\n        my_queue.put(123)\n    \n        assert my_queue.get() == \"some value\"\n        assert my_queue.get() == 123\n    \n        my_queue.put(0)\n        my_queue.put(1, partition_key=\"foo\")\n        my_queue.put(2, partition_key=\"bar\")\n    \n        # Default and \"foo\" partition are ignored by the get operation.\n        assert my_queue.get(partition_key=\"bar\") == 2\n    \n        # Set custom 10s expiration time on \"foo\" partition.\n        my_queue.put(3, partition_key=\"foo\", partition_ttl=10)\n    \n        # (beta feature) Iterate through items in place (read immutably)\n        my_queue.put(1)\n        assert [v for v in my_queue.iterate()] == [0, 1]\n\nCopy\n\nBy default, each partition is cleared 24 hours after the last `put` operation. A lower TTL can be specified by the `partition_ttl` argument in the `put` or `put_many` methods. Each partition’s expiry is handled independently.\n\nAs such, `Queue`s are best used for communication between active functions and not relied on for persistent storage.\n\nAsynchronous calls\n------------------\n\nBoth Dicts and Queues are synchronous by default, but they support asynchronous interaction with the `.aio` function suffix.\n\n    @app.local_entrypoint()\n    async def main():\n        await my_queue.put.aio(100)\n        assert await my_queue.get.aio() == 100\n    \n        await my_dict.put.aio(\"hello\", 400)\n        assert await my_dict.get.aio(\"hello\") == 400\n\nCopy\n\nNote that `.put` and `.get` are aliases for the overloaded indexing operators on Dicts, and you need them name for asynchronous calls.\n\nPlease see the docs on [asynchronous functions](/docs/guide/async)\n for more information.\n\nExample: Dict and Queue Interaction\n-----------------------------------\n\nTo illustrate how dicts and queues can interact together in a simple distributed system, consider the following example program that crawls the web, starting from [wikipedia.org](https://www.wikipedia.org)\n and traversing links to many sites in breadth-first order. The Queue stores pages to crawl, while the Dict is used as a kill switch to stop execution of tasks immediately upon completion.\n\n    import queue\n    import sys\n    from datetime import datetime\n    \n    from modal import Dict, Image, Queue, App\n    \n    \n    app = App(image=Image.debian_slim().pip_install(\"requests\", \"beautifulsoup4\"))\n    \n    \n    def extract_links(url: str) -> list[str]:\n        \"\"\"Extract links from a given URL.\"\"\"\n        import requests\n        import urllib.parse\n        from bs4 import BeautifulSoup\n    \n        resp = requests.get(url, timeout=10)\n        resp.raise_for_status()\n        soup = BeautifulSoup(resp.text, \"html.parser\")\n        links = []\n        for link in soup.find_all(\"a\"):\n            links.append(urllib.parse.urljoin(url, link.get(\"href\")))\n        return links\n    \n    \n    @app.function()\n    def crawl_pages(q: Queue, d: Dict, urls: set[str]) -> None:\n        for url in urls:\n            if \"stop\" in d:\n                return\n            try:\n                s = datetime.now()\n                links = extract_links(url)\n                print(f\"Crawled: {url} in {datetime.now() - s}, with {len(links)} links\")\n                q.put_many(links)\n            except Exception as exc:\n                print(f\"Failed to crawl: {url} with error {exc}, skipping...\", file=sys.stderr)\n    \n    \n    @app.function()\n    def scrape(url: str):\n        start_time = datetime.now()\n    \n        # Create ephemeral dicts and queues\n        with Dict.ephemeral() as d, Queue.ephemeral() as q:\n            # The dict is used to signal the scraping to stop\n            # The queue contains the URLs that have been crawled\n    \n            # Initialize queue with a starting URL\n            q.put(url)\n    \n            # Crawl until the queue is empty, or reaching some number of URLs\n            visited = set()\n            max_urls = 50000\n            while True:\n                try:\n                    next_urls = q.get_many(2000, timeout=5)\n                except queue.Empty:\n                    break\n                new_urls = set(next_urls) - visited\n                visited |= new_urls\n                if len(visited) < max_urls:\n                    crawl_pages.spawn(q, d, new_urls)\n                else:\n                    d[\"stop\"] = True\n    \n            elapsed = (datetime.now() - start_time).total_seconds()\n            print(f\"Crawled {len(visited)} URLs in {elapsed:.2f} seconds\")\n    \n    \n    @app.local_entrypoint()\n    def main():\n        scrape.remote(\"https://www.wikipedia.org/\")\n\nCopy\n\nStarting from Wikipedia, this spawns several dozen containers (auto-scaled on demand) to crawl over 200,000 URLs in 40 seconds.\n\nData durability\n---------------\n\nDict and Queue objects are backed by an in-memory database, and thus are not resilient to database server restarts. Queues and Dicts are also subject to expiration, as described by the [modal.Dict](/docs/reference/modal.Dict#modaldictdict)\n and [modal.Queue](/docs/reference/modal.Queue#modalqueuequeue)\n reference pages.\n\n[Please get in touch](mailto:support@modal.com)\n if you need durability for Dict or Queue objects.\n\n[Dicts and queues](#dicts-and-queues)\n [Modal Dicts](#modal-dicts)\n [Modal Queues](#modal-queues)\n [Queue partitions](#queue-partitions)\n [Asynchronous calls](#asynchronous-calls)\n [Example: Dict and Queue Interaction](#example-dict-and-queue-interaction)\n [Data durability](#data-durability)",
    "metadata": {
      "title": "Dicts and queues | Modal Docs",
      "description": "Modal provides a variety of distributed objects to enable seamless interactivity and data transfer across different components of a distributed system. Two key objects are dicts and queues, both of which serve specific roles in facilitating communication and data management in your applications.",
      "ogTitle": "Dicts and queues",
      "ogDescription": "Modal provides a variety of distributed objects to enable seamless interactivity and data transfer across different components of a distributed system. Two key objects are dicts and queues, both of which serve specific roles in facilitating communication and data management in your applications.",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/dicts-and-queues",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nJob processing\n==============\n\nModal can be used as a scalable job queue to handle asynchronous tasks submitted from a web app or any other Python application. This allows you to offload long-running or resource-intensive tasks to Modal, while your main application remains responsive.\n\nBasic pattern\n-------------\n\nThe basic pattern for using Modal as a job queue involves three key steps:\n\n1.  Defining and deploying the job processing function using `modal deploy`.\n2.  Submitting a job using [`modal.functions.Function.spawn()`](/docs/reference/modal.Function#spawn)\n    \n3.  Polling for the job’s result using [`modal.functions.FunctionCall.get()`](/docs/reference/modal.Function#get)\n    \n\nHere’s a simple example that you can run with `modal run my_job_queue.py`:\n\n    # my_job_queue.py\n    import modal\n    \n    app = modal.App(\"my-job-queue\")\n    \n    @app.function()\n    def process_job(data):\n        # Perform the job processing here\n        return {\"result\": data}\n    \n    def submit_job(data):\n        # Since the `process_job` function is deployed, need to first look it up\n        process_job = modal.Function.lookup(\"my-job-queue\", \"process_job\")\n        call = process_job.spawn(data)\n        return call.object_id\n    \n    def get_job_result(call_id):\n        function_call = modal.functions.FunctionCall.from_id(call_id)\n        try:\n            result = function_call.get(timeout=5)\n        except TimeoutError:\n            result = {\"result\": \"pending\"}\n        return result\n    \n    @app.local_entrypoint()\n    def main():\n        data = \"my-data\"\n    \n        # Submit the job to Modal\n        call_id = submit_job(data)\n        print(get_job_result(call_id))\n\nCopy\n\nIn this example:\n\n*   `process_job` is the Modal function that performs the actual job processing. To deploy the `process_job` function on Modal, run `modal deploy my_job_queue.py`.\n*   `submit_job` submits a new job by first looking up the deployed `process_job` function, then calling `.spawn()` with the job data. It returns the unique ID of the spawned function call.\n*   `get_job_result` attempts to retrieve the result of a previously submitted job using [`FunctionCall.from_id()`](/docs/reference/modal.Function#from_id)\n     and [`FunctionCall.get()`](/docs/reference/modal.Function#get)\n    . [`FunctionCall.get()`](/docs/reference/modal.Function#get)\n     waits indefinitely by default. It takes an optional timeout argument that specifies the maximum number of seconds to wait, which can be set to 0 to poll for an output immediately. Here, if the job hasn’t completed yet, we return a pending response.\n*   The results of a `.spawn()` are accessible via `FunctionCall.get()` for up to 1 hour after completion.\n\n[Document OCR Web App](/docs/examples/doc_ocr_webapp)\n is an example that uses this pattern.\n\nIntegration with web frameworks\n-------------------------------\n\nYou can easily integrate the job queue pattern with web frameworks like FastAPI. Here’s an example, assuming that you have already deployed `process_job` on Modal with `modal deploy` as above.\n\n    import fastapi\n    from modal import App, asgi_app\n    from modal.functions import FunctionCall\n    \n    app = App(\"fastapi-modal\")\n    web_app = fastapi.FastAPI()\n    \n    @app.function()\n    @asgi_app()\n    def fastapi_app():\n        return web_app\n    \n    @web_app.post(\"/submit\")\n    async def submit_job_endpoint(data):\n        process_job = modal.Function.lookup(\"my-job-queue\", \"process_job\")\n    \n        call = process_job.spawn(data)\n        return {\"call_id\": call.object_id}\n    \n    @web_app.get(\"/result/{call_id}\")\n    async def get_job_result_endpoint(call_id: str):\n    \n        function_call = FunctionCall.from_id(call_id)\n        try:\n            result = function_call.get(timeout=0)\n        except TimeoutError:\n            return fastapi.responses.JSONResponse(content=\"\", status_code=202)\n    \n        return result\n\nCopy\n\nIn this example:\n\n*   The `/submit` endpoint accepts job data, submits a new job using `process_job.spawn()`, and returns the job’s ID to the client.\n*   The `/result/{call_id}` endpoint allows the client to poll for the job’s result using the job ID. If the job hasn’t completed yet, it returns a 202 status code to indicate that the job is still being processed.\n\nScaling and reliability\n-----------------------\n\nModal automatically scales the job queue based on the workload, spinning up new instances as needed to process jobs concurrently. It also provides built-in reliability features like automatic retries and timeout handling.\n\nYou can customize the behavior of the job queue by configuring the `@app.function()` decorator with options like [`retries`](/docs/guide/retries#function-retries)\n, [`timeout`](/docs/guide/timeouts#timeouts)\n, and [`concurrency_limit`](/docs/guide/scale#limiting-concurrency)\n.\n\n[Job processing](#job-processing)\n [Basic pattern](#basic-pattern)\n [Integration with web frameworks](#integration-with-web-frameworks)\n [Scaling and reliability](#scaling-and-reliability)",
    "markdown": "* * *\n\nJob processing\n==============\n\nModal can be used as a scalable job queue to handle asynchronous tasks submitted from a web app or any other Python application. This allows you to offload long-running or resource-intensive tasks to Modal, while your main application remains responsive.\n\nBasic pattern\n-------------\n\nThe basic pattern for using Modal as a job queue involves three key steps:\n\n1.  Defining and deploying the job processing function using `modal deploy`.\n2.  Submitting a job using [`modal.functions.Function.spawn()`](/docs/reference/modal.Function#spawn)\n    \n3.  Polling for the job’s result using [`modal.functions.FunctionCall.get()`](/docs/reference/modal.Function#get)\n    \n\nHere’s a simple example that you can run with `modal run my_job_queue.py`:\n\n    # my_job_queue.py\n    import modal\n    \n    app = modal.App(\"my-job-queue\")\n    \n    @app.function()\n    def process_job(data):\n        # Perform the job processing here\n        return {\"result\": data}\n    \n    def submit_job(data):\n        # Since the `process_job` function is deployed, need to first look it up\n        process_job = modal.Function.lookup(\"my-job-queue\", \"process_job\")\n        call = process_job.spawn(data)\n        return call.object_id\n    \n    def get_job_result(call_id):\n        function_call = modal.functions.FunctionCall.from_id(call_id)\n        try:\n            result = function_call.get(timeout=5)\n        except TimeoutError:\n            result = {\"result\": \"pending\"}\n        return result\n    \n    @app.local_entrypoint()\n    def main():\n        data = \"my-data\"\n    \n        # Submit the job to Modal\n        call_id = submit_job(data)\n        print(get_job_result(call_id))\n\nCopy\n\nIn this example:\n\n*   `process_job` is the Modal function that performs the actual job processing. To deploy the `process_job` function on Modal, run `modal deploy my_job_queue.py`.\n*   `submit_job` submits a new job by first looking up the deployed `process_job` function, then calling `.spawn()` with the job data. It returns the unique ID of the spawned function call.\n*   `get_job_result` attempts to retrieve the result of a previously submitted job using [`FunctionCall.from_id()`](/docs/reference/modal.Function#from_id)\n     and [`FunctionCall.get()`](/docs/reference/modal.Function#get)\n    . [`FunctionCall.get()`](/docs/reference/modal.Function#get)\n     waits indefinitely by default. It takes an optional timeout argument that specifies the maximum number of seconds to wait, which can be set to 0 to poll for an output immediately. Here, if the job hasn’t completed yet, we return a pending response.\n*   The results of a `.spawn()` are accessible via `FunctionCall.get()` for up to 1 hour after completion.\n\n[Document OCR Web App](/docs/examples/doc_ocr_webapp)\n is an example that uses this pattern.\n\nIntegration with web frameworks\n-------------------------------\n\nYou can easily integrate the job queue pattern with web frameworks like FastAPI. Here’s an example, assuming that you have already deployed `process_job` on Modal with `modal deploy` as above.\n\n    import fastapi\n    from modal import App, asgi_app\n    from modal.functions import FunctionCall\n    \n    app = App(\"fastapi-modal\")\n    web_app = fastapi.FastAPI()\n    \n    @app.function()\n    @asgi_app()\n    def fastapi_app():\n        return web_app\n    \n    @web_app.post(\"/submit\")\n    async def submit_job_endpoint(data):\n        process_job = modal.Function.lookup(\"my-job-queue\", \"process_job\")\n    \n        call = process_job.spawn(data)\n        return {\"call_id\": call.object_id}\n    \n    @web_app.get(\"/result/{call_id}\")\n    async def get_job_result_endpoint(call_id: str):\n    \n        function_call = FunctionCall.from_id(call_id)\n        try:\n            result = function_call.get(timeout=0)\n        except TimeoutError:\n            return fastapi.responses.JSONResponse(content=\"\", status_code=202)\n    \n        return result\n\nCopy\n\nIn this example:\n\n*   The `/submit` endpoint accepts job data, submits a new job using `process_job.spawn()`, and returns the job’s ID to the client.\n*   The `/result/{call_id}` endpoint allows the client to poll for the job’s result using the job ID. If the job hasn’t completed yet, it returns a 202 status code to indicate that the job is still being processed.\n\nScaling and reliability\n-----------------------\n\nModal automatically scales the job queue based on the workload, spinning up new instances as needed to process jobs concurrently. It also provides built-in reliability features like automatic retries and timeout handling.\n\nYou can customize the behavior of the job queue by configuring the `@app.function()` decorator with options like [`retries`](/docs/guide/retries#function-retries)\n, [`timeout`](/docs/guide/timeouts#timeouts)\n, and [`concurrency_limit`](/docs/guide/scale#limiting-concurrency)\n.\n\n[Job processing](#job-processing)\n [Basic pattern](#basic-pattern)\n [Integration with web frameworks](#integration-with-web-frameworks)\n [Scaling and reliability](#scaling-and-reliability)",
    "metadata": {
      "title": "Job processing | Modal Docs",
      "description": "Modal can be used as a scalable job queue to handle asynchronous tasks submitted from a web app or any other Python application. This allows you to offload long-running or resource-intensive tasks to Modal, while your main application remains responsive.",
      "ogTitle": "Job processing",
      "ogDescription": "Modal can be used as a scalable job queue to handle asynchronous tasks submitted from a web app or any other Python application. This allows you to offload long-running or resource-intensive tasks to Modal, while your main application remains responsive.",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/job-queue",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nConcurrent inputs on a single container (beta)\n==============================================\n\nThis guide explores why and how to configure containers to process multiple inputs simultaneously.\n\nDefault parallelism\n-------------------\n\nModal offers beautifully simple parallelism: when there is a large backlog of inputs enqueued, the number of containers scales up automatically. This is the ideal source of parallelism in the majority of cases.\n\nWhen to use concurrent inputs\n-----------------------------\n\nThere are, however, a few cases where it is ideal to run multiple inputs on each container _concurrently_.\n\nOne use case is hosting [web applications](https://modal.com/docs/guide/webhooks#web-endpoints)\n where the endpoints are not CPU-bound - for example, making an asynchronous request to a deployed Modal function or querying a database. Only a handful of containers can handle hundreds of simultaneous requests for such applications if you allow concurrent inputs.\n\nAnother use case is to support continuous batching on GPU-accelerated containers. Frameworks such as [vLLM](/docs/examples/vllm_inference)\n allow us to push higher token throughputs by maximizing compute in each forward pass. In LLMs, this means each GPU step can generate tokens for multiple user queries; in diffusion models, you can denoise multiple images concurrently. In order to take full advantage of this, containers need to be processing multiple inputs concurrently.\n\nConfiguring concurrent inputs within a container\n------------------------------------------------\n\nTo configure functions to allow each individual container to process `n` inputs concurrently, set `allow_concurrent_inputs=n` on the function decorator.\n\nThe Modal container will execute concurrent inputs on separate threads if the function is synchronous. You must ensure that **the function implementation is thread-safe.**\n\nOn the other hand, if the function is asynchronous, the Modal container will execute the concurrent inputs on separate `asyncio` tasks, using a single thread. Allowing concurrent inputs inside an `async` function does not require the function to be thread-safe.\n\n    # Each container executes up to 10 inputs in separate threads\n    @app.function(allow_concurrent_inputs=10)\n    def sleep_sync():\n        # Function must be thread-safe\n        time.sleep(1)\n    \n    # Each container executes up to 10 inputs in separate async tasks\n    @app.function(allow_concurrent_inputs=10)\n    async def sleep_async():\n        await asyncio.sleep(1)\n\nCopy\n\nThis is an advanced feature, and you should make sure that your function satisfies the requirements outlined before proceeding with concurrent inputs.\n\nHow does autoscaling work on Modal?\n-----------------------------------\n\nTo recap, there are three different scaling parameters you can set on each function:\n\n*   **`concurrency_limit`** controls the maximum number of containers (default: None).\n*   **`keep_warm`** controls the number of “warm” containers that should be kept running, even during periods of reduced traffic (default: 0).\n*   **`allow_concurrent_inputs`** sets the capacity of _a single container_ to handle some number of simultaneous inputs (default: 1).\n\nModal uses these three parameters, as well as traffic and your `container_idle_timeout`, to determine when to create new runners or decommission old ones. This is done on a per-function basis. Each Modal function gets its own, independently scaling pool of runners.\n\nA new container is created when the _number of inputs_ exceeds the _total capacity_ of all running containers. This means that there are inputs waiting to be processed. Containers are removed when they are no longer serving traffic. For example:\n\n1.  You have a text generation endpoint with `allow_concurrent_inputs=20`, and there are 100 inputs enqueued.\n2.  Modal will scale up to 5 containers to handle the load, and each container will process 20 inputs concurrently. There are now 100 inputs running.\n3.  If another input comes in, there will now be 1 enqueued input and 100 running inputs. Modal will create a new container.\n4.  If the traffic drops, Modal will decommission containers as they become idle.\n5.  Once all inputs are processed, the containers will be terminated.\n\nOur automatic scaling is fine-grained, and containers are spawned immediately after an input is received that exceeds the current runners’ capacity.\n\n[Concurrent inputs on a single container (beta)](#concurrent-inputs-on-a-single-container-beta)\n [Default parallelism](#default-parallelism)\n [When to use concurrent inputs](#when-to-use-concurrent-inputs)\n [Configuring concurrent inputs within a container](#configuring-concurrent-inputs-within-a-container)\n [How does autoscaling work on Modal?](#how-does-autoscaling-work-on-modal)\n\nSee it in action\n\n[Single GPU serving concurrent requests](/docs/examples/vllm_inference)\n\n[Responsive web app on one low-cost container](/docs/examples/stable_diffusion_xl#a-user-interface)",
    "markdown": "* * *\n\nConcurrent inputs on a single container (beta)\n==============================================\n\nThis guide explores why and how to configure containers to process multiple inputs simultaneously.\n\nDefault parallelism\n-------------------\n\nModal offers beautifully simple parallelism: when there is a large backlog of inputs enqueued, the number of containers scales up automatically. This is the ideal source of parallelism in the majority of cases.\n\nWhen to use concurrent inputs\n-----------------------------\n\nThere are, however, a few cases where it is ideal to run multiple inputs on each container _concurrently_.\n\nOne use case is hosting [web applications](https://modal.com/docs/guide/webhooks#web-endpoints)\n where the endpoints are not CPU-bound - for example, making an asynchronous request to a deployed Modal function or querying a database. Only a handful of containers can handle hundreds of simultaneous requests for such applications if you allow concurrent inputs.\n\nAnother use case is to support continuous batching on GPU-accelerated containers. Frameworks such as [vLLM](/docs/examples/vllm_inference)\n allow us to push higher token throughputs by maximizing compute in each forward pass. In LLMs, this means each GPU step can generate tokens for multiple user queries; in diffusion models, you can denoise multiple images concurrently. In order to take full advantage of this, containers need to be processing multiple inputs concurrently.\n\nConfiguring concurrent inputs within a container\n------------------------------------------------\n\nTo configure functions to allow each individual container to process `n` inputs concurrently, set `allow_concurrent_inputs=n` on the function decorator.\n\nThe Modal container will execute concurrent inputs on separate threads if the function is synchronous. You must ensure that **the function implementation is thread-safe.**\n\nOn the other hand, if the function is asynchronous, the Modal container will execute the concurrent inputs on separate `asyncio` tasks, using a single thread. Allowing concurrent inputs inside an `async` function does not require the function to be thread-safe.\n\n    # Each container executes up to 10 inputs in separate threads\n    @app.function(allow_concurrent_inputs=10)\n    def sleep_sync():\n        # Function must be thread-safe\n        time.sleep(1)\n    \n    # Each container executes up to 10 inputs in separate async tasks\n    @app.function(allow_concurrent_inputs=10)\n    async def sleep_async():\n        await asyncio.sleep(1)\n\nCopy\n\nThis is an advanced feature, and you should make sure that your function satisfies the requirements outlined before proceeding with concurrent inputs.\n\nHow does autoscaling work on Modal?\n-----------------------------------\n\nTo recap, there are three different scaling parameters you can set on each function:\n\n*   **`concurrency_limit`** controls the maximum number of containers (default: None).\n*   **`keep_warm`** controls the number of “warm” containers that should be kept running, even during periods of reduced traffic (default: 0).\n*   **`allow_concurrent_inputs`** sets the capacity of _a single container_ to handle some number of simultaneous inputs (default: 1).\n\nModal uses these three parameters, as well as traffic and your `container_idle_timeout`, to determine when to create new runners or decommission old ones. This is done on a per-function basis. Each Modal function gets its own, independently scaling pool of runners.\n\nA new container is created when the _number of inputs_ exceeds the _total capacity_ of all running containers. This means that there are inputs waiting to be processed. Containers are removed when they are no longer serving traffic. For example:\n\n1.  You have a text generation endpoint with `allow_concurrent_inputs=20`, and there are 100 inputs enqueued.\n2.  Modal will scale up to 5 containers to handle the load, and each container will process 20 inputs concurrently. There are now 100 inputs running.\n3.  If another input comes in, there will now be 1 enqueued input and 100 running inputs. Modal will create a new container.\n4.  If the traffic drops, Modal will decommission containers as they become idle.\n5.  Once all inputs are processed, the containers will be terminated.\n\nOur automatic scaling is fine-grained, and containers are spawned immediately after an input is received that exceeds the current runners’ capacity.\n\n[Concurrent inputs on a single container (beta)](#concurrent-inputs-on-a-single-container-beta)\n [Default parallelism](#default-parallelism)\n [When to use concurrent inputs](#when-to-use-concurrent-inputs)\n [Configuring concurrent inputs within a container](#configuring-concurrent-inputs-within-a-container)\n [How does autoscaling work on Modal?](#how-does-autoscaling-work-on-modal)\n\nSee it in action\n\n[Single GPU serving concurrent requests](/docs/examples/vllm_inference)\n\n[Responsive web app on one low-cost container](/docs/examples/stable_diffusion_xl#a-user-interface)",
    "metadata": {
      "title": "Concurrent inputs on a single container (beta) | Modal Docs",
      "description": "This guide explores why and how to configure containers to process multiple inputs simultaneously.",
      "ogTitle": "Concurrent inputs on a single container (beta)",
      "ogDescription": "This guide explores why and how to configure containers to process multiple inputs simultaneously.",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/concurrent-inputs",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nSecrets\n=======\n\nSecrets provide a dictionary of environment variables for images.\n\nSecrets are a secure way to add credentials and other sensitive information to the containers your functions run in. You can create and edit secrets on [the dashboard](https://modal.com/secrets)\n, or programmatically from Python code.\n\nUsing secrets\n-------------\n\nTo inject Secrets into the container running your function, you add the `secrets=[...]` argument to your `app.function` annotation. For deployed Secrets (typically created via the Modal dashboard) you can refer to those using `Secret.from_name(secret_name)`.\n\nFor example, if you have a Secret called _secret-keys_ containing the key `MY_PASSWORD`:\n\n    import os\n    import modal\n    \n    app = modal.App()\n    \n    \n    @app.function(secrets=[modal.Secret.from_name(\"secret-keys\")])\n    def some_function():\n        secret_key = os.environ[\"MY_PASSWORD\"]\n        ...\n\nCopy\n\nEach Secret can contain multiple keys and values but you can also inject multiple Secrets, allowing you to separate Secrets into smaller reusable units:\n\n    @app.function(secrets=[\\\n        modal.Secret.from_name(\"my-secret-name\"),\\\n        modal.Secret.from_name(\"other-secret\"),\\\n    ])\n    def other_function():\n        ...\n\nCopy\n\nThe Secrets are applied in order, so key-values from later `modal.Secret` objects in the list will overwrite earlier key-values in the case of a clash. For example, if both `modal.Secret` objects above contained the key `FOO`, then the value from `\"other-secret\"` would always be present in `os.environ[\"FOO\"]`.\n\nProgrammatic creation of secrets\n--------------------------------\n\nIn addition to defining Secrets on the Modal web dashboard, you can programmatically create a Secret directly in your script and send it along to your function using `Secret.from_dict(...)`. This can be useful if you want to send Secrets from your local development machine to the remote Modal app.\n\n    import os\n    import modal\n    \n    app = modal.App()\n    \n    if modal.is_local():\n        local_secret = modal.Secret.from_dict({\"FOO\": os.environ[\"LOCAL_FOO\"]})\n    else:\n        local_secret = modal.Secret.from_dict({})\n    \n    \n    @app.function(secrets=[local_secret])\n    def some_function():\n        print(os.environ[\"FOO\"])\n\nCopy\n\nYou can also use `Secret.from_dotenv()` to load any secrets defined in an `.env` file:\n\n    @app.function(secrets=[modal.Secret.from_dotenv()])\n    def some_other_function():\n        print(os.environ[\"USERNAME\"])\n\nCopy\n\n[Secrets](#secrets)\n [Using secrets](#using-secrets)\n [Programmatic creation of secrets](#programmatic-creation-of-secrets)\n\nSee it in action\n\n[OpenAI secret for LangChain RAG](/docs/examples/potus_speech_qanda)\n\n[HuggingFace access token for gated models](/docs/examples/vllm_gemma)\n\n[Write to Google Sheets](/docs/examples/db_to_sheet)",
    "markdown": "* * *\n\nSecrets\n=======\n\nSecrets provide a dictionary of environment variables for images.\n\nSecrets are a secure way to add credentials and other sensitive information to the containers your functions run in. You can create and edit secrets on [the dashboard](https://modal.com/secrets)\n, or programmatically from Python code.\n\nUsing secrets\n-------------\n\nTo inject Secrets into the container running your function, you add the `secrets=[...]` argument to your `app.function` annotation. For deployed Secrets (typically created via the Modal dashboard) you can refer to those using `Secret.from_name(secret_name)`.\n\nFor example, if you have a Secret called _secret-keys_ containing the key `MY_PASSWORD`:\n\n    import os\n    import modal\n    \n    app = modal.App()\n    \n    \n    @app.function(secrets=[modal.Secret.from_name(\"secret-keys\")])\n    def some_function():\n        secret_key = os.environ[\"MY_PASSWORD\"]\n        ...\n\nCopy\n\nEach Secret can contain multiple keys and values but you can also inject multiple Secrets, allowing you to separate Secrets into smaller reusable units:\n\n    @app.function(secrets=[\\\n        modal.Secret.from_name(\"my-secret-name\"),\\\n        modal.Secret.from_name(\"other-secret\"),\\\n    ])\n    def other_function():\n        ...\n\nCopy\n\nThe Secrets are applied in order, so key-values from later `modal.Secret` objects in the list will overwrite earlier key-values in the case of a clash. For example, if both `modal.Secret` objects above contained the key `FOO`, then the value from `\"other-secret\"` would always be present in `os.environ[\"FOO\"]`.\n\nProgrammatic creation of secrets\n--------------------------------\n\nIn addition to defining Secrets on the Modal web dashboard, you can programmatically create a Secret directly in your script and send it along to your function using `Secret.from_dict(...)`. This can be useful if you want to send Secrets from your local development machine to the remote Modal app.\n\n    import os\n    import modal\n    \n    app = modal.App()\n    \n    if modal.is_local():\n        local_secret = modal.Secret.from_dict({\"FOO\": os.environ[\"LOCAL_FOO\"]})\n    else:\n        local_secret = modal.Secret.from_dict({})\n    \n    \n    @app.function(secrets=[local_secret])\n    def some_function():\n        print(os.environ[\"FOO\"])\n\nCopy\n\nYou can also use `Secret.from_dotenv()` to load any secrets defined in an `.env` file:\n\n    @app.function(secrets=[modal.Secret.from_dotenv()])\n    def some_other_function():\n        print(os.environ[\"USERNAME\"])\n\nCopy\n\n[Secrets](#secrets)\n [Using secrets](#using-secrets)\n [Programmatic creation of secrets](#programmatic-creation-of-secrets)\n\nSee it in action\n\n[OpenAI secret for LangChain RAG](/docs/examples/potus_speech_qanda)\n\n[HuggingFace access token for gated models](/docs/examples/vllm_gemma)\n\n[Write to Google Sheets](/docs/examples/db_to_sheet)",
    "metadata": {
      "title": "Secrets | Modal Docs",
      "description": "Secrets provide a dictionary of environment variables for images.",
      "ogTitle": "Secrets",
      "ogDescription": "Secrets provide a dictionary of environment variables for images.",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/secrets",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nEnvironment variables\n=====================\n\nRuntime environment variables\n-----------------------------\n\nThe Modal runtime sets several environment variables during initialization. The keys for these environment variables are reserved and cannot be overridden by your function configuration.\n\nThe following variables provide information about the function runtime environment:\n\n*   **`MODAL_CLOUD_PROVIDER`** — Modal executes functions across a number of cloud providers ([AWS](https://aws.amazon.com/)\n    , [GCP](https://cloud.google.com/)\n    , [OCI](https://www.oracle.com/cloud/)\n    ). This variable specifies which cloud provider the Modal function is running within.\n*   **`MODAL_ENVIRONMENT`** — The name of the [Modal Environment](/docs/guide/environments)\n     the function is running within.\n*   **`MODAL_IMAGE_ID`** — The ID of the [`modal.Image`](/docs/reference/modal.Image)\n     used by the Modal Function.\n*   **`MODAL_IS_REMOTE`** - Set to ‘1’ to indicate that the function is running in a remote container.\n*   **`MODAL_REGION`** — This will correspond to a geographic area identifier from the cloud provider associated with the function (see above). For AWS, the identifier is a “region”. For GCP it is a “zone”, and for OCI it is an “availability domain”. Example values are `us-east-1` (AWS), `us-central1` (GCP), `us-ashburn-1` (OCI).\n*   **`MODAL_TASK_ID`** — The ID of the container running the Modal Function.\n\nContainer image environment variables\n-------------------------------------\n\nThe container image layers used by a Modal Function’s `modal.Image` may set environment variables. These variables will be present within your Function’s runtime environment. For example, the [`debian_slim`](/docs/reference/modal.Image#debian_slim)\n image sets the `GPG_KEY` variable.\n\nTo override image variables or set new ones, use the [`.env`](https://modal.com/docs/reference/modal.Image#env)\n method provided by `modal.Image`.\n\n[Environment variables](#environment-variables)\n [Runtime environment variables](#runtime-environment-variables)\n [Container image environment variables](#container-image-environment-variables)",
    "markdown": "* * *\n\nEnvironment variables\n=====================\n\nRuntime environment variables\n-----------------------------\n\nThe Modal runtime sets several environment variables during initialization. The keys for these environment variables are reserved and cannot be overridden by your function configuration.\n\nThe following variables provide information about the function runtime environment:\n\n*   **`MODAL_CLOUD_PROVIDER`** — Modal executes functions across a number of cloud providers ([AWS](https://aws.amazon.com/)\n    , [GCP](https://cloud.google.com/)\n    , [OCI](https://www.oracle.com/cloud/)\n    ). This variable specifies which cloud provider the Modal function is running within.\n*   **`MODAL_ENVIRONMENT`** — The name of the [Modal Environment](/docs/guide/environments)\n     the function is running within.\n*   **`MODAL_IMAGE_ID`** — The ID of the [`modal.Image`](/docs/reference/modal.Image)\n     used by the Modal Function.\n*   **`MODAL_IS_REMOTE`** - Set to ‘1’ to indicate that the function is running in a remote container.\n*   **`MODAL_REGION`** — This will correspond to a geographic area identifier from the cloud provider associated with the function (see above). For AWS, the identifier is a “region”. For GCP it is a “zone”, and for OCI it is an “availability domain”. Example values are `us-east-1` (AWS), `us-central1` (GCP), `us-ashburn-1` (OCI).\n*   **`MODAL_TASK_ID`** — The ID of the container running the Modal Function.\n\nContainer image environment variables\n-------------------------------------\n\nThe container image layers used by a Modal Function’s `modal.Image` may set environment variables. These variables will be present within your Function’s runtime environment. For example, the [`debian_slim`](/docs/reference/modal.Image#debian_slim)\n image sets the `GPG_KEY` variable.\n\nTo override image variables or set new ones, use the [`.env`](https://modal.com/docs/reference/modal.Image#env)\n method provided by `modal.Image`.\n\n[Environment variables](#environment-variables)\n [Runtime environment variables](#runtime-environment-variables)\n [Container image environment variables](#container-image-environment-variables)",
    "metadata": {
      "title": "Environment variables | Modal Docs",
      "description": "The Modal runtime sets several environment variables during initialization. The keys for these environment variables are reserved and cannot be overridden by your function configuration.",
      "ogTitle": "Environment variables",
      "ogDescription": "The Modal runtime sets several environment variables during initialization. The keys for these environment variables are reserved and cannot be overridden by your function configuration.",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/environment_variables",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nApps, Stubs, and entrypoints\n============================\n\nAn `App` is the object that represents an application running on Modal. Every other object in Modal is attached to some [`App`](/docs/reference/modal.App#modalapp)\n, including Functions, [Secrets](/docs/guide/secrets)\n, and [Images](/docs/guide/custom-container)\n. When you [`run`](/docs/reference/cli/run)\n or [`deploy`](/docs/reference/cli/deploy)\n an `App`, it creates an ephemeral or a deployed `App`, respectively.\n\nYou can view a list of all currently running Apps on the [`apps`](/apps)\n page.\n\nApps were once Stubs\n--------------------\n\nThe `App` class in the client was previously called `Stub`. Both names are still supported, but `Stub` is an alias for `App` and will not be supported at some point in the future.\n\nEphemeral Apps\n--------------\n\nAn ephemeral App is created when you use the [`modal run`](/docs/reference/cli/run)\n CLI command, or the [`app.run`](/docs/reference/modal.App#run)\n method. This creates a temporary App that only exists for the duration of your script.\n\nEphemeral Apps are stopped automatically when the calling program exits, or when the server detects that the client is no longer connected. You can use [`--detach`](/docs/reference/cli/run)\n in order to keep an ephemeral App running even after the client exits.\n\nDeployed Apps\n-------------\n\nA deployed App is created using the [`modal deploy`](/docs/reference/cli/deploy)\n CLI command. The App is persisted indefinitely until you delete it via the [web UI](/apps)\n. Functions in a deployed App that have an attached [schedule](/docs/guide/cron)\n will be run on a schedule. Otherwise, you can invoke them manually using [web endpoints or Python](/docs/guide/trigger-deployed-functions)\n.\n\nDeployed Apps are named via the [`App`](/docs/reference/modal.App#modalapp)\n constructor. Re-deploying an existing `App` (based on the name) will update it in place.\n\nEntrypoints for ephemeral Apps\n------------------------------\n\nThe code that runs first when you `modal run` an App is called the “entrypoint”.\n\nYou can register a local entrypoint using the [`@app.local_entrypoint()`](/docs/reference/modal.App#local_entrypoint)\n decorator. You can also use a regular Modal function as an entrypoint, in which case only the code in global scope is executed locally.\n\n### Argument parsing\n\nIf your entrypoint function takes arguments with primitive types, `modal run` automatically parses them as CLI options. For example, the following function can be called with `modal run script.py --foo 1 --bar \"hello\"`:\n\n    # script.py\n    \n    @app.local_entrypoint()\n    def main(foo: int, bar: str):\n        some_modal_function.call(foo, bar)\n\nCopy\n\n### Manually specifying an entrypoint\n\nIf there is only one `local_entrypoint` registered, [`modal run script.py`](/docs/reference/cli/run)\n will automatically use it. If you have no entrypoint specified, and just one decorated Modal function, that will be used as a remote entrypoint instead. Otherwise, you can direct `modal run` to use a specific entrypoint.\n\nFor example, if you have a function decorated with [`@app.function()`](/docs/reference/modal.App#function)\n in your file:\n\n    # script.py\n    \n    @app.function()\n    def f():\n        print(\"Hello world!\")\n    \n    \n    @app.function()\n    def g():\n        print(\"Goodbye world!\")\n    \n    \n    @app.local_entrypoint()\n    def main():\n        f.remote()\n\nCopy\n\nRunning [`modal run script.py`](/docs/reference/cli/run)\n will execute the `main` function locally, which would call the `f` function remotely. However you can instead run `modal run script.py::app.f` or `modal run script.py::app.g` to execute `f` or `g` directly.\n\n[Apps, Stubs, and entrypoints](#apps-stubs-and-entrypoints)\n [Apps were once Stubs](#apps-were-once-stubs)\n [Ephemeral Apps](#ephemeral-apps)\n [Deployed Apps](#deployed-apps)\n [Entrypoints for ephemeral Apps](#entrypoints-for-ephemeral-apps)\n [Argument parsing](#argument-parsing)\n [Manually specifying an entrypoint](#manually-specifying-an-entrypoint)",
    "markdown": "* * *\n\nApps, Stubs, and entrypoints\n============================\n\nAn `App` is the object that represents an application running on Modal. Every other object in Modal is attached to some [`App`](/docs/reference/modal.App#modalapp)\n, including Functions, [Secrets](/docs/guide/secrets)\n, and [Images](/docs/guide/custom-container)\n. When you [`run`](/docs/reference/cli/run)\n or [`deploy`](/docs/reference/cli/deploy)\n an `App`, it creates an ephemeral or a deployed `App`, respectively.\n\nYou can view a list of all currently running Apps on the [`apps`](/apps)\n page.\n\nApps were once Stubs\n--------------------\n\nThe `App` class in the client was previously called `Stub`. Both names are still supported, but `Stub` is an alias for `App` and will not be supported at some point in the future.\n\nEphemeral Apps\n--------------\n\nAn ephemeral App is created when you use the [`modal run`](/docs/reference/cli/run)\n CLI command, or the [`app.run`](/docs/reference/modal.App#run)\n method. This creates a temporary App that only exists for the duration of your script.\n\nEphemeral Apps are stopped automatically when the calling program exits, or when the server detects that the client is no longer connected. You can use [`--detach`](/docs/reference/cli/run)\n in order to keep an ephemeral App running even after the client exits.\n\nDeployed Apps\n-------------\n\nA deployed App is created using the [`modal deploy`](/docs/reference/cli/deploy)\n CLI command. The App is persisted indefinitely until you delete it via the [web UI](/apps)\n. Functions in a deployed App that have an attached [schedule](/docs/guide/cron)\n will be run on a schedule. Otherwise, you can invoke them manually using [web endpoints or Python](/docs/guide/trigger-deployed-functions)\n.\n\nDeployed Apps are named via the [`App`](/docs/reference/modal.App#modalapp)\n constructor. Re-deploying an existing `App` (based on the name) will update it in place.\n\nEntrypoints for ephemeral Apps\n------------------------------\n\nThe code that runs first when you `modal run` an App is called the “entrypoint”.\n\nYou can register a local entrypoint using the [`@app.local_entrypoint()`](/docs/reference/modal.App#local_entrypoint)\n decorator. You can also use a regular Modal function as an entrypoint, in which case only the code in global scope is executed locally.\n\n### Argument parsing\n\nIf your entrypoint function takes arguments with primitive types, `modal run` automatically parses them as CLI options. For example, the following function can be called with `modal run script.py --foo 1 --bar \"hello\"`:\n\n    # script.py\n    \n    @app.local_entrypoint()\n    def main(foo: int, bar: str):\n        some_modal_function.call(foo, bar)\n\nCopy\n\n### Manually specifying an entrypoint\n\nIf there is only one `local_entrypoint` registered, [`modal run script.py`](/docs/reference/cli/run)\n will automatically use it. If you have no entrypoint specified, and just one decorated Modal function, that will be used as a remote entrypoint instead. Otherwise, you can direct `modal run` to use a specific entrypoint.\n\nFor example, if you have a function decorated with [`@app.function()`](/docs/reference/modal.App#function)\n in your file:\n\n    # script.py\n    \n    @app.function()\n    def f():\n        print(\"Hello world!\")\n    \n    \n    @app.function()\n    def g():\n        print(\"Goodbye world!\")\n    \n    \n    @app.local_entrypoint()\n    def main():\n        f.remote()\n\nCopy\n\nRunning [`modal run script.py`](/docs/reference/cli/run)\n will execute the `main` function locally, which would call the `f` function remotely. However you can instead run `modal run script.py::app.f` or `modal run script.py::app.g` to execute `f` or `g` directly.\n\n[Apps, Stubs, and entrypoints](#apps-stubs-and-entrypoints)\n [Apps were once Stubs](#apps-were-once-stubs)\n [Ephemeral Apps](#ephemeral-apps)\n [Deployed Apps](#deployed-apps)\n [Entrypoints for ephemeral Apps](#entrypoints-for-ephemeral-apps)\n [Argument parsing](#argument-parsing)\n [Manually specifying an entrypoint](#manually-specifying-an-entrypoint)",
    "metadata": {
      "title": "Apps, Stubs, and entrypoints | Modal Docs",
      "description": "An App is the object that represents an application running on Modal. Every other object in Modal is attached to some App, including Functions, Secrets, and Images. When you run or deploy an App, it creates an ephemeral or a deployed App, respectively.",
      "ogTitle": "Apps, Stubs, and entrypoints",
      "ogDescription": "An App is the object that represents an application running on Modal. Every other object in Modal is attached to some App, including Functions, Secrets, and Images. When you run or deploy an App, it creates an ephemeral or a deployed App, respectively.",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/apps",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nManaging deployments\n====================\n\nOnce you’ve finished using `modal run` or `modal serve` to iterate on your Modal code, it’s time to deploy. A Modal deployment creates and then persists an application and its objects, providing the following benefits:\n\n*   Repeated application function executions will be grouped under the deployment, aiding observability and usage tracking. Programmatically triggering lots of ephemeral app runs can clutter your web and CLI interfaces.\n*   Function calls are much faster because deployed functions are persistent and reused, not created on-demand by calls. Learn how to trigger deployed functions in [Invoking deployed functions](/docs/guide/trigger-deployed-functions)\n    .\n*   [Scheduled functions](/docs/guide/cron)\n     will continue scheduling separate from any local iteration you do, and will notify you on failure.\n*   [Web endpoints](/docs/guide/webhooks)\n     keep running when you close your laptop, and their URL address matches the deployment name.\n\nCreating deployments\n--------------------\n\nDeployments are created using the [`modal deploy` command](/docs/reference/cli/app#modal-app-list)\n.\n\n     % modal deploy whisper_pod_transcriber.main\n    ✓ Initialized. View app page at https://modal.com/apps/ap-PYc2Tb7JrkskFUI8U5w0KG.\n    ✓ Created objects.\n    ├── 🔨 Created populate_podcast_metadata.\n    ├── 🔨 Mounted /home/ubuntu/whisper_pod_transcriber at /root/whisper_pod_transcriber\n    ├── 🔨 Created fastapi_app => https://modal-labs-whisper-pod-transcriber-fastapi-app.modal.run\n    ├── 🔨 Mounted /home/ubuntu/whisper_pod_transcriber/whisper_frontend/dist at /assets\n    ├── 🔨 Created search_podcast.\n    ├── 🔨 Created refresh_index.\n    ├── 🔨 Created transcribe_segment.\n    ├── 🔨 Created transcribe_episode..\n    └── 🔨 Created fetch_episodes.\n    ✓ App deployed! 🎉\n    \n    View Deployment: https://modal.com/apps/modal-labs/whisper-pod-transcriber\n\nCopy\n\nRunning this command on an existing deployment will redeploy the app, incrementing its version. For detail on how live deployed apps transition between versions, see the [Updating deployments](#updating-deployments)\n section.\n\nDeployments can also be created using [Modal’s client library](/docs/reference/modal.runner#modalrunnerdeploy_app)\n.\n\nViewing deployments\n-------------------\n\nDeployments can be viewed either on the [apps](/apps)\n web page or by using the [`modal app list` command](/docs/reference/cli/app#modal-app-list)\n.\n\n![viewing deployments in the web interface](https://modal.com/_app/immutable/assets/view-deployments.7df98218.png)\n\nUpdating deployments\n--------------------\n\nA deployment can deploy new apps or redeploy new versions of an existing deployed app. It’s useful to understand how Modal handles the transition between versions of running deployment. Modal deployments only take a few seconds, but we still ensure things run smoothly in this short deployment period.\n\nA running deployed app will continue running and accepting requests while a deployment is happening. Existing function executions will also keep running. They will not be terminated by the deployment because they’re outdated.\n\nHowever, any existing container running the old version of the app will be marked by Modal as outdated and will become ineligible to serve new requests. These outdated containers will become idle and gracefully terminate.\n\nAny warm pool containers will also be cycled during a deployment, as the previous version’s warm pool are now outdated.\n\nStopping deployments\n--------------------\n\nDeployed apps can be stopped in the web UI by clicking the red delete button on the deployment’s app page, or alternatively by using the [`modal app stop` command](/docs/reference/cli/app#modal-app-stop)\n.\n\nStopped deployments are eventually garbage collected.\n\n[Managing deployments](#managing-deployments)\n [Creating deployments](#creating-deployments)\n [Viewing deployments](#viewing-deployments)\n [Updating deployments](#updating-deployments)\n [Stopping deployments](#stopping-deployments)",
    "markdown": "* * *\n\nManaging deployments\n====================\n\nOnce you’ve finished using `modal run` or `modal serve` to iterate on your Modal code, it’s time to deploy. A Modal deployment creates and then persists an application and its objects, providing the following benefits:\n\n*   Repeated application function executions will be grouped under the deployment, aiding observability and usage tracking. Programmatically triggering lots of ephemeral app runs can clutter your web and CLI interfaces.\n*   Function calls are much faster because deployed functions are persistent and reused, not created on-demand by calls. Learn how to trigger deployed functions in [Invoking deployed functions](/docs/guide/trigger-deployed-functions)\n    .\n*   [Scheduled functions](/docs/guide/cron)\n     will continue scheduling separate from any local iteration you do, and will notify you on failure.\n*   [Web endpoints](/docs/guide/webhooks)\n     keep running when you close your laptop, and their URL address matches the deployment name.\n\nCreating deployments\n--------------------\n\nDeployments are created using the [`modal deploy` command](/docs/reference/cli/app#modal-app-list)\n.\n\n     % modal deploy whisper_pod_transcriber.main\n    ✓ Initialized. View app page at https://modal.com/apps/ap-PYc2Tb7JrkskFUI8U5w0KG.\n    ✓ Created objects.\n    ├── 🔨 Created populate_podcast_metadata.\n    ├── 🔨 Mounted /home/ubuntu/whisper_pod_transcriber at /root/whisper_pod_transcriber\n    ├── 🔨 Created fastapi_app => https://modal-labs-whisper-pod-transcriber-fastapi-app.modal.run\n    ├── 🔨 Mounted /home/ubuntu/whisper_pod_transcriber/whisper_frontend/dist at /assets\n    ├── 🔨 Created search_podcast.\n    ├── 🔨 Created refresh_index.\n    ├── 🔨 Created transcribe_segment.\n    ├── 🔨 Created transcribe_episode..\n    └── 🔨 Created fetch_episodes.\n    ✓ App deployed! 🎉\n    \n    View Deployment: https://modal.com/apps/modal-labs/whisper-pod-transcriber\n\nCopy\n\nRunning this command on an existing deployment will redeploy the app, incrementing its version. For detail on how live deployed apps transition between versions, see the [Updating deployments](#updating-deployments)\n section.\n\nDeployments can also be created using [Modal’s client library](/docs/reference/modal.runner#modalrunnerdeploy_app)\n.\n\nViewing deployments\n-------------------\n\nDeployments can be viewed either on the [apps](/apps)\n web page or by using the [`modal app list` command](/docs/reference/cli/app#modal-app-list)\n.\n\n![viewing deployments in the web interface](https://modal.com/_app/immutable/assets/view-deployments.7df98218.png)\n\nUpdating deployments\n--------------------\n\nA deployment can deploy new apps or redeploy new versions of an existing deployed app. It’s useful to understand how Modal handles the transition between versions of running deployment. Modal deployments only take a few seconds, but we still ensure things run smoothly in this short deployment period.\n\nA running deployed app will continue running and accepting requests while a deployment is happening. Existing function executions will also keep running. They will not be terminated by the deployment because they’re outdated.\n\nHowever, any existing container running the old version of the app will be marked by Modal as outdated and will become ineligible to serve new requests. These outdated containers will become idle and gracefully terminate.\n\nAny warm pool containers will also be cycled during a deployment, as the previous version’s warm pool are now outdated.\n\nStopping deployments\n--------------------\n\nDeployed apps can be stopped in the web UI by clicking the red delete button on the deployment’s app page, or alternatively by using the [`modal app stop` command](/docs/reference/cli/app#modal-app-stop)\n.\n\nStopped deployments are eventually garbage collected.\n\n[Managing deployments](#managing-deployments)\n [Creating deployments](#creating-deployments)\n [Viewing deployments](#viewing-deployments)\n [Updating deployments](#updating-deployments)\n [Stopping deployments](#stopping-deployments)",
    "metadata": {
      "title": "Managing deployments | Modal Docs",
      "description": "Once you’ve finished using modal run or modal serve to iterate on your Modal code, it’s time to deploy. A Modal deployment creates and then persists an application and its objects, providing the following benefits:",
      "ogTitle": "Managing deployments",
      "ogDescription": "Once you’ve finished using modal run or modal serve to iterate on your Modal code, it’s time to deploy. A Modal deployment creates and then persists an application and its objects, providing the following benefits:",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/managing-deployments",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nInvoking deployed functions\n===========================\n\nModal lets you take a function created by a [deployment](/docs/guide/managing-deployments)\n and call it from other contexts.\n\nThere are two ways of invoking deployed functions. If the invoking client is running Python, then the same [Modal client library](https://pypi.org/project/modal/)\n used to write Modal code can be used. HTTPS is used if the invoking client is not running Python and therefore cannot import the Modal client library.\n\nInvoking with Python\n--------------------\n\nSome use cases for Python invocation include:\n\n*   An existing Python web server (eg. Django, Flask) wants to invoke Modal functions.\n*   You have split your product or system into multiple Modal applications that deploy independently and call each other.\n\n### Function lookup and invocation basics\n\nLet’s say you have a script `my_shared_app.py` and this script defines a Modal app with a function that computes the square of a number:\n\n    import modal\n    \n    app = modal.App(\"my-shared-app\")\n    \n    \n    @app.function()\n    def square(x: int):\n        return x ** 2\n\nCopy\n\nYou can deploy this app to create a persistent deployment:\n\n    % modal deploy shared_app.py\n    ✓ Initialized.\n    ✓ Created objects.\n    ├── 🔨 Created square.\n    ├── 🔨 Mounted /Users/erikbern/modal/shared_app.py.\n    ✓ App deployed! 🎉\n    \n    View Deployment: https://modal.com/apps/erikbern/my-shared-app\n\nCopy\n\nLet’s try to run this function from a different context. For instance, let’s fire up the Python interactive interpreter:\n\n    % python\n    Python 3.9.5 (default, May  4 2021, 03:29:30)\n    [Clang 12.0.0 (clang-1200.0.32.27)] on darwin\n    Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n    >>> import modal\n    >>> f = modal.Function.lookup(\"my-shared-app\", \"square\")\n    >>> f.remote(42)\n    1764\n    >>>\n\nCopy\n\nThis works exactly the same as a regular modal `Function` object. For example, you can `.map()` over functions invoked this way too:\n\n    >>> f = modal.Function.lookup(\"my-shared-app\", \"square\")\n    >>> f.map([1, 2, 3, 4, 5])\n    [1, 4, 9, 16, 25]\n\nCopy\n\n#### Authentication\n\nThe Modal Python SDK will read the token from `~/.modal.toml` which typically is created using `modal token new`.\n\nAnother method of providing the credentials is to set the environment variables `MODAL_TOKEN_ID` and `MODAL_TOKEN_SECRET`. If you want to call a Modal function from a context such as a web server, you can expose these environment variables to the process.\n\n#### Lookup of lifecycle functions\n\n[Lifecycle functions](/docs/guide/lifecycle-functions)\n are defined on classes, which you can look up in a different way. Consider this code:\n\n    from modal import App, enter, method\n    \n    app = App(\"my-shared-app\")\n    \n    @app.cls()\n    class MyLifecycleClass:\n        @enter()\n        def enter(self):\n            self.var = \"hello world\"\n    \n        @method()\n        def foo(self):\n            return self.var\n\nCopy\n\nLet’s say you deploy this app. You can then call the function by doing this:\n\n    >>> cls = modal.Cls.lookup(\"my-shared-app\", \"MyLifecycleClass\")\n    >>> obj = cls()  # You can pass any constructor arguments here\n    >>> obj.foo.remote()\n    'hello world'\n\nCopy\n\n### Asynchronous invocation\n\nIn certain contexts, a Modal client will need to trigger Modal functions without waiting on the result. This is done by spawning functions and receiving a [`FunctionCall`](/docs/reference/modal.Function#modalfunctionsfunctioncall)\n as a handle to the triggered execution.\n\nThe following is an example of a Flask web server (running outside Modal) which accepts model training jobs to be executed within Modal. Instead of the HTTP POST request waiting on a training job to complete, which would be infeasible, the relevant Modal function is spawned and the [`FunctionCall`](/docs/reference/modal.Function#modalfunctionsfunctioncall)\n object is stored for later polling of execution status.\n\n    from uuid import uuid4\n    from flask import Flask, jsonify, request\n    \n    app = Flask(__name__)\n    pending_jobs = {}\n    \n    ...\n    \n    @app.route(\"/jobs\", methods = [\"POST\"])\n    def create_job():\n        predict_fn = modal.Function.lookup(\"example\", \"train_model\")\n        job_id = str(uuid4())\n        function_call = predict_fn.spawn(\n            job_id=job_id,\n            params=request.json,\n        )\n        pending_jobs[job_id] = function_call\n        return {\n            \"job_id\": job_id,\n            \"status\": \"pending\",\n        }\n\nCopy\n\n### Importing a Modal function between Modal apps\n\nYou can also import one function defined in an app from another app:\n\n    import modal\n    \n    app = modal.App(\"another-app\")\n    app.square = modal.Function.from_name(\"my-shared-app\", \"square\")\n    \n    \n    @app.function()\n    def cube(x):\n        return x * app.square.remote(x)\n    \n    \n    @app.local_entrypoint()\n    def main():\n        assert cube.remote(42) == 74088\n\nCopy\n\n### Comparison with HTTPS\n\nCompared with HTTPS invocation, Python invocation has the following benefits:\n\n*   Avoids the need to create web endpoint functions.\n*   Avoids handling serialization of request and response data between Modal and your client.\n*   Uses the Modal client library’s built-in authentication.\n    *   Web endpoints are public to the entire internet, whereas function `lookup` only exposes your code to you (and your org).\n*   You can work with shared Modal functions as if they are normal Python functions, which might be more convenient.\n\nInvoking with HTTPS\n-------------------\n\nAny non-Python application client can interact with deployed Modal applications via [web endpoint functions](/docs/guide/webhooks)\n.\n\nAnything able to make HTTPS requests can trigger a Modal web endpoint function. Note that all deployed web endpoint functions have [a stable HTTPS URL](/docs/guide/webhook-urls)\n.\n\nSome use cases for HTTPS invocation include:\n\n*   Calling Modal functions from a web browser client running Javascript\n*   Calling Modal functions from non-Python backend services (Java, Go, Ruby, NodeJS, etc)\n*   Calling Modal functions using UNIX tools (`curl`, `wget`)\n\nHowever, if the client of your Modal deployment is running Python, it’s better to use the [Modal client library](https://pypi.org/project/modal/)\n to invoke your Modal code.\n\nFor more detail on setting up functions for invocation over HTTP see the [web endpoints guide](/docs/guide/webhooks)\n.\n\n[Invoking deployed functions](#invoking-deployed-functions)\n [Invoking with Python](#invoking-with-python)\n [Function lookup and invocation basics](#function-lookup-and-invocation-basics)\n [Authentication](#authentication)\n [Lookup of lifecycle functions](#lookup-of-lifecycle-functions)\n [Asynchronous invocation](#asynchronous-invocation)\n [Importing a Modal function between Modal apps](#importing-a-modal-function-between-modal-apps)\n [Comparison with HTTPS](#comparison-with-https)\n [Invoking with HTTPS](#invoking-with-https)",
    "markdown": "* * *\n\nInvoking deployed functions\n===========================\n\nModal lets you take a function created by a [deployment](/docs/guide/managing-deployments)\n and call it from other contexts.\n\nThere are two ways of invoking deployed functions. If the invoking client is running Python, then the same [Modal client library](https://pypi.org/project/modal/)\n used to write Modal code can be used. HTTPS is used if the invoking client is not running Python and therefore cannot import the Modal client library.\n\nInvoking with Python\n--------------------\n\nSome use cases for Python invocation include:\n\n*   An existing Python web server (eg. Django, Flask) wants to invoke Modal functions.\n*   You have split your product or system into multiple Modal applications that deploy independently and call each other.\n\n### Function lookup and invocation basics\n\nLet’s say you have a script `my_shared_app.py` and this script defines a Modal app with a function that computes the square of a number:\n\n    import modal\n    \n    app = modal.App(\"my-shared-app\")\n    \n    \n    @app.function()\n    def square(x: int):\n        return x ** 2\n\nCopy\n\nYou can deploy this app to create a persistent deployment:\n\n    % modal deploy shared_app.py\n    ✓ Initialized.\n    ✓ Created objects.\n    ├── 🔨 Created square.\n    ├── 🔨 Mounted /Users/erikbern/modal/shared_app.py.\n    ✓ App deployed! 🎉\n    \n    View Deployment: https://modal.com/apps/erikbern/my-shared-app\n\nCopy\n\nLet’s try to run this function from a different context. For instance, let’s fire up the Python interactive interpreter:\n\n    % python\n    Python 3.9.5 (default, May  4 2021, 03:29:30)\n    [Clang 12.0.0 (clang-1200.0.32.27)] on darwin\n    Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n    >>> import modal\n    >>> f = modal.Function.lookup(\"my-shared-app\", \"square\")\n    >>> f.remote(42)\n    1764\n    >>>\n\nCopy\n\nThis works exactly the same as a regular modal `Function` object. For example, you can `.map()` over functions invoked this way too:\n\n    >>> f = modal.Function.lookup(\"my-shared-app\", \"square\")\n    >>> f.map([1, 2, 3, 4, 5])\n    [1, 4, 9, 16, 25]\n\nCopy\n\n#### Authentication\n\nThe Modal Python SDK will read the token from `~/.modal.toml` which typically is created using `modal token new`.\n\nAnother method of providing the credentials is to set the environment variables `MODAL_TOKEN_ID` and `MODAL_TOKEN_SECRET`. If you want to call a Modal function from a context such as a web server, you can expose these environment variables to the process.\n\n#### Lookup of lifecycle functions\n\n[Lifecycle functions](/docs/guide/lifecycle-functions)\n are defined on classes, which you can look up in a different way. Consider this code:\n\n    from modal import App, enter, method\n    \n    app = App(\"my-shared-app\")\n    \n    @app.cls()\n    class MyLifecycleClass:\n        @enter()\n        def enter(self):\n            self.var = \"hello world\"\n    \n        @method()\n        def foo(self):\n            return self.var\n\nCopy\n\nLet’s say you deploy this app. You can then call the function by doing this:\n\n    >>> cls = modal.Cls.lookup(\"my-shared-app\", \"MyLifecycleClass\")\n    >>> obj = cls()  # You can pass any constructor arguments here\n    >>> obj.foo.remote()\n    'hello world'\n\nCopy\n\n### Asynchronous invocation\n\nIn certain contexts, a Modal client will need to trigger Modal functions without waiting on the result. This is done by spawning functions and receiving a [`FunctionCall`](/docs/reference/modal.Function#modalfunctionsfunctioncall)\n as a handle to the triggered execution.\n\nThe following is an example of a Flask web server (running outside Modal) which accepts model training jobs to be executed within Modal. Instead of the HTTP POST request waiting on a training job to complete, which would be infeasible, the relevant Modal function is spawned and the [`FunctionCall`](/docs/reference/modal.Function#modalfunctionsfunctioncall)\n object is stored for later polling of execution status.\n\n    from uuid import uuid4\n    from flask import Flask, jsonify, request\n    \n    app = Flask(__name__)\n    pending_jobs = {}\n    \n    ...\n    \n    @app.route(\"/jobs\", methods = [\"POST\"])\n    def create_job():\n        predict_fn = modal.Function.lookup(\"example\", \"train_model\")\n        job_id = str(uuid4())\n        function_call = predict_fn.spawn(\n            job_id=job_id,\n            params=request.json,\n        )\n        pending_jobs[job_id] = function_call\n        return {\n            \"job_id\": job_id,\n            \"status\": \"pending\",\n        }\n\nCopy\n\n### Importing a Modal function between Modal apps\n\nYou can also import one function defined in an app from another app:\n\n    import modal\n    \n    app = modal.App(\"another-app\")\n    app.square = modal.Function.from_name(\"my-shared-app\", \"square\")\n    \n    \n    @app.function()\n    def cube(x):\n        return x * app.square.remote(x)\n    \n    \n    @app.local_entrypoint()\n    def main():\n        assert cube.remote(42) == 74088\n\nCopy\n\n### Comparison with HTTPS\n\nCompared with HTTPS invocation, Python invocation has the following benefits:\n\n*   Avoids the need to create web endpoint functions.\n*   Avoids handling serialization of request and response data between Modal and your client.\n*   Uses the Modal client library’s built-in authentication.\n    *   Web endpoints are public to the entire internet, whereas function `lookup` only exposes your code to you (and your org).\n*   You can work with shared Modal functions as if they are normal Python functions, which might be more convenient.\n\nInvoking with HTTPS\n-------------------\n\nAny non-Python application client can interact with deployed Modal applications via [web endpoint functions](/docs/guide/webhooks)\n.\n\nAnything able to make HTTPS requests can trigger a Modal web endpoint function. Note that all deployed web endpoint functions have [a stable HTTPS URL](/docs/guide/webhook-urls)\n.\n\nSome use cases for HTTPS invocation include:\n\n*   Calling Modal functions from a web browser client running Javascript\n*   Calling Modal functions from non-Python backend services (Java, Go, Ruby, NodeJS, etc)\n*   Calling Modal functions using UNIX tools (`curl`, `wget`)\n\nHowever, if the client of your Modal deployment is running Python, it’s better to use the [Modal client library](https://pypi.org/project/modal/)\n to invoke your Modal code.\n\nFor more detail on setting up functions for invocation over HTTP see the [web endpoints guide](/docs/guide/webhooks)\n.\n\n[Invoking deployed functions](#invoking-deployed-functions)\n [Invoking with Python](#invoking-with-python)\n [Function lookup and invocation basics](#function-lookup-and-invocation-basics)\n [Authentication](#authentication)\n [Lookup of lifecycle functions](#lookup-of-lifecycle-functions)\n [Asynchronous invocation](#asynchronous-invocation)\n [Importing a Modal function between Modal apps](#importing-a-modal-function-between-modal-apps)\n [Comparison with HTTPS](#comparison-with-https)\n [Invoking with HTTPS](#invoking-with-https)",
    "metadata": {
      "title": "Invoking deployed functions | Modal Docs",
      "description": "Modal lets you take a function created by a deployment and call it from other contexts.",
      "ogTitle": "Invoking deployed functions",
      "ogDescription": "Modal lets you take a function created by a deployment and call it from other contexts.",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/trigger-deployed-functions",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nContinuous deployment\n=====================\n\nIt’s a common pattern to auto-deploy your Modal app as part of a CI/CD pipeline. To get you started, below is a guide to doing continuous deployment of a Modal app in GitHub.\n\nGitHub Actions\n--------------\n\nHere’s a sample GitHub Actions workflow that deploys your app on every push to the `main` branch.\n\nThis requires you to create a [Modal token](/settings/tokens)\n and add it as a [secret for your Github Actions workflow](https://github.com/Azure/actions-workflow-samples/blob/master/assets/create-secrets-for-GitHub-workflows.md)\n.\n\nAfter setting up secrets, create a new workflow file in your repository at `.github/workflows/ci-cd.yml` with the following contents:\n\n    name: CI/CD\n    \n    on:\n      push:\n        branches:\n          - main\n    \n    jobs:\n      deploy:\n        name: Deploy\n        runs-on: ubuntu-latest\n        env:\n          MODAL_TOKEN_ID: ${{ secrets.MODAL_TOKEN_ID }}\n          MODAL_TOKEN_SECRET: ${{ secrets.MODAL_TOKEN_SECRET }}\n    \n        steps:\n          - name: Checkout Repository\n            uses: actions/checkout@v4\n    \n          - name: Install Python\n            uses: actions/setup-python@v5\n            with:\n              python-version: \"3.10\"\n    \n          - name: Install Modal\n            run: |\n              python -m pip install --upgrade pip\n              pip install modal\n    \n          - name: Deploy job\n            run: |\n              modal deploy my_package.my_file\n\nCopy\n\nBe sure to replace `my_package.my_file` with your actual entrypoint.\n\nIf you use multiple Modal [Environments](/docs/guide/environments)\n, you can additionally specify the target environment in the YAML using `MODAL_ENVIRONMENT=xyz`.\n\n[Continuous deployment](#continuous-deployment)\n [GitHub Actions](#github-actions)",
    "markdown": "* * *\n\nContinuous deployment\n=====================\n\nIt’s a common pattern to auto-deploy your Modal app as part of a CI/CD pipeline. To get you started, below is a guide to doing continuous deployment of a Modal app in GitHub.\n\nGitHub Actions\n--------------\n\nHere’s a sample GitHub Actions workflow that deploys your app on every push to the `main` branch.\n\nThis requires you to create a [Modal token](/settings/tokens)\n and add it as a [secret for your Github Actions workflow](https://github.com/Azure/actions-workflow-samples/blob/master/assets/create-secrets-for-GitHub-workflows.md)\n.\n\nAfter setting up secrets, create a new workflow file in your repository at `.github/workflows/ci-cd.yml` with the following contents:\n\n    name: CI/CD\n    \n    on:\n      push:\n        branches:\n          - main\n    \n    jobs:\n      deploy:\n        name: Deploy\n        runs-on: ubuntu-latest\n        env:\n          MODAL_TOKEN_ID: ${{ secrets.MODAL_TOKEN_ID }}\n          MODAL_TOKEN_SECRET: ${{ secrets.MODAL_TOKEN_SECRET }}\n    \n        steps:\n          - name: Checkout Repository\n            uses: actions/checkout@v4\n    \n          - name: Install Python\n            uses: actions/setup-python@v5\n            with:\n              python-version: \"3.10\"\n    \n          - name: Install Modal\n            run: |\n              python -m pip install --upgrade pip\n              pip install modal\n    \n          - name: Deploy job\n            run: |\n              modal deploy my_package.my_file\n\nCopy\n\nBe sure to replace `my_package.my_file` with your actual entrypoint.\n\nIf you use multiple Modal [Environments](/docs/guide/environments)\n, you can additionally specify the target environment in the YAML using `MODAL_ENVIRONMENT=xyz`.\n\n[Continuous deployment](#continuous-deployment)\n [GitHub Actions](#github-actions)",
    "metadata": {
      "title": "Continuous deployment | Modal Docs",
      "description": "It’s a common pattern to auto-deploy your Modal app as part of a CI/CD pipeline. To get you started, below is a guide to doing continuous deployment of a Modal app in GitHub.",
      "ogTitle": "Continuous deployment",
      "ogDescription": "It’s a common pattern to auto-deploy your Modal app as part of a CI/CD pipeline. To get you started, below is a guide to doing continuous deployment of a Modal app in GitHub.",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/continuous-deployment",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nScheduling remote cron jobs\n===========================\n\nA common requirement is to perform some task at a given time every day or week automatically. Modal facilitates this through function schedules.\n\nBasic scheduling\n----------------\n\nLet’s say we have a Python module `heavy.py` with a function, `perform_heavy_computation()`.\n\n    # heavy.py\n    def perform_heavy_computation():\n        ...\n    \n    if __name__ == \"__main__\":\n        perform_heavy_computation()\n\nCopy\n\nTo schedule this function to run once per day, we create a Modal App and attach our function to it with the `@app.function` decorator and a schedule parameter:\n\n    # heavy.py\n    import modal\n    \n    app = modal.App()\n    \n    @app.function(schedule=modal.Period(days=1))\n    def perform_heavy_computation():\n        ...\n\nCopy\n\nTo activate the schedule, deploy your app, either through the CLI:\n\n    modal deploy --name daily_heavy heavy.py\n\nCopy\n\nOr programmatically:\n\n    if __name__ == \"__main__\":\n        modal.runner.deploy_app(app)\n\nCopy\n\nWhen you make changes to your function, just rerun the deploy command to overwrite the old deployment.\n\nNote that when you redeploy your function, `modal.Period` resets, and the schedule will run X hours after this most recent deployment.\n\nIf you want to run your function at a regular schedule not disturbed by deploys, `modal.Cron` (see below) is a better option.\n\nMonitoring your scheduled runs\n------------------------------\n\nTo see past execution logs for the scheduled function, go to the [Apps](https://modal.com/apps)\n section on the Modal web site.\n\nSchedules currently cannot be paused. Instead the schedule should be removed and the app redeployed. Schedules can be started manually on the app’s dashboard page, using the “run now” button.\n\nSchedule types\n--------------\n\nThere are two kinds of base schedule values - [`modal.Period`](/docs/reference/modal.Period)\n and [`modal.Cron`](/docs/reference/modal.Cron)\n.\n\n[`modal.Period`](/docs/reference/modal.Period)\n lets you specify an interval between function calls, e.g. `Period(days=1)` or `Period(hours=5)`:\n\n    \n    # runs once every 5 hours\n    @app.function(schedule=modal.Period(hours=5))\n    def perform_heavy_computation():\n        ...\n\nCopy\n\n[`modal.Cron`](/docs/reference/modal.Cron)\n gives you finer control using [cron](https://en.wikipedia.org/wiki/Cron)\n syntax:\n\n    # runs at 8 am (UTC) every Monday\n    @app.function(schedule=modal.Cron(\"0 8 * * 1\"))\n    def perform_heavy_computation():\n        ...\n\nCopy\n\nFor more details, see the API reference for [Period](/docs/reference/modal.Period)\n, [Cron](/docs/reference/modal.Cron)\n and [Function](/docs/reference/modal.Function)\n\n[Scheduling remote cron jobs](#scheduling-remote-cron-jobs)\n [Basic scheduling](#basic-scheduling)\n [Monitoring your scheduled runs](#monitoring-your-scheduled-runs)\n [Schedule types](#schedule-types)\n\nSee it in action\n\n[Hacker News Slackbot](/docs/examples/hackernews_alerts)",
    "markdown": "* * *\n\nScheduling remote cron jobs\n===========================\n\nA common requirement is to perform some task at a given time every day or week automatically. Modal facilitates this through function schedules.\n\nBasic scheduling\n----------------\n\nLet’s say we have a Python module `heavy.py` with a function, `perform_heavy_computation()`.\n\n    # heavy.py\n    def perform_heavy_computation():\n        ...\n    \n    if __name__ == \"__main__\":\n        perform_heavy_computation()\n\nCopy\n\nTo schedule this function to run once per day, we create a Modal App and attach our function to it with the `@app.function` decorator and a schedule parameter:\n\n    # heavy.py\n    import modal\n    \n    app = modal.App()\n    \n    @app.function(schedule=modal.Period(days=1))\n    def perform_heavy_computation():\n        ...\n\nCopy\n\nTo activate the schedule, deploy your app, either through the CLI:\n\n    modal deploy --name daily_heavy heavy.py\n\nCopy\n\nOr programmatically:\n\n    if __name__ == \"__main__\":\n        modal.runner.deploy_app(app)\n\nCopy\n\nWhen you make changes to your function, just rerun the deploy command to overwrite the old deployment.\n\nNote that when you redeploy your function, `modal.Period` resets, and the schedule will run X hours after this most recent deployment.\n\nIf you want to run your function at a regular schedule not disturbed by deploys, `modal.Cron` (see below) is a better option.\n\nMonitoring your scheduled runs\n------------------------------\n\nTo see past execution logs for the scheduled function, go to the [Apps](https://modal.com/apps)\n section on the Modal web site.\n\nSchedules currently cannot be paused. Instead the schedule should be removed and the app redeployed. Schedules can be started manually on the app’s dashboard page, using the “run now” button.\n\nSchedule types\n--------------\n\nThere are two kinds of base schedule values - [`modal.Period`](/docs/reference/modal.Period)\n and [`modal.Cron`](/docs/reference/modal.Cron)\n.\n\n[`modal.Period`](/docs/reference/modal.Period)\n lets you specify an interval between function calls, e.g. `Period(days=1)` or `Period(hours=5)`:\n\n    \n    # runs once every 5 hours\n    @app.function(schedule=modal.Period(hours=5))\n    def perform_heavy_computation():\n        ...\n\nCopy\n\n[`modal.Cron`](/docs/reference/modal.Cron)\n gives you finer control using [cron](https://en.wikipedia.org/wiki/Cron)\n syntax:\n\n    # runs at 8 am (UTC) every Monday\n    @app.function(schedule=modal.Cron(\"0 8 * * 1\"))\n    def perform_heavy_computation():\n        ...\n\nCopy\n\nFor more details, see the API reference for [Period](/docs/reference/modal.Period)\n, [Cron](/docs/reference/modal.Cron)\n and [Function](/docs/reference/modal.Function)\n\n[Scheduling remote cron jobs](#scheduling-remote-cron-jobs)\n [Basic scheduling](#basic-scheduling)\n [Monitoring your scheduled runs](#monitoring-your-scheduled-runs)\n [Schedule types](#schedule-types)\n\nSee it in action\n\n[Hacker News Slackbot](/docs/examples/hackernews_alerts)",
    "metadata": {
      "title": "Scheduling remote cron jobs | Modal Docs",
      "description": "A common requirement is to perform some task at a given time every day or week automatically. Modal facilitates this through function schedules.",
      "ogTitle": "Scheduling remote cron jobs",
      "ogDescription": "A common requirement is to perform some task at a given time every day or week automatically. Modal facilitates this through function schedules.",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/cron",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nWeb endpoints\n=============\n\nModal gives you a few ways to expose functions as web endpoints. You can [turn any Modal function into a web endpoint](#web_endpoint)\n with a single line of code, or you can [serve a full app](#serving-asgi-and-wsgi-apps)\n using frameworks like FastAPI, Django, or Flask.\n\n_Note that if you wish to invoke a Modal function from another Python application, you can deploy and [invoke the function](/docs/guide/trigger-deployed-functions)\n directly with our client library._\n\n@web\\_endpoint\n--------------\n\nThe easiest way to create a web endpoint from an existing function is to use the [`@modal.web_endpoint` decorator](/docs/reference/modal.web_endpoint)\n.\n\n    from modal import App, web_endpoint\n    \n    app = App()\n    \n    @app.function()\n    @web_endpoint()\n    def f():\n        return \"Hello world!\"\n\nCopy\n\nThis decorator wraps the Modal function in a [FastAPI application](#how-do-web-endpoints-run-in-the-cloud)\n.\n\n### Developing with `modal serve`\n\nYou can run this code as an ephemeral app, by running the command\n\n    modal serve server_script.py\n\nCopy\n\nWhere `server_script.py` is the file name of your code. This will create an ephemeral app for the duration of your script (until you hit Ctrl-C to stop it). It creates a temporary URL that you can use like any other REST endpoint. This URL is on the public internet.\n\nThe `modal serve` command will live-update an app when any of its supporting files change.\n\nLive updating is particularly useful when working with apps containing web endpoints, as any changes made to web endpoint handlers will show up almost immediately, without requiring a manual restart of the app.\n\n### Deploying with `modal deploy`\n\nYou can also deploy your app and create a persistent web endpoint in the cloud by running `modal deploy`:\n\n### Passing arguments to an endpoint\n\nWhen using `@web_endpoint`, you can use [query parameters](https://fastapi.tiangolo.com/tutorial/query-params/)\n which will be passed to your function as arguments. For instance\n\n    from modal import App, web_endpoint\n    \n    app = App()\n    \n    @app.function()\n    @web_endpoint()\n    def square(x: int):\n        return {\"square\": x**2}\n\nCopy\n\nIf you hit this with an urlencoded query string with the “x” param present, it will send that to the function:\n\n    $ curl https://modal-labs--web-endpoint-square-dev.modal.run?x=42\n    {\"square\":1764}\n\nCopy\n\nIf you want to use a `POST` request, you can use the `method` argument to `@web_endpoint` to set the HTTP verb. To accept any valid JSON, you can [use `Dict` as your type annotation](https://fastapi.tiangolo.com/tutorial/body-nested-models/?h=dict#bodies-of-arbitrary-dicts)\n and FastAPI will handle the rest.\n\n    from typing import Dict\n    \n    from modal import App, web_endpoint\n    \n    app = App()\n    \n    @app.function()\n    @web_endpoint(method=\"POST\")\n    def square(item: Dict):\n        return {\"square\": item['x']**2}\n\nCopy\n\nThis now creates an endpoint that takes a JSON body:\n\n    $ curl -X POST -H 'Content-Type: application/json' --data-binary '{\"x\": 42}' https://modal-labs--web-endpoint-square-dev.modal.run\n    {\"square\":1764}\n\nCopy\n\nThis is often the easiest way to get started, but note that FastAPI recommends that you use [typed Pydantic models](https://fastapi.tiangolo.com/tutorial/body/)\n in order to get automatic validation and documentation. FastAPI also lets you pass data to web endpoints in other ways, for instance as [form data](https://fastapi.tiangolo.com/tutorial/request-forms/)\n and [file uploads](https://fastapi.tiangolo.com/tutorial/request-files/)\n.\n\nHow do web endpoints run in the cloud?\n--------------------------------------\n\nNote that web endpoints, like everything else on Modal, only run when they need to. When you hit the web endpoint the first time, it will boot up the container, which might take a few seconds. Modal keeps the container alive for a short period in case there are subsequent requests. If there are a lot of requests, Modal might create more containers running in parallel.\n\nFor the shortcut `@web_endpoint` decorator, Modal wraps your function in a [FastAPI](https://fastapi.tiangolo.com/)\n application, and so functions you write need to follow the same request and response [semantics](https://fastapi.tiangolo.com/tutorial/)\n. This also means you can use all of FastAPI’s powerful features, such as Pydantic models for automatic validation, typed query and path parameters, and response types.\n\nHere’s everything together, combining Modal’s abilities to run functions in user-defined containers with the expressivity of FastAPI:\n\n    from pydantic import BaseModel\n    from fastapi.responses import HTMLResponse\n    \n    from modal import Image, App, web_endpoint\n    \n    image = Image.debian_slim().pip_install(\"boto3\")\n    app = App(image=image)\n    \n    \n    class Item(BaseModel):\n        name: str\n        qty: int = 42\n    \n    \n    @app.function()\n    @web_endpoint(method=\"POST\")\n    def f(item: Item):\n        import boto3\n        # do things with boto3...\n        return HTMLResponse(f\"<html>Hello, {item.name}!</html>\")\n\nCopy\n\nThis endpoint definition would be called like so:\n\n    curl -d '{\"name\": \"Erik\", \"qty\": 10}' \\\n        -H \"Content-Type: application/json\" \\\n        -X POST https://ecorp--web-demo-f-dev.modal.run\n\nCopy\n\nOr in Python with the [`requests`](https://pypi.org/project/requests/)\n library:\n\n    import requests\n    \n    data = {\"name\": \"Erik\", \"qty\": 10}\n    requests.post(\"https://ecorp--web-demo-f-dev.modal.run\", json=data, timeout=10.0)\n\nCopy\n\nServing ASGI and WSGI apps\n--------------------------\n\nYou can also serve any app written in an [ASGI](https://asgi.readthedocs.io/en/latest/)\n or [WSGI](https://en.wikipedia.org/wiki/Web_Server_Gateway_Interface)\n\\-compatible web framework on Modal.\n\nASGI provides support for async web frameworks. WSGI provides support for synchronous web frameworks.\n\n### ASGI\n\nFor ASGI apps, you can create a function decorated with [`@modal.asgi_app`](/docs/reference/modal.asgi_app)\n that returns a reference to your web app:\n\n    from fastapi import FastAPI, Request\n    from fastapi.responses import HTMLResponse\n    \n    from modal import Image, App, asgi_app\n    \n    web_app = FastAPI()\n    app = App()\n    \n    image = Image.debian_slim().pip_install(\"boto3\")\n    \n    \n    @web_app.post(\"/foo\")\n    async def foo(request: Request):\n        body = await request.json()\n        return body\n    \n    \n    @web_app.get(\"/bar\")\n    async def bar(arg=\"world\"):\n        return HTMLResponse(f\"<h1>Hello Fast {arg}!</h1>\")\n    \n    \n    @app.function(image=image)\n    @asgi_app()\n    def fastapi_app():\n        return web_app\n\nCopy\n\nNow, as before, when you deploy this script as a modal app, you get a URL for your app that you can use:\n\n### WSGI\n\nYou can serve WSGI apps using the [`@modal.wsgi_app`](/docs/reference/modal.wsgi_app)\n decorator:\n\n    from modal import Image, App, wsgi_app\n    \n    app = App()\n    image = Image.debian_slim().pip_install(\"flask\")\n    \n    \n    @app.function(image=image)\n    @wsgi_app()\n    def flask_app():\n        from flask import Flask, request\n    \n        web_app = Flask(__name__)\n    \n        @web_app.get(\"/\")\n        def home():\n            return \"Hello Flask World!\"\n    \n        @web_app.post(\"/echo\")\n        def echo():\n            return request.json\n    \n        return web_app\n\nCopy\n\nSee [Flask’s docs](https://flask.palletsprojects.com/en/2.1.x/deploying/asgi/)\n for more information on using Flask as a WSGI app.\n\nNon-ASGI web servers\n--------------------\n\nNot all web frameworks offer an ASGI or WSGI interface. For example, [aiohttp](https://docs.aiohttp.org/)\n and [tornado](https://www.tornadoweb.org/)\n use their own asynchronous network binding, and some libraries like [text-generation-inference](https://github.com/huggingface/text-generation-inference)\n actually expose a Rust-based HTTP server running as a subprocess.\n\nFor these cases, you can use the [`@modal.web_server`](/docs/reference/modal.web_server)\n decorator to “expose” a port on the container:\n\n    import subprocess\n    \n    from modal import App, web_server\n    \n    app = App()\n    \n    \n    @app.function()\n    @web_server(8000)\n    def my_file_server():\n        subprocess.Popen(\"python -m http.server -d / 8000\", shell=True)\n\nCopy\n\nJust like all web endpoints on Modal, this is only run on-demand. The function is executed on container startup, creating a file server at the root directory. When you hit the web endpoint URL, your request will be routed to the file server listening on port `8000`.\n\nFor `@web_server` endpoints, you need to make sure that the application binds to the external network interface, not just localhost. This usually means binding to `0.0.0.0` instead of `127.0.0.1`.\n\nSee our examples of how to serve [Streamlit](/docs/examples/serve_streamlit)\n and [ComfyUI](/docs/examples/comfyapp)\n on Modal.\n\nWebSockets\n----------\n\nFunctions annotated with `@web_server`, `@asgi_app`, or `@wsgi_app` also support the WebSocket protocol. Consult your web framework for appropriate documentation on how to use WebSockets with that library.\n\nWebSockets on Modal maintain a single function call per connection, which can be useful for keeping state around. Most of the time, you will want to set your handler function to [allow concurrent inputs](/docs/guide/concurrent-inputs)\n, which allows multiple simultaneous WebSocket connections to be handled by the same container.\n\nWe support the full WebSocket protocol as per [RFC 6455](https://www.rfc-editor.org/rfc/rfc6455)\n, but we do not yet have support for [RFC 8441](https://www.rfc-editor.org/rfc/rfc8441)\n (WebSockets over HTTP/2) or [RFC 7692](https://datatracker.ietf.org/doc/html/rfc7692)\n (`permessage-deflate` extension). WebSocket messages can be up to 2 MiB each.\n\nPerformance and scaling\n-----------------------\n\nIf you have no active containers when the web endpoint receives a request, it will experience a “cold start”. Consult the guide page on [cold start performance](/docs/guide/cold-start)\n for more information on when functions incur cold start penalties and advice how to mitigate their impact.\n\nIf your Function has `allow_current_inputs` set, multiple requests to the same endpoint may be handled by the same container. Beyond this limit, additional containers will start up to scale your App horizontally. When you reach the Function’s limit on containers, requests will queue for handling.\n\nEach workspace on Modal has a rate limit on total operations. For a new account, this is set to 200 function inputs or web endpoint requests per second, with a burst multiplier of 5 seconds. If you reach the rate limit, excess requests to web endpoints will return a [429 status code](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429)\n, and you’ll need to [get in touch](mailto:support@modal.com)\n with us about raising the limit.\n\nAuthentication\n--------------\n\nModal doesn’t have a first-class way to add authentication to web endpoints yet. However, we support standard techniques for securing web servers.\n\n### Token-based authentication\n\nThis is easy to implement in whichever framework you’re using. For example, if you’re using `@modal.web_endpoint` or `@modal.asgi_app` with FastAPI, you can validate a Bearer token like this:\n\n    from fastapi import Depends, HTTPException, status, Request\n    from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\n    \n    from modal import Secret, App, web_endpoint\n    \n    \n    app = App(\"auth-example\")\n    \n    auth_scheme = HTTPBearer()\n    \n    \n    @app.function(secrets=[Secret.from_name(\"my-web-auth-token\")])\n    @web_endpoint()\n    async def f(request: Request, token: HTTPAuthorizationCredentials = Depends(auth_scheme)):\n        import os\n    \n        print(os.environ[\"AUTH_TOKEN\"])\n    \n        if token.credentials != os.environ[\"AUTH_TOKEN\"]:\n            raise HTTPException(\n                status_code=status.HTTP_401_UNAUTHORIZED,\n                detail=\"Incorrect bearer token\",\n                headers={\"WWW-Authenticate\": \"Bearer\"},\n            )\n    \n        # Function body\n        return \"success!\"\n\nCopy\n\nThis assumes you have a [Modal secret](https://modal.com/secrets)\n named `my-web-auth-token` created, with contents `{AUTH_TOKEN: secret-random-token}`. Now, your endpoint will return a 401 status code except when you hit it with the correct `Authorization` header set (note that you have to prefix the token with `Bearer` ):\n\n    curl --header \"Authorization: Bearer secret-random-token\" https://modal-labs--auth-example-f.modal.run\n\nCopy\n\n### Client IP address\n\nYou can access the IP address of the client making the request. This can be used for geolocation, whitelists, blacklists, and rate limits.\n\n    from modal import App, web_endpoint\n    from fastapi import Request\n    \n    app = App()\n    \n    \n    @app.function()\n    @web_endpoint()\n    def get_ip_address(request: Request):\n        return f\"Your IP address is {request.client.host}\"\n\nCopy\n\n[Web endpoints](#web-endpoints)\n [@web\\_endpoint](#web_endpoint)\n [Developing with modal serve](#developing-with-modal-serve)\n [Deploying with modal deploy](#deploying-with-modal-deploy)\n [Passing arguments to an endpoint](#passing-arguments-to-an-endpoint)\n [How do web endpoints run in the cloud?](#how-do-web-endpoints-run-in-the-cloud)\n [Serving ASGI and WSGI apps](#serving-asgi-and-wsgi-apps)\n [ASGI](#asgi)\n [WSGI](#wsgi)\n [Non-ASGI web servers](#non-asgi-web-servers)\n [WebSockets](#websockets)\n [Performance and scaling](#performance-and-scaling)\n [Authentication](#authentication)\n [Token-based authentication](#token-based-authentication)\n [Client IP address](#client-ip-address)\n\nFully featured web apps\n\n[LLM Voice Chat (React)](/docs/examples/llm-voice-chat)\n\n[Stable Diffusion (Alpine)](/docs/examples/stable_diffusion_xl)\n\n[Music Generation (React)](/docs/examples/discord-musicgen)\n\n[Whisper Podcast Transcriber (React)](/docs/examples/whisper-transcriber)",
    "markdown": "* * *\n\nWeb endpoints\n=============\n\nModal gives you a few ways to expose functions as web endpoints. You can [turn any Modal function into a web endpoint](#web_endpoint)\n with a single line of code, or you can [serve a full app](#serving-asgi-and-wsgi-apps)\n using frameworks like FastAPI, Django, or Flask.\n\n_Note that if you wish to invoke a Modal function from another Python application, you can deploy and [invoke the function](/docs/guide/trigger-deployed-functions)\n directly with our client library._\n\n@web\\_endpoint\n--------------\n\nThe easiest way to create a web endpoint from an existing function is to use the [`@modal.web_endpoint` decorator](/docs/reference/modal.web_endpoint)\n.\n\n    from modal import App, web_endpoint\n    \n    app = App()\n    \n    @app.function()\n    @web_endpoint()\n    def f():\n        return \"Hello world!\"\n\nCopy\n\nThis decorator wraps the Modal function in a [FastAPI application](#how-do-web-endpoints-run-in-the-cloud)\n.\n\n### Developing with `modal serve`\n\nYou can run this code as an ephemeral app, by running the command\n\n    modal serve server_script.py\n\nCopy\n\nWhere `server_script.py` is the file name of your code. This will create an ephemeral app for the duration of your script (until you hit Ctrl-C to stop it). It creates a temporary URL that you can use like any other REST endpoint. This URL is on the public internet.\n\nThe `modal serve` command will live-update an app when any of its supporting files change.\n\nLive updating is particularly useful when working with apps containing web endpoints, as any changes made to web endpoint handlers will show up almost immediately, without requiring a manual restart of the app.\n\n### Deploying with `modal deploy`\n\nYou can also deploy your app and create a persistent web endpoint in the cloud by running `modal deploy`:\n\n### Passing arguments to an endpoint\n\nWhen using `@web_endpoint`, you can use [query parameters](https://fastapi.tiangolo.com/tutorial/query-params/)\n which will be passed to your function as arguments. For instance\n\n    from modal import App, web_endpoint\n    \n    app = App()\n    \n    @app.function()\n    @web_endpoint()\n    def square(x: int):\n        return {\"square\": x**2}\n\nCopy\n\nIf you hit this with an urlencoded query string with the “x” param present, it will send that to the function:\n\n    $ curl https://modal-labs--web-endpoint-square-dev.modal.run?x=42\n    {\"square\":1764}\n\nCopy\n\nIf you want to use a `POST` request, you can use the `method` argument to `@web_endpoint` to set the HTTP verb. To accept any valid JSON, you can [use `Dict` as your type annotation](https://fastapi.tiangolo.com/tutorial/body-nested-models/?h=dict#bodies-of-arbitrary-dicts)\n and FastAPI will handle the rest.\n\n    from typing import Dict\n    \n    from modal import App, web_endpoint\n    \n    app = App()\n    \n    @app.function()\n    @web_endpoint(method=\"POST\")\n    def square(item: Dict):\n        return {\"square\": item['x']**2}\n\nCopy\n\nThis now creates an endpoint that takes a JSON body:\n\n    $ curl -X POST -H 'Content-Type: application/json' --data-binary '{\"x\": 42}' https://modal-labs--web-endpoint-square-dev.modal.run\n    {\"square\":1764}\n\nCopy\n\nThis is often the easiest way to get started, but note that FastAPI recommends that you use [typed Pydantic models](https://fastapi.tiangolo.com/tutorial/body/)\n in order to get automatic validation and documentation. FastAPI also lets you pass data to web endpoints in other ways, for instance as [form data](https://fastapi.tiangolo.com/tutorial/request-forms/)\n and [file uploads](https://fastapi.tiangolo.com/tutorial/request-files/)\n.\n\nHow do web endpoints run in the cloud?\n--------------------------------------\n\nNote that web endpoints, like everything else on Modal, only run when they need to. When you hit the web endpoint the first time, it will boot up the container, which might take a few seconds. Modal keeps the container alive for a short period in case there are subsequent requests. If there are a lot of requests, Modal might create more containers running in parallel.\n\nFor the shortcut `@web_endpoint` decorator, Modal wraps your function in a [FastAPI](https://fastapi.tiangolo.com/)\n application, and so functions you write need to follow the same request and response [semantics](https://fastapi.tiangolo.com/tutorial/)\n. This also means you can use all of FastAPI’s powerful features, such as Pydantic models for automatic validation, typed query and path parameters, and response types.\n\nHere’s everything together, combining Modal’s abilities to run functions in user-defined containers with the expressivity of FastAPI:\n\n    from pydantic import BaseModel\n    from fastapi.responses import HTMLResponse\n    \n    from modal import Image, App, web_endpoint\n    \n    image = Image.debian_slim().pip_install(\"boto3\")\n    app = App(image=image)\n    \n    \n    class Item(BaseModel):\n        name: str\n        qty: int = 42\n    \n    \n    @app.function()\n    @web_endpoint(method=\"POST\")\n    def f(item: Item):\n        import boto3\n        # do things with boto3...\n        return HTMLResponse(f\"<html>Hello, {item.name}!</html>\")\n\nCopy\n\nThis endpoint definition would be called like so:\n\n    curl -d '{\"name\": \"Erik\", \"qty\": 10}' \\\n        -H \"Content-Type: application/json\" \\\n        -X POST https://ecorp--web-demo-f-dev.modal.run\n\nCopy\n\nOr in Python with the [`requests`](https://pypi.org/project/requests/)\n library:\n\n    import requests\n    \n    data = {\"name\": \"Erik\", \"qty\": 10}\n    requests.post(\"https://ecorp--web-demo-f-dev.modal.run\", json=data, timeout=10.0)\n\nCopy\n\nServing ASGI and WSGI apps\n--------------------------\n\nYou can also serve any app written in an [ASGI](https://asgi.readthedocs.io/en/latest/)\n or [WSGI](https://en.wikipedia.org/wiki/Web_Server_Gateway_Interface)\n\\-compatible web framework on Modal.\n\nASGI provides support for async web frameworks. WSGI provides support for synchronous web frameworks.\n\n### ASGI\n\nFor ASGI apps, you can create a function decorated with [`@modal.asgi_app`](/docs/reference/modal.asgi_app)\n that returns a reference to your web app:\n\n    from fastapi import FastAPI, Request\n    from fastapi.responses import HTMLResponse\n    \n    from modal import Image, App, asgi_app\n    \n    web_app = FastAPI()\n    app = App()\n    \n    image = Image.debian_slim().pip_install(\"boto3\")\n    \n    \n    @web_app.post(\"/foo\")\n    async def foo(request: Request):\n        body = await request.json()\n        return body\n    \n    \n    @web_app.get(\"/bar\")\n    async def bar(arg=\"world\"):\n        return HTMLResponse(f\"<h1>Hello Fast {arg}!</h1>\")\n    \n    \n    @app.function(image=image)\n    @asgi_app()\n    def fastapi_app():\n        return web_app\n\nCopy\n\nNow, as before, when you deploy this script as a modal app, you get a URL for your app that you can use:\n\n### WSGI\n\nYou can serve WSGI apps using the [`@modal.wsgi_app`](/docs/reference/modal.wsgi_app)\n decorator:\n\n    from modal import Image, App, wsgi_app\n    \n    app = App()\n    image = Image.debian_slim().pip_install(\"flask\")\n    \n    \n    @app.function(image=image)\n    @wsgi_app()\n    def flask_app():\n        from flask import Flask, request\n    \n        web_app = Flask(__name__)\n    \n        @web_app.get(\"/\")\n        def home():\n            return \"Hello Flask World!\"\n    \n        @web_app.post(\"/echo\")\n        def echo():\n            return request.json\n    \n        return web_app\n\nCopy\n\nSee [Flask’s docs](https://flask.palletsprojects.com/en/2.1.x/deploying/asgi/)\n for more information on using Flask as a WSGI app.\n\nNon-ASGI web servers\n--------------------\n\nNot all web frameworks offer an ASGI or WSGI interface. For example, [aiohttp](https://docs.aiohttp.org/)\n and [tornado](https://www.tornadoweb.org/)\n use their own asynchronous network binding, and some libraries like [text-generation-inference](https://github.com/huggingface/text-generation-inference)\n actually expose a Rust-based HTTP server running as a subprocess.\n\nFor these cases, you can use the [`@modal.web_server`](/docs/reference/modal.web_server)\n decorator to “expose” a port on the container:\n\n    import subprocess\n    \n    from modal import App, web_server\n    \n    app = App()\n    \n    \n    @app.function()\n    @web_server(8000)\n    def my_file_server():\n        subprocess.Popen(\"python -m http.server -d / 8000\", shell=True)\n\nCopy\n\nJust like all web endpoints on Modal, this is only run on-demand. The function is executed on container startup, creating a file server at the root directory. When you hit the web endpoint URL, your request will be routed to the file server listening on port `8000`.\n\nFor `@web_server` endpoints, you need to make sure that the application binds to the external network interface, not just localhost. This usually means binding to `0.0.0.0` instead of `127.0.0.1`.\n\nSee our examples of how to serve [Streamlit](/docs/examples/serve_streamlit)\n and [ComfyUI](/docs/examples/comfyapp)\n on Modal.\n\nWebSockets\n----------\n\nFunctions annotated with `@web_server`, `@asgi_app`, or `@wsgi_app` also support the WebSocket protocol. Consult your web framework for appropriate documentation on how to use WebSockets with that library.\n\nWebSockets on Modal maintain a single function call per connection, which can be useful for keeping state around. Most of the time, you will want to set your handler function to [allow concurrent inputs](/docs/guide/concurrent-inputs)\n, which allows multiple simultaneous WebSocket connections to be handled by the same container.\n\nWe support the full WebSocket protocol as per [RFC 6455](https://www.rfc-editor.org/rfc/rfc6455)\n, but we do not yet have support for [RFC 8441](https://www.rfc-editor.org/rfc/rfc8441)\n (WebSockets over HTTP/2) or [RFC 7692](https://datatracker.ietf.org/doc/html/rfc7692)\n (`permessage-deflate` extension). WebSocket messages can be up to 2 MiB each.\n\nPerformance and scaling\n-----------------------\n\nIf you have no active containers when the web endpoint receives a request, it will experience a “cold start”. Consult the guide page on [cold start performance](/docs/guide/cold-start)\n for more information on when functions incur cold start penalties and advice how to mitigate their impact.\n\nIf your Function has `allow_current_inputs` set, multiple requests to the same endpoint may be handled by the same container. Beyond this limit, additional containers will start up to scale your App horizontally. When you reach the Function’s limit on containers, requests will queue for handling.\n\nEach workspace on Modal has a rate limit on total operations. For a new account, this is set to 200 function inputs or web endpoint requests per second, with a burst multiplier of 5 seconds. If you reach the rate limit, excess requests to web endpoints will return a [429 status code](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429)\n, and you’ll need to [get in touch](mailto:support@modal.com)\n with us about raising the limit.\n\nAuthentication\n--------------\n\nModal doesn’t have a first-class way to add authentication to web endpoints yet. However, we support standard techniques for securing web servers.\n\n### Token-based authentication\n\nThis is easy to implement in whichever framework you’re using. For example, if you’re using `@modal.web_endpoint` or `@modal.asgi_app` with FastAPI, you can validate a Bearer token like this:\n\n    from fastapi import Depends, HTTPException, status, Request\n    from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\n    \n    from modal import Secret, App, web_endpoint\n    \n    \n    app = App(\"auth-example\")\n    \n    auth_scheme = HTTPBearer()\n    \n    \n    @app.function(secrets=[Secret.from_name(\"my-web-auth-token\")])\n    @web_endpoint()\n    async def f(request: Request, token: HTTPAuthorizationCredentials = Depends(auth_scheme)):\n        import os\n    \n        print(os.environ[\"AUTH_TOKEN\"])\n    \n        if token.credentials != os.environ[\"AUTH_TOKEN\"]:\n            raise HTTPException(\n                status_code=status.HTTP_401_UNAUTHORIZED,\n                detail=\"Incorrect bearer token\",\n                headers={\"WWW-Authenticate\": \"Bearer\"},\n            )\n    \n        # Function body\n        return \"success!\"\n\nCopy\n\nThis assumes you have a [Modal secret](https://modal.com/secrets)\n named `my-web-auth-token` created, with contents `{AUTH_TOKEN: secret-random-token}`. Now, your endpoint will return a 401 status code except when you hit it with the correct `Authorization` header set (note that you have to prefix the token with `Bearer` ):\n\n    curl --header \"Authorization: Bearer secret-random-token\" https://modal-labs--auth-example-f.modal.run\n\nCopy\n\n### Client IP address\n\nYou can access the IP address of the client making the request. This can be used for geolocation, whitelists, blacklists, and rate limits.\n\n    from modal import App, web_endpoint\n    from fastapi import Request\n    \n    app = App()\n    \n    \n    @app.function()\n    @web_endpoint()\n    def get_ip_address(request: Request):\n        return f\"Your IP address is {request.client.host}\"\n\nCopy\n\n[Web endpoints](#web-endpoints)\n [@web\\_endpoint](#web_endpoint)\n [Developing with modal serve](#developing-with-modal-serve)\n [Deploying with modal deploy](#deploying-with-modal-deploy)\n [Passing arguments to an endpoint](#passing-arguments-to-an-endpoint)\n [How do web endpoints run in the cloud?](#how-do-web-endpoints-run-in-the-cloud)\n [Serving ASGI and WSGI apps](#serving-asgi-and-wsgi-apps)\n [ASGI](#asgi)\n [WSGI](#wsgi)\n [Non-ASGI web servers](#non-asgi-web-servers)\n [WebSockets](#websockets)\n [Performance and scaling](#performance-and-scaling)\n [Authentication](#authentication)\n [Token-based authentication](#token-based-authentication)\n [Client IP address](#client-ip-address)\n\nFully featured web apps\n\n[LLM Voice Chat (React)](/docs/examples/llm-voice-chat)\n\n[Stable Diffusion (Alpine)](/docs/examples/stable_diffusion_xl)\n\n[Music Generation (React)](/docs/examples/discord-musicgen)\n\n[Whisper Podcast Transcriber (React)](/docs/examples/whisper-transcriber)",
    "metadata": {
      "title": "Web endpoints | Modal Docs",
      "description": "Modal gives you a few ways to expose functions as web endpoints. You can turn any Modal function into a web endpoint with a single line of code, or you can serve a full app using frameworks like FastAPI, Django, or Flask.",
      "ogTitle": "Web endpoints",
      "ogDescription": "Modal gives you a few ways to expose functions as web endpoints. You can turn any Modal function into a web endpoint with a single line of code, or you can serve a full app using frameworks like FastAPI, Django, or Flask.",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/webhooks",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nStreaming endpoints\n===================\n\nModal web endpoints support streaming responses using FastAPI’s [`StreamingResponse`](https://fastapi.tiangolo.com/advanced/custom-response/#streamingresponse)\n class. This class accepts asynchronous generators, synchronous generators, or any Python object that implements the [_iterator protocol_](https://docs.python.org/3/library/stdtypes.html#typeiter)\n, and can be used with Modal Functions!\n\nSimple example\n--------------\n\nThis simple example combines Modal’s `@web_endpoint` decorator with a `StreamingResponse` object to produce a real-time SSE response.\n\n    import time\n    from fastapi.responses import StreamingResponse\n    \n    from modal import App, web_endpoint\n    \n    app = App()\n    \n    def fake_event_streamer():\n        for i in range(10):\n            yield f\"data: some data {i}\\n\\n\".encode()\n            time.sleep(0.5)\n    \n    \n    @app.function()\n    @web_endpoint()\n    def stream_me():\n        return StreamingResponse(\n            fake_event_streamer(), media_type=\"text/event-stream\"\n        )\n\nCopy\n\nIf you serve this web endpoint and hit it with `curl`, you will see the ten SSE events progressively appear in your terminal over a ~5 second period.\n\n    curl --no-buffer https://modal-labs--example-streaming-stream-me.modal.run\n\nCopy\n\nThe MIME type of `text/event-stream` is important in this example, as it tells the downstream web server to return responses immediately, rather than buffering them in byte chunks (which is more efficient for compression).\n\nYou can still return other content types like large files in streams, but they are not guaranteed to arrive as real-time events.\n\nStreaming responses with `.remote`\n----------------------------------\n\nA Modal Function wrapping a generator function body can have its response passed directly into a `StreamingResponse`. This is particularly useful if you want to do some GPU processing in one Modal Function that is called by a CPU-based web endpoint Modal Function.\n\n    from fastapi.responses import StreamingResponse\n    \n    from modal import App, web_endpoint\n    \n    app = App()\n    \n    @app.function(gpu=\"any\")\n    def fake_video_render():\n        for i in range(10):\n            yield f\"data: finished processing some data from GPU {i}\\n\\n\".encode()\n            time.sleep(1)\n    \n    \n    @app.function()\n    @web_endpoint()\n    def hook():\n        return StreamingResponse(\n            fake_video_render.remote(), media_type=\"text/event-stream\"\n        )\n\nCopy\n\nStreaming responses with `.map` and `.starmap`\n----------------------------------------------\n\nYou can also combine Modal Function parallelization with streaming responses, enabling applications to service a request by farming out to dozens of containers and iteratively returning result chunks to the client.\n\n    from fastapi.responses import StreamingResponse\n    \n    from modal import App, web_endpoint\n    \n    app = App()\n    \n    @app.function()\n    def map_me(i):\n        return f\"segment {i}\\n\"\n    \n    \n    @app.function()\n    @web_endpoint()\n    def mapped():\n        return StreamingResponse(map_me.map(range(10)), media_type=\"text/plain\")\n\nCopy\n\nThis snippet will spread the ten `map_me(i)` executions across containers, and return each string response part as it completes. By default the results will be ordered, but if this isn’t necessary pass `order_outputs=False` as keyword argument to the `.map` call.\n\n### Asynchronous streaming\n\nThe example above uses a synchronous generator, which automatically runs on its own thread, but in asynchronous applications, a loop over a `.map` or `.starmap` call can block the event loop. This will stop the `StreamingResponse` from returning response parts iteratively to the client.\n\nTo avoid this, you can use the `.aio()` method to convert a synchronous `.map` into its async version. Also, other blocking calls should be offloaded to a separate thread with `asyncio.to_thread()`. For example:\n\n    @app.function(gpu=\"any\")\n    @web_endpoint()\n    async def transcribe_video(request):\n        segments = await asyncio.to_thread(split_video, request)\n        return StreamingResponse(wrapper(segments), media_type=\"text/event-stream\")\n    \n    \n    # Notice that this is an async generator.\n    async def wrapper(segments):\n        async for partial_result in transcribe_video.map.aio(segments):\n            yield \"data: \" + partial_result + \"\\n\\n\"\n\nCopy\n\nFurther examples\n----------------\n\n*   Complete code the for the simple examples given above is available [in our modal-examples Github repository](https://github.com/modal-labs/modal-examples/blob/main/07_web_endpoints/streaming.py)\n    .\n*   [An end-to-end example of streaming Youtube video transcriptions with OpenAI’s whisper model.](https://github.com/modal-labs/modal-examples/blob/main/06_gpu_and_ml/openai_whisper/streaming/main.py)\n    \n\n[Streaming endpoints](#streaming-endpoints)\n [Simple example](#simple-example)\n [Streaming responses with .remote](#streaming-responses-with-remote)\n [Streaming responses with .map and .starmap](#streaming-responses-with-map-and-starmap)\n [Asynchronous streaming](#asynchronous-streaming)\n [Further examples](#further-examples)\n\nSee it in action\n\n[LLM Voice Chat](/docs/examples/llm-voice-chat)\n\n[Text Generation Inference](/docs/examples/text_generation_inference)",
    "markdown": "* * *\n\nStreaming endpoints\n===================\n\nModal web endpoints support streaming responses using FastAPI’s [`StreamingResponse`](https://fastapi.tiangolo.com/advanced/custom-response/#streamingresponse)\n class. This class accepts asynchronous generators, synchronous generators, or any Python object that implements the [_iterator protocol_](https://docs.python.org/3/library/stdtypes.html#typeiter)\n, and can be used with Modal Functions!\n\nSimple example\n--------------\n\nThis simple example combines Modal’s `@web_endpoint` decorator with a `StreamingResponse` object to produce a real-time SSE response.\n\n    import time\n    from fastapi.responses import StreamingResponse\n    \n    from modal import App, web_endpoint\n    \n    app = App()\n    \n    def fake_event_streamer():\n        for i in range(10):\n            yield f\"data: some data {i}\\n\\n\".encode()\n            time.sleep(0.5)\n    \n    \n    @app.function()\n    @web_endpoint()\n    def stream_me():\n        return StreamingResponse(\n            fake_event_streamer(), media_type=\"text/event-stream\"\n        )\n\nCopy\n\nIf you serve this web endpoint and hit it with `curl`, you will see the ten SSE events progressively appear in your terminal over a ~5 second period.\n\n    curl --no-buffer https://modal-labs--example-streaming-stream-me.modal.run\n\nCopy\n\nThe MIME type of `text/event-stream` is important in this example, as it tells the downstream web server to return responses immediately, rather than buffering them in byte chunks (which is more efficient for compression).\n\nYou can still return other content types like large files in streams, but they are not guaranteed to arrive as real-time events.\n\nStreaming responses with `.remote`\n----------------------------------\n\nA Modal Function wrapping a generator function body can have its response passed directly into a `StreamingResponse`. This is particularly useful if you want to do some GPU processing in one Modal Function that is called by a CPU-based web endpoint Modal Function.\n\n    from fastapi.responses import StreamingResponse\n    \n    from modal import App, web_endpoint\n    \n    app = App()\n    \n    @app.function(gpu=\"any\")\n    def fake_video_render():\n        for i in range(10):\n            yield f\"data: finished processing some data from GPU {i}\\n\\n\".encode()\n            time.sleep(1)\n    \n    \n    @app.function()\n    @web_endpoint()\n    def hook():\n        return StreamingResponse(\n            fake_video_render.remote(), media_type=\"text/event-stream\"\n        )\n\nCopy\n\nStreaming responses with `.map` and `.starmap`\n----------------------------------------------\n\nYou can also combine Modal Function parallelization with streaming responses, enabling applications to service a request by farming out to dozens of containers and iteratively returning result chunks to the client.\n\n    from fastapi.responses import StreamingResponse\n    \n    from modal import App, web_endpoint\n    \n    app = App()\n    \n    @app.function()\n    def map_me(i):\n        return f\"segment {i}\\n\"\n    \n    \n    @app.function()\n    @web_endpoint()\n    def mapped():\n        return StreamingResponse(map_me.map(range(10)), media_type=\"text/plain\")\n\nCopy\n\nThis snippet will spread the ten `map_me(i)` executions across containers, and return each string response part as it completes. By default the results will be ordered, but if this isn’t necessary pass `order_outputs=False` as keyword argument to the `.map` call.\n\n### Asynchronous streaming\n\nThe example above uses a synchronous generator, which automatically runs on its own thread, but in asynchronous applications, a loop over a `.map` or `.starmap` call can block the event loop. This will stop the `StreamingResponse` from returning response parts iteratively to the client.\n\nTo avoid this, you can use the `.aio()` method to convert a synchronous `.map` into its async version. Also, other blocking calls should be offloaded to a separate thread with `asyncio.to_thread()`. For example:\n\n    @app.function(gpu=\"any\")\n    @web_endpoint()\n    async def transcribe_video(request):\n        segments = await asyncio.to_thread(split_video, request)\n        return StreamingResponse(wrapper(segments), media_type=\"text/event-stream\")\n    \n    \n    # Notice that this is an async generator.\n    async def wrapper(segments):\n        async for partial_result in transcribe_video.map.aio(segments):\n            yield \"data: \" + partial_result + \"\\n\\n\"\n\nCopy\n\nFurther examples\n----------------\n\n*   Complete code the for the simple examples given above is available [in our modal-examples Github repository](https://github.com/modal-labs/modal-examples/blob/main/07_web_endpoints/streaming.py)\n    .\n*   [An end-to-end example of streaming Youtube video transcriptions with OpenAI’s whisper model.](https://github.com/modal-labs/modal-examples/blob/main/06_gpu_and_ml/openai_whisper/streaming/main.py)\n    \n\n[Streaming endpoints](#streaming-endpoints)\n [Simple example](#simple-example)\n [Streaming responses with .remote](#streaming-responses-with-remote)\n [Streaming responses with .map and .starmap](#streaming-responses-with-map-and-starmap)\n [Asynchronous streaming](#asynchronous-streaming)\n [Further examples](#further-examples)\n\nSee it in action\n\n[LLM Voice Chat](/docs/examples/llm-voice-chat)\n\n[Text Generation Inference](/docs/examples/text_generation_inference)",
    "metadata": {
      "title": "Streaming endpoints | Modal Docs",
      "description": "Modal web endpoints support streaming responses using FastAPI’s StreamingResponse class. This class accepts asynchronous generators, synchronous generators, or any Python object that implements the iterator protocol, and can be used with Modal Functions!",
      "ogTitle": "Streaming endpoints",
      "ogDescription": "Modal web endpoints support streaming responses using FastAPI’s StreamingResponse class. This class accepts asynchronous generators, synchronous generators, or any Python object that implements the iterator protocol, and can be used with Modal Functions!",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/streaming-endpoints",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nWeb endpoint URLs\n=================\n\nModal will automatically assign URLs for webhooks, but this can be configured in several different ways.\n\nAuto-generated URLs\n-------------------\n\nBy default, webhooks created by the [web\\_endpoint](/docs/reference/modal.web_endpoint)\n, [asgi\\_app](/docs/reference/modal.asgi_app)\n or [wsgi\\_app](/docs/reference/modal.wsgi_app)\n decorators will be served from the `modal.run` domain. The full URL will be constructed from a number of pieces of information to uniquely identify the endpoint.\n\nAt a high-level, web endpoint URLs for deployed applications have the following structure: `https://<source>--<label>.modal.run`.\n\nThe `source` component represents the workspace and environment where the App is deployed. If your workspace has only a single environment, the `source` will just be the workspace name. Multiple environments are disambiguated by an [“environment suffix”](/docs/guide/environments#environment-web-suffixes)\n, so the full source would be `<workspace>-<suffix>`. However, one environment per workspace is allowed to have a null suffix, in which case the source would just be `<workspace>`.\n\nThe `label` component represents the specific App and Function that the endpoint routes to. By default, these are concatenated with a hyphen, so the label would be `<app>-<function>`.\n\nThese components are normalized to contain only lowercase letters, numerals, and dashes.\n\nTo put this all together, consider the following example. If a member of the `ECorp` workspace uses the `main` environment (which has `prod` as its web suffix) to deploy the `text_to_speech` app with a webhook for the `flask-app` function, the URL will have the following components:\n\n*   _Source_:\n    *   _Workspace name slug_: `ECorp` → `ecorp`\n    *   _Environment web suffix slug_: `main` → `prod`\n*   _Label_:\n    *   _App name slug_: `text_to_speech` → `text-to-speech`\n    *   _Function name slug_: `flask_app` → `flask-app`\n\nThe full URL will be `https://ecorp-prod--text-to-speech-flask-app.modal.run`.\n\nUser-specified labels\n---------------------\n\nIt’s also possible to customize the `label` used for each Function by passing a parameter to the relevant endpoint decorator:\n\n    from modal import App, web_endpoint\n    \n    app = App(name=\"text_to_speech\")\n    \n    @app.function()\n    @web_endpoint(label=\"speechify\")\n    def flask_app():\n        ...\n\nCopy\n\nBuilding on the example above, this code would produce the following URL: `https://ecorp-prod--speechify.modal.run`.\n\nUser-specified labels are not automatically normalized, but labels with invalid characters will be rejected.\n\nEphemeral apps\n--------------\n\nTo support development workflows, webhooks for ephemeral apps (i.e., apps created with `modal serve`) will have a `-dev` suffix appended to their URL label (regardless of whether the label is auto-generated or user-specified). This prevents development work from interfering with deployed versions of the same app.\n\nIf an emphemeral app is serving a webhook while another ephemeral webhook is created seeking the same web endpoint label, the new function will _steal_ the running webhook’s label.\n\nThis ensures that the latest iteration of the ephemeral function is serving requests and that older ones stop recieving web traffic.\n\nTruncation\n----------\n\nIf a generated subdomain label is longer than 63 characters, it will be truncated.\n\nFor example, the following subdomain label is too long, 67 characters: `ecorp--text-to-speech-really-really-realllly-long-function-name-dev`.\n\nThe truncation happens by calculating a SHA-256 hash of the overlong label, then taking the first 6 characters of this hash. The overlong subdomain label is truncated to 56 characters, and then joined by a dash to the hash prefix. In the above example, the resulting URL would be `ecorp--text-to-speech-really-really-rea-1b964b-dev.modal.run`.\n\nThe combination of the label hashing and truncation provides a unique list of 63 characters, complying with both DNS system limits and uniqueness requirements.\n\nCustom domains\n--------------\n\n**Custom domains are available on our [Team and Enterprise plans](/settings/plans)\n.**\n\nFor more customization, you can use your own domain names with Modal web endpoints. If your [plan](/pricing)\n supports custom domains, visit the [Domains tab](/settings/domains)\n in your workspace settings to add a domain name to your workspace.\n\nYou can use three kinds of domains with Modal:\n\n*   **Apex:** root domain names like `example.com`\n*   **Subdomain:** single subdomain entries such as `my-app.example.com`, `api.example.com`, etc.\n*   **Wildcard domain:** either in a subdomain like `*.example.com`, or in a deeper level like `*.modal.example.com`\n\nYou’ll be asked to update your domain DNS records with your domain name registrar and then validate the configuration in Modal. Once the records have been properly updated and propagated, your custom domain will be ready to use.\n\nYou can assign any Modal web endpoint to any registered domain in your workspace with the `custom_domains` argument.\n\n    from modal import App, web_endpoint\n    \n    app = App(\"custom-domains-example\")\n    \n    \n    @app.function()\n    @web_endpoint(custom_domains=[\"api.example.com\"])\n    def hello(message: str):\n        return {\"message\": f\"hello {message}\"}\n\nCopy\n\nYou can then run `modal deploy` to put your web endpoint online, live.\n\n    $ curl -s https://api.example.com?message=world\n    {\"message\": \"hello world\"}\n\nCopy\n\nNote that Modal automatically generates and renews TLS certificates for your custom domains. Since we do this when your domain is first accessed, there may be an additional 1-2s latency on the first request. Additional requests use a cached certificate.\n\nYou can also register multiple domain names and associate them with the same web endpoint.\n\n    from modal import App, web_endpoint\n    \n    app = App(\"custom-domains-example-2\")\n    \n    \n    @app.function()\n    @web_endpoint(custom_domains=[\"api.example.com\", \"api.example.net\"])\n    def hello(message: str):\n        return {\"message\": f\"hello {message}\"}\n\nCopy\n\nFor **Wildcard** domains, Modal will automatically resolve arbitrary custom endpoints (and issue TLS certificates). For example, if you add the wildcard domain `*.example.com`, then you can create any custom domains under `example.com`:\n\n    import random\n    from modal import App, web_endpoint\n    \n    app = App(\"custom-domains-example-2\")\n    \n    random_domain_name = random.choice(range(10))\n    \n    \n    @app.function()\n    @web_endpoint(custom_domains=[f\"{random_domain_name}.example.com\"])\n    def hello(message: str):\n        return {\"message\": f\"hello {message}\"}\n\nCopy\n\nCustom domains can also be used with [ASGI](https://modal.com/docs/reference/modal.asgi_app#modalasgi_app)\n or [WSGI](https://modal.com/docs/reference/modal.wsgi_app)\n apps using the same `custom_domains` argument.\n\n[Web endpoint URLs](#web-endpoint-urls)\n [Auto-generated URLs](#auto-generated-urls)\n [User-specified labels](#user-specified-labels)\n [Ephemeral apps](#ephemeral-apps)\n [Truncation](#truncation)\n [Custom domains](#custom-domains)\n\nSee it in action\n\n[Custom URL for LLM frontend](/docs/examples/text_generation_inference)",
    "markdown": "* * *\n\nWeb endpoint URLs\n=================\n\nModal will automatically assign URLs for webhooks, but this can be configured in several different ways.\n\nAuto-generated URLs\n-------------------\n\nBy default, webhooks created by the [web\\_endpoint](/docs/reference/modal.web_endpoint)\n, [asgi\\_app](/docs/reference/modal.asgi_app)\n or [wsgi\\_app](/docs/reference/modal.wsgi_app)\n decorators will be served from the `modal.run` domain. The full URL will be constructed from a number of pieces of information to uniquely identify the endpoint.\n\nAt a high-level, web endpoint URLs for deployed applications have the following structure: `https://<source>--<label>.modal.run`.\n\nThe `source` component represents the workspace and environment where the App is deployed. If your workspace has only a single environment, the `source` will just be the workspace name. Multiple environments are disambiguated by an [“environment suffix”](/docs/guide/environments#environment-web-suffixes)\n, so the full source would be `<workspace>-<suffix>`. However, one environment per workspace is allowed to have a null suffix, in which case the source would just be `<workspace>`.\n\nThe `label` component represents the specific App and Function that the endpoint routes to. By default, these are concatenated with a hyphen, so the label would be `<app>-<function>`.\n\nThese components are normalized to contain only lowercase letters, numerals, and dashes.\n\nTo put this all together, consider the following example. If a member of the `ECorp` workspace uses the `main` environment (which has `prod` as its web suffix) to deploy the `text_to_speech` app with a webhook for the `flask-app` function, the URL will have the following components:\n\n*   _Source_:\n    *   _Workspace name slug_: `ECorp` → `ecorp`\n    *   _Environment web suffix slug_: `main` → `prod`\n*   _Label_:\n    *   _App name slug_: `text_to_speech` → `text-to-speech`\n    *   _Function name slug_: `flask_app` → `flask-app`\n\nThe full URL will be `https://ecorp-prod--text-to-speech-flask-app.modal.run`.\n\nUser-specified labels\n---------------------\n\nIt’s also possible to customize the `label` used for each Function by passing a parameter to the relevant endpoint decorator:\n\n    from modal import App, web_endpoint\n    \n    app = App(name=\"text_to_speech\")\n    \n    @app.function()\n    @web_endpoint(label=\"speechify\")\n    def flask_app():\n        ...\n\nCopy\n\nBuilding on the example above, this code would produce the following URL: `https://ecorp-prod--speechify.modal.run`.\n\nUser-specified labels are not automatically normalized, but labels with invalid characters will be rejected.\n\nEphemeral apps\n--------------\n\nTo support development workflows, webhooks for ephemeral apps (i.e., apps created with `modal serve`) will have a `-dev` suffix appended to their URL label (regardless of whether the label is auto-generated or user-specified). This prevents development work from interfering with deployed versions of the same app.\n\nIf an emphemeral app is serving a webhook while another ephemeral webhook is created seeking the same web endpoint label, the new function will _steal_ the running webhook’s label.\n\nThis ensures that the latest iteration of the ephemeral function is serving requests and that older ones stop recieving web traffic.\n\nTruncation\n----------\n\nIf a generated subdomain label is longer than 63 characters, it will be truncated.\n\nFor example, the following subdomain label is too long, 67 characters: `ecorp--text-to-speech-really-really-realllly-long-function-name-dev`.\n\nThe truncation happens by calculating a SHA-256 hash of the overlong label, then taking the first 6 characters of this hash. The overlong subdomain label is truncated to 56 characters, and then joined by a dash to the hash prefix. In the above example, the resulting URL would be `ecorp--text-to-speech-really-really-rea-1b964b-dev.modal.run`.\n\nThe combination of the label hashing and truncation provides a unique list of 63 characters, complying with both DNS system limits and uniqueness requirements.\n\nCustom domains\n--------------\n\n**Custom domains are available on our [Team and Enterprise plans](/settings/plans)\n.**\n\nFor more customization, you can use your own domain names with Modal web endpoints. If your [plan](/pricing)\n supports custom domains, visit the [Domains tab](/settings/domains)\n in your workspace settings to add a domain name to your workspace.\n\nYou can use three kinds of domains with Modal:\n\n*   **Apex:** root domain names like `example.com`\n*   **Subdomain:** single subdomain entries such as `my-app.example.com`, `api.example.com`, etc.\n*   **Wildcard domain:** either in a subdomain like `*.example.com`, or in a deeper level like `*.modal.example.com`\n\nYou’ll be asked to update your domain DNS records with your domain name registrar and then validate the configuration in Modal. Once the records have been properly updated and propagated, your custom domain will be ready to use.\n\nYou can assign any Modal web endpoint to any registered domain in your workspace with the `custom_domains` argument.\n\n    from modal import App, web_endpoint\n    \n    app = App(\"custom-domains-example\")\n    \n    \n    @app.function()\n    @web_endpoint(custom_domains=[\"api.example.com\"])\n    def hello(message: str):\n        return {\"message\": f\"hello {message}\"}\n\nCopy\n\nYou can then run `modal deploy` to put your web endpoint online, live.\n\n    $ curl -s https://api.example.com?message=world\n    {\"message\": \"hello world\"}\n\nCopy\n\nNote that Modal automatically generates and renews TLS certificates for your custom domains. Since we do this when your domain is first accessed, there may be an additional 1-2s latency on the first request. Additional requests use a cached certificate.\n\nYou can also register multiple domain names and associate them with the same web endpoint.\n\n    from modal import App, web_endpoint\n    \n    app = App(\"custom-domains-example-2\")\n    \n    \n    @app.function()\n    @web_endpoint(custom_domains=[\"api.example.com\", \"api.example.net\"])\n    def hello(message: str):\n        return {\"message\": f\"hello {message}\"}\n\nCopy\n\nFor **Wildcard** domains, Modal will automatically resolve arbitrary custom endpoints (and issue TLS certificates). For example, if you add the wildcard domain `*.example.com`, then you can create any custom domains under `example.com`:\n\n    import random\n    from modal import App, web_endpoint\n    \n    app = App(\"custom-domains-example-2\")\n    \n    random_domain_name = random.choice(range(10))\n    \n    \n    @app.function()\n    @web_endpoint(custom_domains=[f\"{random_domain_name}.example.com\"])\n    def hello(message: str):\n        return {\"message\": f\"hello {message}\"}\n\nCopy\n\nCustom domains can also be used with [ASGI](https://modal.com/docs/reference/modal.asgi_app#modalasgi_app)\n or [WSGI](https://modal.com/docs/reference/modal.wsgi_app)\n apps using the same `custom_domains` argument.\n\n[Web endpoint URLs](#web-endpoint-urls)\n [Auto-generated URLs](#auto-generated-urls)\n [User-specified labels](#user-specified-labels)\n [Ephemeral apps](#ephemeral-apps)\n [Truncation](#truncation)\n [Custom domains](#custom-domains)\n\nSee it in action\n\n[Custom URL for LLM frontend](/docs/examples/text_generation_inference)",
    "metadata": {
      "title": "Web endpoint URLs | Modal Docs",
      "description": "Modal will automatically assign URLs for webhooks, but this can be configured in several different ways.",
      "ogTitle": "Web endpoint URLs",
      "ogDescription": "Modal will automatically assign URLs for webhooks, but this can be configured in several different ways.",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/webhook-urls",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nRequest timeouts\n================\n\nWeb endpoint (a.k.a. webhook) requests should complete quickly, ideally within a few seconds. All web endpoint function types ([`web_endpoint`, `asgi_app`, `wsgi_app`](/docs/reference/modal.web_endpoint)\n) have a maximum HTTP request timeout of 150 seconds enforced. However, the underlying Modal function can have a longer [timeout](/docs/guide/timeouts)\n.\n\nIn case the function takes more than 150 seconds to complete, a HTTP status 303 redirect response is returned pointing at the original URL with a special query parameter linking it that request. This is the _result URL_ for your function. Most web browsers allow for up to 20 such redirects, effectively allowing up to 50 minutes (20 \\* 150 s) for web endpoints before the request times out.\n\n(**Note:** This does not work with requests that require [CORS](https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS)\n, since the response will not have been returned from your code in time for the server to populate CORS headers.)\n\nSome libraries and tools might require you to add a flag or option in order to follow redirects automatically, e.g. `curl -L ...` or `http --follow ...`.\n\nThe _result URL_ can be reloaded without triggering a new request. It will block until the request completes.\n\nPolling solutions\n-----------------\n\nSometimes it can be useful to be able to poll for results rather than wait for a long running HTTP request. The easiest way to do this is to have your web endpoint spawn a `modal.Function` call and return the function call id that another endpoint can use to poll the submitted function’s status. Here is an example:\n\n    import fastapi\n    from modal import App, asgi_app\n    from modal.functions import FunctionCall\n    \n    \n    app = App()\n    \n    web_app = fastapi.FastAPI()\n    \n    \n    @app.function()\n    @asgi_app()\n    def fastapi_app():\n        return web_app\n    \n    \n    @app.function()\n    def slow_operation():\n        ...\n    \n    \n    @web_app.post(\"/accept\")\n    async def accept_job(request: fastapi.Request):\n        call = slow_operation.spawn()\n        return {\"call_id\": call.object_id}\n    \n    \n    @web_app.get(\"/result/{call_id}\")\n    async def poll_results(call_id: str):\n        function_call = FunctionCall.from_id(call_id)\n        try:\n            return function_call.get(timeout=0)\n        except TimeoutError:\n            http_accepted_code = 202\n            return fastapi.responses.JSONResponse({}, status_code=http_accepted_code)\n\nCopy\n\n[_Document OCR Web App_](/docs/examples/doc_ocr_webapp)\n is an example that uses this pattern.\n\n[Request timeouts](#request-timeouts)\n [Polling solutions](#polling-solutions)",
    "markdown": "* * *\n\nRequest timeouts\n================\n\nWeb endpoint (a.k.a. webhook) requests should complete quickly, ideally within a few seconds. All web endpoint function types ([`web_endpoint`, `asgi_app`, `wsgi_app`](/docs/reference/modal.web_endpoint)\n) have a maximum HTTP request timeout of 150 seconds enforced. However, the underlying Modal function can have a longer [timeout](/docs/guide/timeouts)\n.\n\nIn case the function takes more than 150 seconds to complete, a HTTP status 303 redirect response is returned pointing at the original URL with a special query parameter linking it that request. This is the _result URL_ for your function. Most web browsers allow for up to 20 such redirects, effectively allowing up to 50 minutes (20 \\* 150 s) for web endpoints before the request times out.\n\n(**Note:** This does not work with requests that require [CORS](https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS)\n, since the response will not have been returned from your code in time for the server to populate CORS headers.)\n\nSome libraries and tools might require you to add a flag or option in order to follow redirects automatically, e.g. `curl -L ...` or `http --follow ...`.\n\nThe _result URL_ can be reloaded without triggering a new request. It will block until the request completes.\n\nPolling solutions\n-----------------\n\nSometimes it can be useful to be able to poll for results rather than wait for a long running HTTP request. The easiest way to do this is to have your web endpoint spawn a `modal.Function` call and return the function call id that another endpoint can use to poll the submitted function’s status. Here is an example:\n\n    import fastapi\n    from modal import App, asgi_app\n    from modal.functions import FunctionCall\n    \n    \n    app = App()\n    \n    web_app = fastapi.FastAPI()\n    \n    \n    @app.function()\n    @asgi_app()\n    def fastapi_app():\n        return web_app\n    \n    \n    @app.function()\n    def slow_operation():\n        ...\n    \n    \n    @web_app.post(\"/accept\")\n    async def accept_job(request: fastapi.Request):\n        call = slow_operation.spawn()\n        return {\"call_id\": call.object_id}\n    \n    \n    @web_app.get(\"/result/{call_id}\")\n    async def poll_results(call_id: str):\n        function_call = FunctionCall.from_id(call_id)\n        try:\n            return function_call.get(timeout=0)\n        except TimeoutError:\n            http_accepted_code = 202\n            return fastapi.responses.JSONResponse({}, status_code=http_accepted_code)\n\nCopy\n\n[_Document OCR Web App_](/docs/examples/doc_ocr_webapp)\n is an example that uses this pattern.\n\n[Request timeouts](#request-timeouts)\n [Polling solutions](#polling-solutions)",
    "metadata": {
      "title": "Request timeouts | Modal Docs",
      "description": "Web endpoint (a.k.a. webhook) requests should complete quickly, ideally within a few seconds. All web endpoint function types (web_endpoint, asgi_app, wsgi_app) have a maximum HTTP request timeout of 150 seconds enforced. However, the underlying Modal function can have a longer timeout.",
      "ogTitle": "Request timeouts",
      "ogDescription": "Web endpoint (a.k.a. webhook) requests should complete quickly, ideally within a few seconds. All web endpoint function types (web_endpoint, asgi_app, wsgi_app) have a maximum HTTP request timeout of 150 seconds enforced. However, the underlying Modal function can have a longer timeout.",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/webhook-timeouts",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nTunnels (beta)\n==============\n\nModal allows you to expose live TCP ports on a Modal container. This is done by creating a _tunnel_ that forwards the port to the public Internet.\n\n    from modal import App, forward\n    \n    app = App()\n    \n    \n    @app.function()\n    def start_app():\n        # Inside this `with` block, port 8000 on the container can be accessed by\n        # the address at `tunnel.url`, which is randomly assigned.\n        with forward(8000) as tunnel:\n            print(f\"tunnel.url        = {tunnel.url}\")\n            print(f\"tunnel.tls_socket = {tunnel.tls_socket}\")\n            # ... start some web server at port 8000, using any framework\n\nCopy\n\nTunnels are direct connections and terminate TLS automatically. Within a few milliseconds of container startup, this function prints a message such as:\n\n    tunnel.url        = https://wtqcahqwhd4tu0.r5.modal.host\n    tunnel.tls_socket = ('wtqcahqwhd4tu0.r5.modal.host', 443)\n\nCopy\n\nBuild with tunnels\n------------------\n\nTunnels are the fastest way to get a low-latency, direct connection to a running container. You can use them to run live browser applications with **interactive terminals**, **Jupyter notebooks**, **VS Code servers**, and more.\n\nAs a quick example, here is how you would expose a Jupyter notebook:\n\n    import os\n    import secrets\n    import subprocess\n    \n    from modal import Image, App, forward\n    \n    \n    app = App()\n    app.image = Image.debian_slim().pip_install(\"jupyterlab\")\n    \n    \n    @app.function()\n    def run_jupyter():\n        token = secrets.token_urlsafe(13)\n        with forward(8888) as tunnel:\n            url = tunnel.url + \"/?token=\" + token\n            print(f\"Starting Jupyter at {url}\")\n            subprocess.run(\n                [\\\n                    \"jupyter\",\\\n                    \"lab\",\\\n                    \"--no-browser\",\\\n                    \"--allow-root\",\\\n                    \"--ip=0.0.0.0\",\\\n                    \"--port=8888\",\\\n                    \"--LabApp.allow_origin='*'\",\\\n                    \"--LabApp.allow_remote_access=1\",\\\n                ],\n                env={**os.environ, \"JUPYTER_TOKEN\": token, \"SHELL\": \"/bin/bash\"},\n                stderr=subprocess.DEVNULL,\n            )\n\nCopy\n\nWhen you run the function, it starts Jupyter and gives you the public URL. It’s as simple as that.\n\nAll Modal features are supported. If you [need GPUs](https://modal.com/docs/guide/gpu)\n, pass `gpu=` to the `@app.function()` decorator. If you [need more CPUs, RAM](https://modal.com/docs/guide/resources)\n, or to attach [volumes](https://modal.com/docs/guide/volumes)\n or [network file systems](https://modal.com/docs/guide/network-file-systems)\n, those also just work.\n\n### Programmable startup\n\nThe tunnel API is completely on-demand, so you can start them as the result of a web request.\n\nFor example, you could make something like Jupyter Hub without leaving Modal, giving your users their own Jupyter notebooks when they visit a URL:\n\n    from fastapi import HTTPException\n    from fastapi.responses import RedirectResponse\n    from modal import Queue, App, web_endpoint\n    \n    \n    app = App()\n    \n    \n    @app.function(timeout=900)  # 15 minutes\n    def run_jupyter(q):\n        ...  # as before, but return the URL on app.q\n    \n    \n    @app.function()\n    @web_endpoint(method=\"POST\")\n    def jupyter_hub():\n        ...  # do some validation on the secret or bearer token\n    \n        if is_valid:\n            with Queue.ephemeral() as q:\n                run_jupyter.spawn(q)\n                url = q.get()\n                return RedirectResponse(url, status_code=303)\n    \n        else:\n            raise HTTPException(401, \"Not authenticated\")\n\nCopy\n\nThis gives every user who sends a POST request to the web endpoint their own Jupyter notebook server, on a fully isolated Modal container.\n\nYou could do the same with VS Code and get some basic version of an instant, serverless IDE!\n\n### Advanced: Unencrypted TCP tunnels\n\nBy default, tunnels are only exposed to the Internet at a secure random URL, and connections have automatic TLS (the “S” in HTTPS). However, sometimes you might need to expose a protocol like an SSH server that goes directly over TCP. In this case, we have support for _unencrypted_ tunnels:\n\n    with forward(8000, unencrypted=True) as tunnel:\n        print(f\"tunnel.tcp_socket = {tunnel.tcp_socket}\")\n\nCopy\n\nMight produce an output like:\n\n    tunnel.tcp_socket = ('r3.modal.host', 23447)\n\nCopy\n\nYou can then connect over TCP, for example with `nc r2.modal.host 23447`. Unlike encrypted TLS sockets, these cannot be given a non-guessable, cryptographically random URL due to how the TCP protocol works, so they are assigned a random port number instead.\n\nPricing\n-------\n\nModal only charges for containers based on [the resources you use](https://modal.com/pricing)\n. There is no additional charge for having an active tunnel.\n\nFor example, if you start a Jupyter notebook on port 8888 and access it via tunnel, you can use it for an hour for development (with 0.01 CPUs) and then actually run an intensive job with 16 CPUs for one minute. The amount you would be billed for in that hour is 0.01 + 16 \\* (1/60) = **0.28 CPUs**, even though you had access to 16 CPUs without needing to restart your notebook.\n\nSecurity\n--------\n\nTunnels are run on Modal’s private global network of Internet relays. On startup, your container will connect to the nearest tunnel so you get the minimum latency, very similar in performance to a direct connection with the machine.\n\nThis makes them ideal for live debugging sessions, using web-based terminals like [ttyd](https://github.com/tsl0922/ttyd)\n.\n\nThe generated URLs are cryptographically random, but they are also public on the Internet, so anyone can access your application if they are given the URL.\n\nWe do not currently do any detection of requests above L4, so if you are running a web server, we will not add special proxy HTTP headers or translate HTTP/2. You’re just getting the TLS-encrypted TCP stream directly!\n\n[Tunnels (beta)](#tunnels-beta)\n [Build with tunnels](#build-with-tunnels)\n [Programmable startup](#programmable-startup)\n [Advanced: Unencrypted TCP tunnels](#advanced-unencrypted-tcp-tunnels)\n [Pricing](#pricing)\n [Security](#security)",
    "markdown": "* * *\n\nTunnels (beta)\n==============\n\nModal allows you to expose live TCP ports on a Modal container. This is done by creating a _tunnel_ that forwards the port to the public Internet.\n\n    from modal import App, forward\n    \n    app = App()\n    \n    \n    @app.function()\n    def start_app():\n        # Inside this `with` block, port 8000 on the container can be accessed by\n        # the address at `tunnel.url`, which is randomly assigned.\n        with forward(8000) as tunnel:\n            print(f\"tunnel.url        = {tunnel.url}\")\n            print(f\"tunnel.tls_socket = {tunnel.tls_socket}\")\n            # ... start some web server at port 8000, using any framework\n\nCopy\n\nTunnels are direct connections and terminate TLS automatically. Within a few milliseconds of container startup, this function prints a message such as:\n\n    tunnel.url        = https://wtqcahqwhd4tu0.r5.modal.host\n    tunnel.tls_socket = ('wtqcahqwhd4tu0.r5.modal.host', 443)\n\nCopy\n\nBuild with tunnels\n------------------\n\nTunnels are the fastest way to get a low-latency, direct connection to a running container. You can use them to run live browser applications with **interactive terminals**, **Jupyter notebooks**, **VS Code servers**, and more.\n\nAs a quick example, here is how you would expose a Jupyter notebook:\n\n    import os\n    import secrets\n    import subprocess\n    \n    from modal import Image, App, forward\n    \n    \n    app = App()\n    app.image = Image.debian_slim().pip_install(\"jupyterlab\")\n    \n    \n    @app.function()\n    def run_jupyter():\n        token = secrets.token_urlsafe(13)\n        with forward(8888) as tunnel:\n            url = tunnel.url + \"/?token=\" + token\n            print(f\"Starting Jupyter at {url}\")\n            subprocess.run(\n                [\\\n                    \"jupyter\",\\\n                    \"lab\",\\\n                    \"--no-browser\",\\\n                    \"--allow-root\",\\\n                    \"--ip=0.0.0.0\",\\\n                    \"--port=8888\",\\\n                    \"--LabApp.allow_origin='*'\",\\\n                    \"--LabApp.allow_remote_access=1\",\\\n                ],\n                env={**os.environ, \"JUPYTER_TOKEN\": token, \"SHELL\": \"/bin/bash\"},\n                stderr=subprocess.DEVNULL,\n            )\n\nCopy\n\nWhen you run the function, it starts Jupyter and gives you the public URL. It’s as simple as that.\n\nAll Modal features are supported. If you [need GPUs](https://modal.com/docs/guide/gpu)\n, pass `gpu=` to the `@app.function()` decorator. If you [need more CPUs, RAM](https://modal.com/docs/guide/resources)\n, or to attach [volumes](https://modal.com/docs/guide/volumes)\n or [network file systems](https://modal.com/docs/guide/network-file-systems)\n, those also just work.\n\n### Programmable startup\n\nThe tunnel API is completely on-demand, so you can start them as the result of a web request.\n\nFor example, you could make something like Jupyter Hub without leaving Modal, giving your users their own Jupyter notebooks when they visit a URL:\n\n    from fastapi import HTTPException\n    from fastapi.responses import RedirectResponse\n    from modal import Queue, App, web_endpoint\n    \n    \n    app = App()\n    \n    \n    @app.function(timeout=900)  # 15 minutes\n    def run_jupyter(q):\n        ...  # as before, but return the URL on app.q\n    \n    \n    @app.function()\n    @web_endpoint(method=\"POST\")\n    def jupyter_hub():\n        ...  # do some validation on the secret or bearer token\n    \n        if is_valid:\n            with Queue.ephemeral() as q:\n                run_jupyter.spawn(q)\n                url = q.get()\n                return RedirectResponse(url, status_code=303)\n    \n        else:\n            raise HTTPException(401, \"Not authenticated\")\n\nCopy\n\nThis gives every user who sends a POST request to the web endpoint their own Jupyter notebook server, on a fully isolated Modal container.\n\nYou could do the same with VS Code and get some basic version of an instant, serverless IDE!\n\n### Advanced: Unencrypted TCP tunnels\n\nBy default, tunnels are only exposed to the Internet at a secure random URL, and connections have automatic TLS (the “S” in HTTPS). However, sometimes you might need to expose a protocol like an SSH server that goes directly over TCP. In this case, we have support for _unencrypted_ tunnels:\n\n    with forward(8000, unencrypted=True) as tunnel:\n        print(f\"tunnel.tcp_socket = {tunnel.tcp_socket}\")\n\nCopy\n\nMight produce an output like:\n\n    tunnel.tcp_socket = ('r3.modal.host', 23447)\n\nCopy\n\nYou can then connect over TCP, for example with `nc r2.modal.host 23447`. Unlike encrypted TLS sockets, these cannot be given a non-guessable, cryptographically random URL due to how the TCP protocol works, so they are assigned a random port number instead.\n\nPricing\n-------\n\nModal only charges for containers based on [the resources you use](https://modal.com/pricing)\n. There is no additional charge for having an active tunnel.\n\nFor example, if you start a Jupyter notebook on port 8888 and access it via tunnel, you can use it for an hour for development (with 0.01 CPUs) and then actually run an intensive job with 16 CPUs for one minute. The amount you would be billed for in that hour is 0.01 + 16 \\* (1/60) = **0.28 CPUs**, even though you had access to 16 CPUs without needing to restart your notebook.\n\nSecurity\n--------\n\nTunnels are run on Modal’s private global network of Internet relays. On startup, your container will connect to the nearest tunnel so you get the minimum latency, very similar in performance to a direct connection with the machine.\n\nThis makes them ideal for live debugging sessions, using web-based terminals like [ttyd](https://github.com/tsl0922/ttyd)\n.\n\nThe generated URLs are cryptographically random, but they are also public on the Internet, so anyone can access your application if they are given the URL.\n\nWe do not currently do any detection of requests above L4, so if you are running a web server, we will not add special proxy HTTP headers or translate HTTP/2. You’re just getting the TLS-encrypted TCP stream directly!\n\n[Tunnels (beta)](#tunnels-beta)\n [Build with tunnels](#build-with-tunnels)\n [Programmable startup](#programmable-startup)\n [Advanced: Unencrypted TCP tunnels](#advanced-unencrypted-tcp-tunnels)\n [Pricing](#pricing)\n [Security](#security)",
    "metadata": {
      "title": "Tunnels (beta) | Modal Docs",
      "description": "Modal allows you to expose live TCP ports on a Modal container. This is done by creating a tunnel that forwards the port to the public Internet.",
      "ogTitle": "Tunnels (beta)",
      "ogDescription": "Modal allows you to expose live TCP ports on a Modal container. This is done by creating a tunnel that forwards the port to the public Internet.",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/tunnels",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nPassing local data\n==================\n\nIf you have a function that needs access to some data not present in your Python files themselves you have a few options for bundling that data with your Modal app.\n\nPassing function arguments\n--------------------------\n\nThe simplest and most straight-forward way is to read the data from your local script and pass the data to the outermost Modal function call:\n\n    import json\n    \n    \n    @app.function()\n    def foo(a):\n        print(sum(a[\"numbers\"]))\n    \n    \n    @app.local_entrypoint()\n    def main():\n        data_structure = json.load(open(\"blob.json\"))\n        foo.remote(data_structure)\n\nCopy\n\nAny data of reasonable size that is serializable through [cloudpickle](https://github.com/cloudpipe/cloudpickle)\n is passable as an argument to Modal functions.\n\nRefer to the section on [global variables](/docs/guide/global-variables)\n for how to work with objects in global scope that can only be initialized locally.\n\nMounting directories\n--------------------\n\nIf you want to forward _files_ from your local system, you can do that through [`modal.Mount`](/docs/reference/modal.Mount)\n objects and the `mounts` function decorator option:\n\n    @app.function(mounts=[modal.Mount.from_local_dir(\"/user/john/.aws\", remote_path=\"/root/.aws\")])\n    def aws_stuff():\n        ...\n\nCopy\n\nNote: the mounted directory will not be shared between worker instances, so modifying files or writing new files to a mount will not be reflected in other functions calls with the same mount. For this reason, you should typically treat the `Mount` as read-only.\n\n### Mounting local packages\n\nFor the special case of mounting a local package so it’s also available within your Python environment inside the container, Modal provides a [`Mount.from_local_python_packages`](/docs/reference/modal.Mount#from_local_python_packages)\n helper function:\n\n    import modal\n    import my_local_module\n    \n    app = modal.App()\n    \n    @app.function(mounts=[modal.Mount.from_local_python_packages(\"my_local_module\", \"my_other_module\")])\n    def f():\n        my_local_module.do_stuff()\n\nCopy\n\n[Passing local data](#passing-local-data)\n [Passing function arguments](#passing-function-arguments)\n [Mounting directories](#mounting-directories)\n [Mounting local packages](#mounting-local-packages)",
    "markdown": "* * *\n\nPassing local data\n==================\n\nIf you have a function that needs access to some data not present in your Python files themselves you have a few options for bundling that data with your Modal app.\n\nPassing function arguments\n--------------------------\n\nThe simplest and most straight-forward way is to read the data from your local script and pass the data to the outermost Modal function call:\n\n    import json\n    \n    \n    @app.function()\n    def foo(a):\n        print(sum(a[\"numbers\"]))\n    \n    \n    @app.local_entrypoint()\n    def main():\n        data_structure = json.load(open(\"blob.json\"))\n        foo.remote(data_structure)\n\nCopy\n\nAny data of reasonable size that is serializable through [cloudpickle](https://github.com/cloudpipe/cloudpickle)\n is passable as an argument to Modal functions.\n\nRefer to the section on [global variables](/docs/guide/global-variables)\n for how to work with objects in global scope that can only be initialized locally.\n\nMounting directories\n--------------------\n\nIf you want to forward _files_ from your local system, you can do that through [`modal.Mount`](/docs/reference/modal.Mount)\n objects and the `mounts` function decorator option:\n\n    @app.function(mounts=[modal.Mount.from_local_dir(\"/user/john/.aws\", remote_path=\"/root/.aws\")])\n    def aws_stuff():\n        ...\n\nCopy\n\nNote: the mounted directory will not be shared between worker instances, so modifying files or writing new files to a mount will not be reflected in other functions calls with the same mount. For this reason, you should typically treat the `Mount` as read-only.\n\n### Mounting local packages\n\nFor the special case of mounting a local package so it’s also available within your Python environment inside the container, Modal provides a [`Mount.from_local_python_packages`](/docs/reference/modal.Mount#from_local_python_packages)\n helper function:\n\n    import modal\n    import my_local_module\n    \n    app = modal.App()\n    \n    @app.function(mounts=[modal.Mount.from_local_python_packages(\"my_local_module\", \"my_other_module\")])\n    def f():\n        my_local_module.do_stuff()\n\nCopy\n\n[Passing local data](#passing-local-data)\n [Passing function arguments](#passing-function-arguments)\n [Mounting directories](#mounting-directories)\n [Mounting local packages](#mounting-local-packages)",
    "metadata": {
      "title": "Passing local data | Modal Docs",
      "description": "If you have a function that needs access to some data not present in your Python files themselves you have a few options for bundling that data with your Modal app.",
      "ogTitle": "Passing local data",
      "ogDescription": "If you have a function that needs access to some data not present in your Python files themselves you have a few options for bundling that data with your Modal app.",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/local-data",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nVolumes\n=======\n\nThe [`modal.Volume`](/docs/reference/modal.Volume)\n is a mutable volume built for high-performance file serving. Like the [`modal.NetworkFileSystem`](/docs/reference/modal.NetworkFileSystem)\n, these volumes can be simultaneously attached to multiple Modal functions, supporting concurrent reading and writing. But unlike the `modal.NetworkFileSystem`, the `modal.Volume` has been designed for fast reads and does not automatically synchronize writes between mounted volumes.\n\nThe `modal.Volume` works best with write-once, read-many I/O workloads.\n\nVolumes work best when they contain less then 50,000 files and directories. The latency to attach or modify a volume scales linearly with the number of files in the volume, and past a few tens of thousands of files the linear component starts to dominate the fixed overhead.\n\nThere is currently a hard limit of 500,000 inodes (files, directories and symbolic links) per volume. If you reach this limit, any further attempts to create new files or directories will error with `ENOSPC` (No space left on device).\n\nCreating a volume\n-----------------\n\nThe easiest way to create a Volume and use it as a part of your app is to use the `modal volume create` CLI command. This will create the volume and output some sample code:\n\n    % modal volume create my-test-volume\n    Created volume 'my-test-volume' in environment 'main'.\n\nCopy\n\nUsing the volume from code\n--------------------------\n\nTo attach a volume to a function, use `Volume.from_name`:\n\n    from modal import App, Volume\n    \n    app = App()\n    \n    vol = Volume.from_name(\"my-volume\")\n    \n    @app.function(volumes={\"/data\": vol})\n    def run():\n        with open(\"/data/xyz.txt\", \"w\") as f:\n            f.write(\"hello\")\n        vol.commit()  # Needed to make sure all changes are persisted\n\nCopy\n\n### Creating volumes lazily\n\nYou can also create volumes lazily from code using:\n\n    from modal import Volume\n    \n    vol = Volume.from_name(\"my-volume\", create_if_missing=True)\n\nCopy\n\nThis will create the volume if it doesn’t exist.\n\nUsing the volume from the command line\n--------------------------------------\n\nYou can also interact with volumes using the command line interface. You can run `modal volume` to get a full list of its subcommands:\n\n    % modal volume\n    \n     Usage: modal volume [OPTIONS] COMMAND [ARGS]...\n    \n     Read and edit modal.Volume volumes.\n     Note: users of modal.NetworkFileSystem should use the modal nfs command instead.\n    \n    ╭─ Options ─────────────────────────────────────────────────────────────────────────────────────────────╮\n    │ --help          Show this message and exit.                                                           │\n    ╰───────────────────────────────────────────────────────────────────────────────────────────────────────╯\n    ╭─ Commands ────────────────────────────────────────────────────────────────────────────────────────────╮\n    │ cp      Copy source file to destination file or multiple source files to destination directory.       │\n    │ create  Create a named, persistent modal.Volume.                                                      │\n    │ delete  Delete a named, persistent modal.Volume.                                                      │\n    │ get     Download files from a modal.Volume.                                                           │\n    │ list    List the details of all modal.Volume volumes in an environment.                               │\n    │ ls      List files and directories in a modal.Volume volume.                                          │\n    │ put     Upload a file or directory to a volume.                                                       │\n    │ rm      Delete a file or directory from a volume.                                                     │\n    ╰───────────────────────────────────────────────────────────────────────────────────────────────────────╯\n\nCopy\n\nVolume commits and reloads\n--------------------------\n\nUnlike a networked filesystem, you need to explicitly reload the Volume to see changes made since it was first mounted. This reload is handled by invoking the [`.reload()`](/docs/reference/modal.Volume#reload)\n method on a Volume object. Similarly, any volume changes made within a container need to be committed for those the changes to become visible outside the current container. This is handled by invoking the [`.commit()`](/docs/reference/modal.Volume#commit)\n method on a Volume object, or by enabling [background commits](#background-commits)\n.\n\nAt container creation time the latest state of an attached Volume is mounted. If the Volume is then subsequently modified by a commit operation in another running container, that Volume modification won’t become available until the original container does a [`.reload()`](/docs/reference/modal.Volume#reload)\n.\n\nConsider this example which demonstrates the effect of a reload:\n\n    import pathlib\n    import modal\n    \n    app = modal.App()\n    \n    volume = modal.Volume.from_name(\"my-volume\")\n    \n    p = pathlib.Path(\"/root/foo/bar.txt\")\n    \n    \n    @app.function(volumes={\"/root/foo\": volume})\n    def f():\n        p.write_text(\"hello\")\n        print(f\"Created {p=}\")\n        volume.commit()  # Persist changes\n        print(f\"Committed {p=}\")\n    \n    \n    @app.function(volumes={\"/root/foo\": volume})\n    def g(reload: bool = False):\n        if reload:\n            volume.reload()  # Fetch latest changes\n        if p.exists():\n            print(f\"{p=} contains '{p.read_text()}'\")\n        else:\n            print(f\"{p=} does not exist!\")\n    \n    \n    @app.local_entrypoint()\n    def main():\n        g.remote()  # 1. container for `g` starts\n        f.remote()  # 2. container for `f` starts, commits file\n        g.remote(reload=False)  # 3. reuses container for `g`, no reload\n        g.remote(reload=True)   # 4. reuses container, but reloads to see file.\n\nCopy\n\nThe output for this example is this:\n\n    p=PosixPath('/root/foo/bar.txt') does not exist!\n    Created p=PosixPath('/root/foo/bar.txt')\n    Committed p=PosixPath('/root/foo/bar.txt')\n    p=PosixPath('/root/foo/bar.txt') does not exist!\n    p=PosixPath('/root/foo/bar.txt') contains hello\n\nCopy\n\nThis code runs two containers, one for `f` and one for `g`. Only the last function invocation reads the file created and committed by `f` because it was configured to reload.\n\nBackground commits\n------------------\n\nVolumes have support for background committing that is in beta. This functionality periodically commits the state of your Volume so that your application code does not need to invoke `.commit()`.\n\nThis functionality is enabled using the [`_allow_background_volume_commits`](/docs/reference/modal.App#function)\n flag on [`@app.function`](/docs/reference/modal.App#function)\n.\n\n    @app.function(volumes={\"/vol/models\": volume}, _allow_background_volume_commits=True)\n    def train():\n        p = pathlib.Path(\"/vol/models/dummy.txt\")\n        p.write_text(\"I will be persisted without volume.commit()!\")\n        ...\n\nCopy\n\nDuring the execution of the `train` function shown above, every few seconds the attached Volume will be snapshotted and its new changes committed. A final snapshot and commit is also automatically performed on container shutdown.\n\nBeing able to persist changes to Volumes without changing your application code is especially useful when training or fine-tuning models.\n\nModel serving\n-------------\n\nA single ML model can be served by simply baking it into a `modal.Image` at build time using [`run_function`](/docs/reference/modal.Image#run_function)\n. But if you have dozens of models to serve, or otherwise need to decouple image builds from model storage and serving, use a `modal.Volume`.\n\nVolumes can be used to save a large number of ML models and later serve any one of them at runtime with much better performance than can be achieved with a [`modal.NetworkFileSystem`](/docs/reference/modal.NetworkFileSystem)\n.\n\nThis snippet below shows the basic structure of the solution.\n\n    import modal\n    \n    app = modal.App()\n    volume = modal.Volume.from_name(\"model-store\")\n    model_store_path = \"/vol/models\"\n    \n    \n    @app.function(volumes={model_store_path: volume}, gpu=\"any\")\n    def run_training():\n        model = train(...)\n        save(model_store_path, model)\n        volume.commit()  # Persist changes\n    \n    \n    @app.function(volumes={model_store_path: volume})\n    def inference(model_id: str, request):\n        try:\n            model = load_model(model_store_path, model_id)\n        except NotFound:\n            volume.reload()  # Fetch latest changes\n            model = load_model(model_store_path, model_id)\n        return model.run(request)\n\nCopy\n\nModel checkpointing\n-------------------\n\nCheckpoints are snapshots of an ML model and can be configured by the callback functions of ML frameworks. You can use saved checkpoints to restart a training job from the last saved checkpoint. This is particularly helpful in managing [preemption](/docs/guide/preemption)\n.\n\n### Huggingface `transformers`\n\nTo periodically checkpoint into a `modal.Volume`, you must:\n\n*   Enable [background commits](#background-commits)\n    \n*   Set the Trainer’s [`output_dir`](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments.output_dir)\n     to write into the volume’s mount location.\n\n    import pathlib\n    \n    VOL_MOUNT_PATH = pathlib.Path(\"/vol\")\n    \n    \n    @app.function(\n        gpu=\"A10g\",\n        timeout=7_200,\n        volumes={VOL_MOUNT_PATH: volume},\n    )\n    def finetune():\n        from transformers import Seq2SeqTrainer\n        ...\n    \n        training_args = Seq2SeqTrainingArguments(\n            output_dir=str(VOL_MOUNT_PATH / \"model\"),\n            ...\n        )\n    \n        trainer = Seq2SeqTrainer(\n            model=model,\n            args=training_args,\n            train_dataset=tokenized_xsum_train,\n            eval_dataset=tokenized_xsum_test,\n        )\n\nCopy\n\nFilesystem consistency\n----------------------\n\n### Concurrent modification\n\nConcurrent modification from multiple containers is supported, but concurrent modifications of the same files should be avoided. Last write wins in case of concurrent modification of the same file — any data the last writer didn’t have when committing changes will be lost!\n\nThe number of commits you can run concurrently is limited. If you run too many concurrent commits each commit will take longer due to contention. If you are committing small changes, avoid doing more than 5 concurrent commits (the number of concurrent commits you can make is proportional to the size of the changes being committed).\n\nAs a result, volumes are typically not a good fit for use cases where you need to make concurrent modifications to the same file (nor is distributed file locking supported).\n\nWhile a commit or reload is in progress the volume will appear empty to the container that initiated the commit. That means you cannot read from or write to a volume in a container where a commit or reload is ongoing (note that this only applies to the container where the commit or reload was issued, other containers remain unaffected).\n\nFor example, this is not going to work:\n\n    volume = modal.Volume.from_name(\"my-volume\")\n    \n    \n    @app.function(image=modal.Image.debian_slim().pip_install(\"aiofiles\"), volumes={\"/vol\": volume})\n    async def concurrent_write_and_commit():\n        async with aiofiles.open(\"/vol/big.file\", \"w\") as f:\n            await f.write(\"hello\" * 1024 * 1024 * 500)\n    \n        async def f():\n            await asyncio.sleep(0.1)  # Wait for the commit to start\n            # This is going to fail with:\n            # PermissionError: [Errno 1] Operation not permitted: '/vol/other.file'\n            # since the commit is in progress when we attempt the write.\n            async with aiofiles.open(\"/vol/other.file\", \"w\") as f:\n                await f.write(\"hello\")\n    \n        await asyncio.gather(volume.commit.aio(), f())\n\nCopy\n\n### Busy volume errors\n\nYou can only reload a volume when there no open files on the volume. If you have open files on the volume the [`.reload()`](/docs/reference/modal.Volume#reload)\n operation will fail with “volume busy”. The following is a simple example of how a “volume busy” error can occur:\n\n    volume = modal.Volume.from_name(\"my-volume\")\n    \n    \n    @app.function(volumes={\"/vol\": volume})\n    def reload_with_open_files():\n        f = open(\"/vol/data.txt\", \"r\")\n        volume.reload()  # Cannot reload when files in the volume are open.\n\nCopy\n\nCan’t find file on volume errors\n--------------------------------\n\nWhen accessing files in your volume, don’t forget to pre-pend where your Volume is mounted in the container.\n\nIn the example below, where the volume has been mounted at `/data`, “hello” is being written to `/data/xyz.txt`.\n\n    from modal import App, Volume\n    \n    app = App()\n    \n    vol = Volume.from_name(\"my-volume\")\n    \n    @app.function(volumes={\"/data\": vol}, _allow_background_volume_commits=True)\n    def run():\n        with open(\"/data/xyz.txt\", \"w\") as f:\n            f.write(\"hello\")\n        vol.commit()\n\nCopy\n\nIf you instead write to “/xyz.txt”, when you dump the contents of the volume, you will not see the `xyz.txt` file.\n\nFurther examples\n----------------\n\n*   [_Pet Art Dreambooth with Hugging Face and Gradio_ uses a volume for model storage](/docs/examples/dreambooth_app)\n    \n\n[Volumes](#volumes)\n [Creating a volume](#creating-a-volume)\n [Using the volume from code](#using-the-volume-from-code)\n [Creating volumes lazily](#creating-volumes-lazily)\n [Using the volume from the command line](#using-the-volume-from-the-command-line)\n [Volume commits and reloads](#volume-commits-and-reloads)\n [Background commits](#background-commits)\n [Model serving](#model-serving)\n [Model checkpointing](#model-checkpointing)\n [Huggingface transformers](#huggingface-transformers)\n [Filesystem consistency](#filesystem-consistency)\n [Concurrent modification](#concurrent-modification)\n [Busy volume errors](#busy-volume-errors)\n [Can’t find file on volume errors](#cant-find-file-on-volume-errors)\n [Further examples](#further-examples)\n\nSee it in action\n\n[Fine-tuning and serving custom LLaMA models](/docs/examples/llm-finetuning)",
    "markdown": "* * *\n\nVolumes\n=======\n\nThe [`modal.Volume`](/docs/reference/modal.Volume)\n is a mutable volume built for high-performance file serving. Like the [`modal.NetworkFileSystem`](/docs/reference/modal.NetworkFileSystem)\n, these volumes can be simultaneously attached to multiple Modal functions, supporting concurrent reading and writing. But unlike the `modal.NetworkFileSystem`, the `modal.Volume` has been designed for fast reads and does not automatically synchronize writes between mounted volumes.\n\nThe `modal.Volume` works best with write-once, read-many I/O workloads.\n\nVolumes work best when they contain less then 50,000 files and directories. The latency to attach or modify a volume scales linearly with the number of files in the volume, and past a few tens of thousands of files the linear component starts to dominate the fixed overhead.\n\nThere is currently a hard limit of 500,000 inodes (files, directories and symbolic links) per volume. If you reach this limit, any further attempts to create new files or directories will error with `ENOSPC` (No space left on device).\n\nCreating a volume\n-----------------\n\nThe easiest way to create a Volume and use it as a part of your app is to use the `modal volume create` CLI command. This will create the volume and output some sample code:\n\n    % modal volume create my-test-volume\n    Created volume 'my-test-volume' in environment 'main'.\n\nCopy\n\nUsing the volume from code\n--------------------------\n\nTo attach a volume to a function, use `Volume.from_name`:\n\n    from modal import App, Volume\n    \n    app = App()\n    \n    vol = Volume.from_name(\"my-volume\")\n    \n    @app.function(volumes={\"/data\": vol})\n    def run():\n        with open(\"/data/xyz.txt\", \"w\") as f:\n            f.write(\"hello\")\n        vol.commit()  # Needed to make sure all changes are persisted\n\nCopy\n\n### Creating volumes lazily\n\nYou can also create volumes lazily from code using:\n\n    from modal import Volume\n    \n    vol = Volume.from_name(\"my-volume\", create_if_missing=True)\n\nCopy\n\nThis will create the volume if it doesn’t exist.\n\nUsing the volume from the command line\n--------------------------------------\n\nYou can also interact with volumes using the command line interface. You can run `modal volume` to get a full list of its subcommands:\n\n    % modal volume\n    \n     Usage: modal volume [OPTIONS] COMMAND [ARGS]...\n    \n     Read and edit modal.Volume volumes.\n     Note: users of modal.NetworkFileSystem should use the modal nfs command instead.\n    \n    ╭─ Options ─────────────────────────────────────────────────────────────────────────────────────────────╮\n    │ --help          Show this message and exit.                                                           │\n    ╰───────────────────────────────────────────────────────────────────────────────────────────────────────╯\n    ╭─ Commands ────────────────────────────────────────────────────────────────────────────────────────────╮\n    │ cp      Copy source file to destination file or multiple source files to destination directory.       │\n    │ create  Create a named, persistent modal.Volume.                                                      │\n    │ delete  Delete a named, persistent modal.Volume.                                                      │\n    │ get     Download files from a modal.Volume.                                                           │\n    │ list    List the details of all modal.Volume volumes in an environment.                               │\n    │ ls      List files and directories in a modal.Volume volume.                                          │\n    │ put     Upload a file or directory to a volume.                                                       │\n    │ rm      Delete a file or directory from a volume.                                                     │\n    ╰───────────────────────────────────────────────────────────────────────────────────────────────────────╯\n\nCopy\n\nVolume commits and reloads\n--------------------------\n\nUnlike a networked filesystem, you need to explicitly reload the Volume to see changes made since it was first mounted. This reload is handled by invoking the [`.reload()`](/docs/reference/modal.Volume#reload)\n method on a Volume object. Similarly, any volume changes made within a container need to be committed for those the changes to become visible outside the current container. This is handled by invoking the [`.commit()`](/docs/reference/modal.Volume#commit)\n method on a Volume object, or by enabling [background commits](#background-commits)\n.\n\nAt container creation time the latest state of an attached Volume is mounted. If the Volume is then subsequently modified by a commit operation in another running container, that Volume modification won’t become available until the original container does a [`.reload()`](/docs/reference/modal.Volume#reload)\n.\n\nConsider this example which demonstrates the effect of a reload:\n\n    import pathlib\n    import modal\n    \n    app = modal.App()\n    \n    volume = modal.Volume.from_name(\"my-volume\")\n    \n    p = pathlib.Path(\"/root/foo/bar.txt\")\n    \n    \n    @app.function(volumes={\"/root/foo\": volume})\n    def f():\n        p.write_text(\"hello\")\n        print(f\"Created {p=}\")\n        volume.commit()  # Persist changes\n        print(f\"Committed {p=}\")\n    \n    \n    @app.function(volumes={\"/root/foo\": volume})\n    def g(reload: bool = False):\n        if reload:\n            volume.reload()  # Fetch latest changes\n        if p.exists():\n            print(f\"{p=} contains '{p.read_text()}'\")\n        else:\n            print(f\"{p=} does not exist!\")\n    \n    \n    @app.local_entrypoint()\n    def main():\n        g.remote()  # 1. container for `g` starts\n        f.remote()  # 2. container for `f` starts, commits file\n        g.remote(reload=False)  # 3. reuses container for `g`, no reload\n        g.remote(reload=True)   # 4. reuses container, but reloads to see file.\n\nCopy\n\nThe output for this example is this:\n\n    p=PosixPath('/root/foo/bar.txt') does not exist!\n    Created p=PosixPath('/root/foo/bar.txt')\n    Committed p=PosixPath('/root/foo/bar.txt')\n    p=PosixPath('/root/foo/bar.txt') does not exist!\n    p=PosixPath('/root/foo/bar.txt') contains hello\n\nCopy\n\nThis code runs two containers, one for `f` and one for `g`. Only the last function invocation reads the file created and committed by `f` because it was configured to reload.\n\nBackground commits\n------------------\n\nVolumes have support for background committing that is in beta. This functionality periodically commits the state of your Volume so that your application code does not need to invoke `.commit()`.\n\nThis functionality is enabled using the [`_allow_background_volume_commits`](/docs/reference/modal.App#function)\n flag on [`@app.function`](/docs/reference/modal.App#function)\n.\n\n    @app.function(volumes={\"/vol/models\": volume}, _allow_background_volume_commits=True)\n    def train():\n        p = pathlib.Path(\"/vol/models/dummy.txt\")\n        p.write_text(\"I will be persisted without volume.commit()!\")\n        ...\n\nCopy\n\nDuring the execution of the `train` function shown above, every few seconds the attached Volume will be snapshotted and its new changes committed. A final snapshot and commit is also automatically performed on container shutdown.\n\nBeing able to persist changes to Volumes without changing your application code is especially useful when training or fine-tuning models.\n\nModel serving\n-------------\n\nA single ML model can be served by simply baking it into a `modal.Image` at build time using [`run_function`](/docs/reference/modal.Image#run_function)\n. But if you have dozens of models to serve, or otherwise need to decouple image builds from model storage and serving, use a `modal.Volume`.\n\nVolumes can be used to save a large number of ML models and later serve any one of them at runtime with much better performance than can be achieved with a [`modal.NetworkFileSystem`](/docs/reference/modal.NetworkFileSystem)\n.\n\nThis snippet below shows the basic structure of the solution.\n\n    import modal\n    \n    app = modal.App()\n    volume = modal.Volume.from_name(\"model-store\")\n    model_store_path = \"/vol/models\"\n    \n    \n    @app.function(volumes={model_store_path: volume}, gpu=\"any\")\n    def run_training():\n        model = train(...)\n        save(model_store_path, model)\n        volume.commit()  # Persist changes\n    \n    \n    @app.function(volumes={model_store_path: volume})\n    def inference(model_id: str, request):\n        try:\n            model = load_model(model_store_path, model_id)\n        except NotFound:\n            volume.reload()  # Fetch latest changes\n            model = load_model(model_store_path, model_id)\n        return model.run(request)\n\nCopy\n\nModel checkpointing\n-------------------\n\nCheckpoints are snapshots of an ML model and can be configured by the callback functions of ML frameworks. You can use saved checkpoints to restart a training job from the last saved checkpoint. This is particularly helpful in managing [preemption](/docs/guide/preemption)\n.\n\n### Huggingface `transformers`\n\nTo periodically checkpoint into a `modal.Volume`, you must:\n\n*   Enable [background commits](#background-commits)\n    \n*   Set the Trainer’s [`output_dir`](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments.output_dir)\n     to write into the volume’s mount location.\n\n    import pathlib\n    \n    VOL_MOUNT_PATH = pathlib.Path(\"/vol\")\n    \n    \n    @app.function(\n        gpu=\"A10g\",\n        timeout=7_200,\n        volumes={VOL_MOUNT_PATH: volume},\n    )\n    def finetune():\n        from transformers import Seq2SeqTrainer\n        ...\n    \n        training_args = Seq2SeqTrainingArguments(\n            output_dir=str(VOL_MOUNT_PATH / \"model\"),\n            ...\n        )\n    \n        trainer = Seq2SeqTrainer(\n            model=model,\n            args=training_args,\n            train_dataset=tokenized_xsum_train,\n            eval_dataset=tokenized_xsum_test,\n        )\n\nCopy\n\nFilesystem consistency\n----------------------\n\n### Concurrent modification\n\nConcurrent modification from multiple containers is supported, but concurrent modifications of the same files should be avoided. Last write wins in case of concurrent modification of the same file — any data the last writer didn’t have when committing changes will be lost!\n\nThe number of commits you can run concurrently is limited. If you run too many concurrent commits each commit will take longer due to contention. If you are committing small changes, avoid doing more than 5 concurrent commits (the number of concurrent commits you can make is proportional to the size of the changes being committed).\n\nAs a result, volumes are typically not a good fit for use cases where you need to make concurrent modifications to the same file (nor is distributed file locking supported).\n\nWhile a commit or reload is in progress the volume will appear empty to the container that initiated the commit. That means you cannot read from or write to a volume in a container where a commit or reload is ongoing (note that this only applies to the container where the commit or reload was issued, other containers remain unaffected).\n\nFor example, this is not going to work:\n\n    volume = modal.Volume.from_name(\"my-volume\")\n    \n    \n    @app.function(image=modal.Image.debian_slim().pip_install(\"aiofiles\"), volumes={\"/vol\": volume})\n    async def concurrent_write_and_commit():\n        async with aiofiles.open(\"/vol/big.file\", \"w\") as f:\n            await f.write(\"hello\" * 1024 * 1024 * 500)\n    \n        async def f():\n            await asyncio.sleep(0.1)  # Wait for the commit to start\n            # This is going to fail with:\n            # PermissionError: [Errno 1] Operation not permitted: '/vol/other.file'\n            # since the commit is in progress when we attempt the write.\n            async with aiofiles.open(\"/vol/other.file\", \"w\") as f:\n                await f.write(\"hello\")\n    \n        await asyncio.gather(volume.commit.aio(), f())\n\nCopy\n\n### Busy volume errors\n\nYou can only reload a volume when there no open files on the volume. If you have open files on the volume the [`.reload()`](/docs/reference/modal.Volume#reload)\n operation will fail with “volume busy”. The following is a simple example of how a “volume busy” error can occur:\n\n    volume = modal.Volume.from_name(\"my-volume\")\n    \n    \n    @app.function(volumes={\"/vol\": volume})\n    def reload_with_open_files():\n        f = open(\"/vol/data.txt\", \"r\")\n        volume.reload()  # Cannot reload when files in the volume are open.\n\nCopy\n\nCan’t find file on volume errors\n--------------------------------\n\nWhen accessing files in your volume, don’t forget to pre-pend where your Volume is mounted in the container.\n\nIn the example below, where the volume has been mounted at `/data`, “hello” is being written to `/data/xyz.txt`.\n\n    from modal import App, Volume\n    \n    app = App()\n    \n    vol = Volume.from_name(\"my-volume\")\n    \n    @app.function(volumes={\"/data\": vol}, _allow_background_volume_commits=True)\n    def run():\n        with open(\"/data/xyz.txt\", \"w\") as f:\n            f.write(\"hello\")\n        vol.commit()\n\nCopy\n\nIf you instead write to “/xyz.txt”, when you dump the contents of the volume, you will not see the `xyz.txt` file.\n\nFurther examples\n----------------\n\n*   [_Pet Art Dreambooth with Hugging Face and Gradio_ uses a volume for model storage](/docs/examples/dreambooth_app)\n    \n\n[Volumes](#volumes)\n [Creating a volume](#creating-a-volume)\n [Using the volume from code](#using-the-volume-from-code)\n [Creating volumes lazily](#creating-volumes-lazily)\n [Using the volume from the command line](#using-the-volume-from-the-command-line)\n [Volume commits and reloads](#volume-commits-and-reloads)\n [Background commits](#background-commits)\n [Model serving](#model-serving)\n [Model checkpointing](#model-checkpointing)\n [Huggingface transformers](#huggingface-transformers)\n [Filesystem consistency](#filesystem-consistency)\n [Concurrent modification](#concurrent-modification)\n [Busy volume errors](#busy-volume-errors)\n [Can’t find file on volume errors](#cant-find-file-on-volume-errors)\n [Further examples](#further-examples)\n\nSee it in action\n\n[Fine-tuning and serving custom LLaMA models](/docs/examples/llm-finetuning)",
    "metadata": {
      "title": "Volumes | Modal Docs",
      "description": "The modal.Volume is a mutable volume built for high-performance file serving. Like the modal.NetworkFileSystem, these volumes can be simultaneously attached to multiple Modal functions, supporting concurrent reading and writing. But unlike the modal.NetworkFileSystem, the modal.Volume has been designed for fast reads and does not automatically synchronize writes between mounted volumes.",
      "ogTitle": "Volumes",
      "ogDescription": "The modal.Volume is a mutable volume built for high-performance file serving. Like the modal.NetworkFileSystem, these volumes can be simultaneously attached to multiple Modal functions, supporting concurrent reading and writing. But unlike the modal.NetworkFileSystem, the modal.Volume has been designed for fast reads and does not automatically synchronize writes between mounted volumes.",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/volumes",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nMounting local files and directories\n====================================\n\nWhen you run your code on Modal, it executes in a [containerized environment](/docs/guide/custom-container)\n separate from your local machine.\n\nThere are two ways to make local files available to your Modal app:\n\n1.  **Mounting:** Mounting is the process of making local files and directories accessible to your Modal function or application during runtime. Mounting is intended for files that change frequently during development. It allows you to modify your code locally and rerun it on Modal without needing to rebuild the container image each time. This can significantly speed up your development iteration cycle.\n    \n2.  **Adding files to the container image:** For files that don’t change often, you can add them directly to your Modal container image during the build process with [`copy_local_file`](/docs/reference/modal.Image#copy_local_file)\n     or [`copy_local_dir`](/docs/reference/modal.Image#copy_local_dir)\n    . This is suitable for dependencies and static assets that remain relatively constant throughout your development process.\n    \n\nThis page is concerned with mounting. To use local files and packages within your Modal app via mounting, they either need to be _automounted_ or explicitly mounted.\n\nAutomount\n---------\n\nBy default, with [`automount=True`](/docs/reference/modal.config#other-configuration-options)\n, Modal mounts local Python packages that you have used (imported) in your code but are not installed globally on your system (like those in `site-packages`, where globally installed packages reside).\n\nFor example, if you have a local module `deps.py` that contains a function you would like to import, `dependency`. You import it as follows:\n\n    from deps import dependency\n    \n    app = modal.App()\n\nCopy\n\nModal will automatically mount `deps.py`, and all of its dependencies not in `site-packages`.\n\nAll Python packages that are installed in `site-packages` will be excluded from automounting. This includes packages installed in virtual environments.\n\nNon-Python files will be automounted only if they are located in the same directory or subdirectory of a Python package. Note that the directory where your Modal entrypoint is located is considered a package if it contains a `__init__.py` file and is being called as a package.\n\n### Editable-mode exclusion\n\nIf local packages that you _thought_ were installed in `site-packages` are being automounted, it’s possible that those packages were installed in editable-mode.\n\nWhen you install a package in editable-mode (also known as “development mode”), instead of copying the package files to the `site-packages` directory, a link (symbolic link or .egg-link file) is placed there. This link points to the actual location of the package files, which are typically in your project directory or a separate source directory. As a result, they may be automounted.\n\n### Automounts take precedence over PyPI packages\n\nAutomounts take precedence over PyPI packages, so in the case where you `pip_install` or otherwise include a package by building it into your image, the automount will still trigger and shadow the `site-packages` installed version. An example of when this would happen is if you have binary parts of local modules such that they need to be built as part of the image build.\n\nExample #1: Simple directory structure\n--------------------------------------\n\nLet’s look at an example directory structure:\n\n    mountingexample1\n    ├── __init__.py\n    ├── data\n    │   └── my_data.jsonl\n    └── entrypoint.py\n\nCopy\n\nAnd let’s say your `entrypoint.py` code looks like this:\n\n    from modal import App\n    \n    app = App()\n    \n    \n    @app.function()\n    def app_function():\n        print(\"app function\")\n\nCopy\n\nWhen you run `modal run entrypoint.py` from inside the `mountingexample1` directory, you will see the following items mounted:\n\n    ✓ Created objects.\n    ├── 🔨 Created mount /Users/yirenlu/modal-scrap/mountingexample1/entrypoint.py\n    └── 🔨 Created app_function.\n\nCopy\n\nThe `data` directory is not auto-mounted, because `mountingexample1` is not being treated like a package in this case.\n\nNow, let’s say you run `cd .. && modal run mountingexample1.entrypoint`. You should see the following items mounted:\n\n    ✓ Created objects.\n    ├── 🔨 Created mount PythonPackage:mountingexample1.entrypoint\n    ├── 🔨 Created mount PythonPackage:mountingexample1\n    └── 🔨 Created app_function.\n\nCopy\n\nThe entire `mountingexample1` package is mounted, including the `data` subdirectory.\n\nThis is because the `mountingexample1` directory is being treated as a package.\n\nExample #2: Global scope imports\n--------------------------------\n\nOftentimes when you are building on Modal, you will be migrating an existing codebase that is spread across multiple files and packages. Let’s say your directory looks like this:\n\n    mountingexample2\n    ├── __init__.py\n    ├── data\n    │   └── my_data.jsonl\n    ├── entrypoint.py\n    └── package\n        ├── __init__.py\n        ├── package_data\n        │   └── library_data.jsonl\n        └── package_function.py\n\nCopy\n\nAnd your entrypoint code looks like this:\n\n    from modal import App\n    from package.package_function import package_dependency\n    \n    app = App()\n    \n    \n    @app.function()\n    def app_function():\n        package_dependency()\n\nCopy\n\nWhen you run the entrypoint code with `modal run mountingexample2.entrypoint`, you will see the following items mounted:\n\n    ✓ Created objects.\n    ├── 🔨 Created mount PythonPackage:mountingexample2.entrypoint\n    ├── 🔨 Created mount PythonPackage:mountingexample2\n    └── 🔨 Created app_function.\n\nCopy\n\nThe entire contents of `mountingexample2` is mounted, including the `/data` directory and the `package` package inside of it.\n\nFinally, let’s check what happens when you remove the `package` import from your entrypoint code and run it with `modal run entrypoint.py`.\n\n    ✓ Created objects.\n    ├── 🔨 Created mount /Users/yirenlu/modal-scrap/mountingexample2/entrypoint.py\n    └── 🔨 Created app_function.\n\nCopy\n\nOnly the entrypoint file is mounted, and nothing else.\n\nMounting files manually\n-----------------------\n\nIf something that you want to have mounted is not included in an automount, you have a few options:\n\n1.  Specify local files and directories through [`Mount`](https://modal.com/docs/guide/local-data#mounting-directories)\n     objects.\n2.  Include local Python modules with the function [`Mount.from_local_python_packages()`](/docs/reference/modal.Mount#from_local_python_packages)\n    .\n3.  Refactor your directory structure so that the relevant files and directories are automounted as part of packages.\n\n[Mounting local files and directories](#mounting-local-files-and-directories)\n [Automount](#automount)\n [Editable-mode exclusion](#editable-mode-exclusion)\n [Automounts take precedence over PyPI packages](#automounts-take-precedence-over-pypi-packages)\n [Example #1: Simple directory structure](#example-1-simple-directory-structure)\n [Example #2: Global scope imports](#example-2-global-scope-imports)\n [Mounting files manually](#mounting-files-manually)",
    "markdown": "* * *\n\nMounting local files and directories\n====================================\n\nWhen you run your code on Modal, it executes in a [containerized environment](/docs/guide/custom-container)\n separate from your local machine.\n\nThere are two ways to make local files available to your Modal app:\n\n1.  **Mounting:** Mounting is the process of making local files and directories accessible to your Modal function or application during runtime. Mounting is intended for files that change frequently during development. It allows you to modify your code locally and rerun it on Modal without needing to rebuild the container image each time. This can significantly speed up your development iteration cycle.\n    \n2.  **Adding files to the container image:** For files that don’t change often, you can add them directly to your Modal container image during the build process with [`copy_local_file`](/docs/reference/modal.Image#copy_local_file)\n     or [`copy_local_dir`](/docs/reference/modal.Image#copy_local_dir)\n    . This is suitable for dependencies and static assets that remain relatively constant throughout your development process.\n    \n\nThis page is concerned with mounting. To use local files and packages within your Modal app via mounting, they either need to be _automounted_ or explicitly mounted.\n\nAutomount\n---------\n\nBy default, with [`automount=True`](/docs/reference/modal.config#other-configuration-options)\n, Modal mounts local Python packages that you have used (imported) in your code but are not installed globally on your system (like those in `site-packages`, where globally installed packages reside).\n\nFor example, if you have a local module `deps.py` that contains a function you would like to import, `dependency`. You import it as follows:\n\n    from deps import dependency\n    \n    app = modal.App()\n\nCopy\n\nModal will automatically mount `deps.py`, and all of its dependencies not in `site-packages`.\n\nAll Python packages that are installed in `site-packages` will be excluded from automounting. This includes packages installed in virtual environments.\n\nNon-Python files will be automounted only if they are located in the same directory or subdirectory of a Python package. Note that the directory where your Modal entrypoint is located is considered a package if it contains a `__init__.py` file and is being called as a package.\n\n### Editable-mode exclusion\n\nIf local packages that you _thought_ were installed in `site-packages` are being automounted, it’s possible that those packages were installed in editable-mode.\n\nWhen you install a package in editable-mode (also known as “development mode”), instead of copying the package files to the `site-packages` directory, a link (symbolic link or .egg-link file) is placed there. This link points to the actual location of the package files, which are typically in your project directory or a separate source directory. As a result, they may be automounted.\n\n### Automounts take precedence over PyPI packages\n\nAutomounts take precedence over PyPI packages, so in the case where you `pip_install` or otherwise include a package by building it into your image, the automount will still trigger and shadow the `site-packages` installed version. An example of when this would happen is if you have binary parts of local modules such that they need to be built as part of the image build.\n\nExample #1: Simple directory structure\n--------------------------------------\n\nLet’s look at an example directory structure:\n\n    mountingexample1\n    ├── __init__.py\n    ├── data\n    │   └── my_data.jsonl\n    └── entrypoint.py\n\nCopy\n\nAnd let’s say your `entrypoint.py` code looks like this:\n\n    from modal import App\n    \n    app = App()\n    \n    \n    @app.function()\n    def app_function():\n        print(\"app function\")\n\nCopy\n\nWhen you run `modal run entrypoint.py` from inside the `mountingexample1` directory, you will see the following items mounted:\n\n    ✓ Created objects.\n    ├── 🔨 Created mount /Users/yirenlu/modal-scrap/mountingexample1/entrypoint.py\n    └── 🔨 Created app_function.\n\nCopy\n\nThe `data` directory is not auto-mounted, because `mountingexample1` is not being treated like a package in this case.\n\nNow, let’s say you run `cd .. && modal run mountingexample1.entrypoint`. You should see the following items mounted:\n\n    ✓ Created objects.\n    ├── 🔨 Created mount PythonPackage:mountingexample1.entrypoint\n    ├── 🔨 Created mount PythonPackage:mountingexample1\n    └── 🔨 Created app_function.\n\nCopy\n\nThe entire `mountingexample1` package is mounted, including the `data` subdirectory.\n\nThis is because the `mountingexample1` directory is being treated as a package.\n\nExample #2: Global scope imports\n--------------------------------\n\nOftentimes when you are building on Modal, you will be migrating an existing codebase that is spread across multiple files and packages. Let’s say your directory looks like this:\n\n    mountingexample2\n    ├── __init__.py\n    ├── data\n    │   └── my_data.jsonl\n    ├── entrypoint.py\n    └── package\n        ├── __init__.py\n        ├── package_data\n        │   └── library_data.jsonl\n        └── package_function.py\n\nCopy\n\nAnd your entrypoint code looks like this:\n\n    from modal import App\n    from package.package_function import package_dependency\n    \n    app = App()\n    \n    \n    @app.function()\n    def app_function():\n        package_dependency()\n\nCopy\n\nWhen you run the entrypoint code with `modal run mountingexample2.entrypoint`, you will see the following items mounted:\n\n    ✓ Created objects.\n    ├── 🔨 Created mount PythonPackage:mountingexample2.entrypoint\n    ├── 🔨 Created mount PythonPackage:mountingexample2\n    └── 🔨 Created app_function.\n\nCopy\n\nThe entire contents of `mountingexample2` is mounted, including the `/data` directory and the `package` package inside of it.\n\nFinally, let’s check what happens when you remove the `package` import from your entrypoint code and run it with `modal run entrypoint.py`.\n\n    ✓ Created objects.\n    ├── 🔨 Created mount /Users/yirenlu/modal-scrap/mountingexample2/entrypoint.py\n    └── 🔨 Created app_function.\n\nCopy\n\nOnly the entrypoint file is mounted, and nothing else.\n\nMounting files manually\n-----------------------\n\nIf something that you want to have mounted is not included in an automount, you have a few options:\n\n1.  Specify local files and directories through [`Mount`](https://modal.com/docs/guide/local-data#mounting-directories)\n     objects.\n2.  Include local Python modules with the function [`Mount.from_local_python_packages()`](/docs/reference/modal.Mount#from_local_python_packages)\n    .\n3.  Refactor your directory structure so that the relevant files and directories are automounted as part of packages.\n\n[Mounting local files and directories](#mounting-local-files-and-directories)\n [Automount](#automount)\n [Editable-mode exclusion](#editable-mode-exclusion)\n [Automounts take precedence over PyPI packages](#automounts-take-precedence-over-pypi-packages)\n [Example #1: Simple directory structure](#example-1-simple-directory-structure)\n [Example #2: Global scope imports](#example-2-global-scope-imports)\n [Mounting files manually](#mounting-files-manually)",
    "metadata": {
      "title": "Mounting local files and directories | Modal Docs",
      "description": "When you run your code on Modal, it executes in a containerized environment separate from your local machine.",
      "ogTitle": "Mounting local files and directories",
      "ogDescription": "When you run your code on Modal, it executes in a containerized environment separate from your local machine.",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/mounting",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nStoring model weights on Modal\n==============================\n\nEfficiently managing the weights of large models is crucial for optimizing the build times and startup latency of ML and AI applications. This page discusses best practices for handling model weights with Modal, focusing on two key patterns:\n\n1.  Storing weights in container images when they are built, with [`@build`](/docs/guide/lifecycle-functions#build)\n    \n2.  Storing weights in a distributed file system, with Modal [Volumes](/docs/reference/modal.Volume)\n    \n\nThe first pattern leads to faster downloads and startup times, but it is only possible for weights that are known at build time, like the weights of pretrained models.\n\nIn both cases, you can further optimize latencies by loading weights into memory at container startup.\n\nPattern #1 - Storing weights in container images\n------------------------------------------------\n\nWhenever possible, you should store weights in your image as it is built, just as you store your code dependencies. Modal’s custom container runtime stack is designed to make builds and loads of large images as fast as possible.\n\nIn the code below, we demonstrate this pattern. We define a Python function, `download_model_to_folder`, that downloads the weights of a model from Hugging Face. Notice that the method has been annotated with the [`@build`](/docs/guide/lifecycle-functions#build)\n decorator. Methods of `modal.Cls`s that are decorated with `@build` are run while your container image is being built, just like commands to install dependencies with `.pip_install`. You can also use the [`run_function`](/docs/reference/modal.Image#run_function)\n method on the `Image` class for the same purpose.\n\n    image = (  # start building the image\n        Image\n        .debian_slim()\n        .pip_install(\"huggingface\", \"other_dependencies\")\n    )\n    \n    # ... other setup\n    \n    @app.cls(gpu=\"any\", image=image)\n    class Model:\n        @build()  # add another step to the image build\n        def download_model_to_folder(self):\n            from huggingface_hub import snapshot_download\n    \n            os.makedirs(MODEL_DIR, exist_ok=True)\n            snapshot_download(\"stabilityai/sdxl-turbo\", local_dir=MODEL_DIR)\n\nCopy\n\n### Pre-loading weights into memory with `@enter`\n\nBecause they are part of the container image, your model weights will be available as files when your functions start, just like your code dependencies. But model weights must still be loaded into memory before they can be used for inference. For models with billions of weights, that can still take several seconds.\n\nTo avoid spending that time on every input, you can load the weights into memory when your Modal containers start, but before they begin running your function, with another decorator: `@enter`. A method decorated with the `@enter` decorator will only run once at container startup.\n\n        @enter()\n        def setup(self):\n            self.pipe = AutoPipelineForImage2Image.from_pretrained(\"stabilityai/sdxl-turbo\")\n    \n        @method()\n        def inference(self, prompt):\n            return self.pipe(prompt)\n\nCopy\n\nYou can also stack `@build` and `@enter` decorators on the same method. This can have some benefits, as discussed [here](/docs/guide/lifecycle-functions#build)\n. For even further optimization of startup times with `@enter`, consider the (beta) [memory snapshot feature](/docs/guide/memory-snapshot#memory-snapshot-beta)\n.\n\nPattern #2 - Storing weights in Volumes\n---------------------------------------\n\nNot all applications use model weights that are known when the app’s container image is built.\n\nFor example, you might be\n\n*   serving models that are regularly fine-tuned\n*   serving too many different large models from one app to store them in a single image\n*   training models on the fly as your app runs\n\nIn each case, different components of your application will need to store, retrieve, and communicate weights over time. For this, we recommend Modal [Volumes](/docs/guide/volumes)\n, which act as a distributed file system, a “shared disk” all of your Modal functions can access.\n\nTo store your model weights in a Volume, you need to make the Volume available to a function that creates or retrieves the model weights, as in the snippet below.\n\n    import modal\n    \n    # create a Volume, or retrieve it if it exists\n    volume = modal.Volume.from_name(\"model-weights-vol\", create_if_missing=True)\n    MODEL_DIR = \"/vol/models\"\n    \n    @app.function(\n      volumes={MODEL_DIR: volume},  # \"mount\" the Volume, sharing it with your function\n      _allow_background_volume_commits=True  # use this flag when writing large files like model weights\n      gpu=\"any\",\n    )\n    def run_training():\n        model = train(...)\n        save(MODEL_DIR, model)\n\nCopy\n\nYou can then read those weights from the Volume as you would normally read them from disk, so long as you attach the Volume to your function or class.\n\n    @app.cls(gpu=\"any\", volumes={MODEL_DIR: volume})\n    class Model:\n        @method()\n        def inference(self, prompt):\n          model = load_model(MODEL_DIR)\n          self.model.run(prompt)\n\nCopy\n\nIn the above code sample, weights are loaded into memory each time the inference function is run. You can once again use `@enter` to load weights only once, at container boot.\n\n        @enter()\n        def setup(self):\n            self.model = load_model(MODEL_DIR)\n    \n        @method()\n        def inference(self, prompt):\n            return self.model.run(prompt)\n\nCopy\n\n### Pre-loading weights for multiple models dynamically with `__init__` and `@enter`\n\nFinally, you might be serving several different models from the same app and so need to dynamically determine which weights to load.\n\nEven in this case, you can avoid loading the weights at every inference. Just define a [`__init__`](/docs/guide/lifecycle-functions#parametrized-functions)\n method on the `modal.Cls` with arguments that identify which model to use and then use the [`@enter`](/docs/guide/lifecycle-functions#enter)\n method decorator to load those weights into memory:\n\n    @app.cls(gpu=\"any\", volumes={MODEL_DIR: volume})\n    class Model:\n    \n        def __init__(self, model_id):\n            self.model_id = model_id\n    \n        @enter()\n        def setup(self):\n            volume.reload()  # Fetch latest changes to the volume\n            self.model = load_model(MODEL_DIR, self.model_id)\n    \n        @method()\n        def inference(self, prompt):\n            return self.model.run(prompt)\n\nCopy\n\n[Storing model weights on Modal](#storing-model-weights-on-modal)\n [Pattern #1 - Storing weights in container images](#pattern-1---storing-weights-in-container-images)\n [Pre-loading weights into memory with @enter](#pre-loading-weights-into-memory-with-enter)\n [Pattern #2 - Storing weights in Volumes](#pattern-2---storing-weights-in-volumes)\n [Pre-loading weights for multiple models dynamically with \\_\\_init\\_\\_ and @enter](#pre-loading-weights-for-multiple-models-dynamically-with-__init__-and-enter)",
    "markdown": "* * *\n\nStoring model weights on Modal\n==============================\n\nEfficiently managing the weights of large models is crucial for optimizing the build times and startup latency of ML and AI applications. This page discusses best practices for handling model weights with Modal, focusing on two key patterns:\n\n1.  Storing weights in container images when they are built, with [`@build`](/docs/guide/lifecycle-functions#build)\n    \n2.  Storing weights in a distributed file system, with Modal [Volumes](/docs/reference/modal.Volume)\n    \n\nThe first pattern leads to faster downloads and startup times, but it is only possible for weights that are known at build time, like the weights of pretrained models.\n\nIn both cases, you can further optimize latencies by loading weights into memory at container startup.\n\nPattern #1 - Storing weights in container images\n------------------------------------------------\n\nWhenever possible, you should store weights in your image as it is built, just as you store your code dependencies. Modal’s custom container runtime stack is designed to make builds and loads of large images as fast as possible.\n\nIn the code below, we demonstrate this pattern. We define a Python function, `download_model_to_folder`, that downloads the weights of a model from Hugging Face. Notice that the method has been annotated with the [`@build`](/docs/guide/lifecycle-functions#build)\n decorator. Methods of `modal.Cls`s that are decorated with `@build` are run while your container image is being built, just like commands to install dependencies with `.pip_install`. You can also use the [`run_function`](/docs/reference/modal.Image#run_function)\n method on the `Image` class for the same purpose.\n\n    image = (  # start building the image\n        Image\n        .debian_slim()\n        .pip_install(\"huggingface\", \"other_dependencies\")\n    )\n    \n    # ... other setup\n    \n    @app.cls(gpu=\"any\", image=image)\n    class Model:\n        @build()  # add another step to the image build\n        def download_model_to_folder(self):\n            from huggingface_hub import snapshot_download\n    \n            os.makedirs(MODEL_DIR, exist_ok=True)\n            snapshot_download(\"stabilityai/sdxl-turbo\", local_dir=MODEL_DIR)\n\nCopy\n\n### Pre-loading weights into memory with `@enter`\n\nBecause they are part of the container image, your model weights will be available as files when your functions start, just like your code dependencies. But model weights must still be loaded into memory before they can be used for inference. For models with billions of weights, that can still take several seconds.\n\nTo avoid spending that time on every input, you can load the weights into memory when your Modal containers start, but before they begin running your function, with another decorator: `@enter`. A method decorated with the `@enter` decorator will only run once at container startup.\n\n        @enter()\n        def setup(self):\n            self.pipe = AutoPipelineForImage2Image.from_pretrained(\"stabilityai/sdxl-turbo\")\n    \n        @method()\n        def inference(self, prompt):\n            return self.pipe(prompt)\n\nCopy\n\nYou can also stack `@build` and `@enter` decorators on the same method. This can have some benefits, as discussed [here](/docs/guide/lifecycle-functions#build)\n. For even further optimization of startup times with `@enter`, consider the (beta) [memory snapshot feature](/docs/guide/memory-snapshot#memory-snapshot-beta)\n.\n\nPattern #2 - Storing weights in Volumes\n---------------------------------------\n\nNot all applications use model weights that are known when the app’s container image is built.\n\nFor example, you might be\n\n*   serving models that are regularly fine-tuned\n*   serving too many different large models from one app to store them in a single image\n*   training models on the fly as your app runs\n\nIn each case, different components of your application will need to store, retrieve, and communicate weights over time. For this, we recommend Modal [Volumes](/docs/guide/volumes)\n, which act as a distributed file system, a “shared disk” all of your Modal functions can access.\n\nTo store your model weights in a Volume, you need to make the Volume available to a function that creates or retrieves the model weights, as in the snippet below.\n\n    import modal\n    \n    # create a Volume, or retrieve it if it exists\n    volume = modal.Volume.from_name(\"model-weights-vol\", create_if_missing=True)\n    MODEL_DIR = \"/vol/models\"\n    \n    @app.function(\n      volumes={MODEL_DIR: volume},  # \"mount\" the Volume, sharing it with your function\n      _allow_background_volume_commits=True  # use this flag when writing large files like model weights\n      gpu=\"any\",\n    )\n    def run_training():\n        model = train(...)\n        save(MODEL_DIR, model)\n\nCopy\n\nYou can then read those weights from the Volume as you would normally read them from disk, so long as you attach the Volume to your function or class.\n\n    @app.cls(gpu=\"any\", volumes={MODEL_DIR: volume})\n    class Model:\n        @method()\n        def inference(self, prompt):\n          model = load_model(MODEL_DIR)\n          self.model.run(prompt)\n\nCopy\n\nIn the above code sample, weights are loaded into memory each time the inference function is run. You can once again use `@enter` to load weights only once, at container boot.\n\n        @enter()\n        def setup(self):\n            self.model = load_model(MODEL_DIR)\n    \n        @method()\n        def inference(self, prompt):\n            return self.model.run(prompt)\n\nCopy\n\n### Pre-loading weights for multiple models dynamically with `__init__` and `@enter`\n\nFinally, you might be serving several different models from the same app and so need to dynamically determine which weights to load.\n\nEven in this case, you can avoid loading the weights at every inference. Just define a [`__init__`](/docs/guide/lifecycle-functions#parametrized-functions)\n method on the `modal.Cls` with arguments that identify which model to use and then use the [`@enter`](/docs/guide/lifecycle-functions#enter)\n method decorator to load those weights into memory:\n\n    @app.cls(gpu=\"any\", volumes={MODEL_DIR: volume})\n    class Model:\n    \n        def __init__(self, model_id):\n            self.model_id = model_id\n    \n        @enter()\n        def setup(self):\n            volume.reload()  # Fetch latest changes to the volume\n            self.model = load_model(MODEL_DIR, self.model_id)\n    \n        @method()\n        def inference(self, prompt):\n            return self.model.run(prompt)\n\nCopy\n\n[Storing model weights on Modal](#storing-model-weights-on-modal)\n [Pattern #1 - Storing weights in container images](#pattern-1---storing-weights-in-container-images)\n [Pre-loading weights into memory with @enter](#pre-loading-weights-into-memory-with-enter)\n [Pattern #2 - Storing weights in Volumes](#pattern-2---storing-weights-in-volumes)\n [Pre-loading weights for multiple models dynamically with \\_\\_init\\_\\_ and @enter](#pre-loading-weights-for-multiple-models-dynamically-with-__init__-and-enter)",
    "metadata": {
      "title": "Storing model weights on Modal | Modal Docs",
      "description": "Efficiently managing the weights of large models is crucial for optimizing the build times and startup latency of ML and AI applications. This page discusses best practices for handling model weights with Modal, focusing on two key patterns:",
      "ogTitle": "Storing model weights on Modal",
      "ogDescription": "Efficiently managing the weights of large models is crucial for optimizing the build times and startup latency of ML and AI applications. This page discusses best practices for handling model weights with Modal, focusing on two key patterns:",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/model-weights",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nLarge dataset ingestion\n=======================\n\nThis guide provides best practices for downloading, transforming, and storing large datasets within Modal. A dataset is considered large if it contains hundreds of thousands of files and/or is over 100 GiB in size.\n\nThese guidelines ensure that large datasets can be ingested fully and reliably.\n\nConfigure your Function for heavy disk usage\n--------------------------------------------\n\nLarge datasets should be downloaded and transformed using a `modal.Function` and stored into a `modal.CloudBucketMount`. We recommend backing the latter with a Cloudflare R2 bucket, because Cloudflare does not charge network egress fees and has lower GiB/month storage costs than AWS S3.\n\nThis `modal.Function` should specify a large `timeout` because large dataset processing can take hours, and it should request a larger ephemeral disk in cases where the dataset being downloaded and processed is hundreds of GiBs.\n\n    @app.function(\n        volumes={\n            \"/mnt\": modal.CloudBucketMount(\n                \"datasets\",\n                bucket_endpoint_url=\"https://abc123example.r2.cloudflarestorage.com\",\n                secret=modal.Secret.from_name(\"cloudflare-r2-datasets\"),\n            )\n        },\n        ephemeral_disk=1000 * 1000,  # 1 TiB\n        timeout=60 * 60 * 12,  # 12 hours\n    \n    )\n    def download_and_transform() -> None:\n        ...\n\nCopy\n\n### Use compressed archives on Modal Volumes\n\n`modal.Volume`s are designed for storing tens of thousands of individual files, but not for hundreds of thousands or millions of files. However they can be still be used for storing large datasets if files are first combined and compressed in a dataset transformation step before saving them into the Volume.\n\nSee the [transforming](#transforming)\n section below for more details.\n\nExperimentation\n---------------\n\nDownloading and transforming large datasets can be fiddly. While iterating on a reliable ingestion program it is recommended to start a long-running `modal.Function` serving a JupyterHub server so that you can maintain disk state in the face of application errors.\n\nSee the [running Jupyter server within a Modal function](https://github.com/modal-labs/modal-examples/blob/main/11_notebooks/jupyter_inside_modal.py)\n example as base code.\n\nDownloading\n-----------\n\nThe raw dataset data should be first downloaded into the container at `/tmp/` and not placed directly into the mounted volume. This serves a couple purposes.\n\n1.  Certain download libraries and tools (e.g. `wget`) perform filesystem operations not supported properly by `CloudBucketMount`.\n2.  The raw dataset data may need to be transformed before use, in which case it is wasteful to store it permanently.\n\nThis snippet shows the basic download-and-copy procedure:\n\n    tmp_path = pathlib.Path(\"/tmp/imagenet/\")\n    vol_path = pathlib.Path(\"/mnt/imagenet/\")\n    filename = \"imagenet-object-localization-challenge.zip\"\n    # 1. Download into /tmp/\n    subprocess.run(\n        f\"kaggle competitions download -c imagenet-object-localization-challenge --path {tmp_path}\",\n        shell=True,\n        check=True\n    )\n    vol_path.mkdir(exist_ok=True)\n    # 2. Copy (without transform) into mounted volume.\n    shutil.copy(tmp_path / filename, vol_path / filename)\n\nCopy\n\nTransforming\n------------\n\nWhen ingesting a large dataset it is sometimes necessary to tranform it before storage, so that it is in an optimal format for loading at runtime. A common kind of necessary transform is gzip decompression. Very large datasets are often gzipped for storage and network transmission efficiency, but gzip decompression (80 MiB/s) is hundreds of times slower than reading from a solid state drive and should be done once before storage to avoid decompressing on every read against the dataset.\n\nTransformations should be performed after storing the raw dataset in `/tmp/`. Performing transformations almost always increases container disk usage and this is where the [`ephemeral_disk` parameter](/docs/reference/modal.App#function)\n parameter becomes important. For example, a 100 GiB raw, compressed dataset may decompress to into 500 GiB, occupying 600 GiB of container disk space.\n\nTransformations should also typically be performed against `/tmp/`. This is because\n\n1.  transforms can be IO intensive and IO latency is lower against local SSD.\n2.  transforms can create temporary data which is wasteful to store permanently.\n\nExamples\n--------\n\nThe best practices offered in this guide are demonstrated in the [`modal-examples` repository](https://github.com/modal-labs/modal-examples/tree/main/12_datasets)\n.\n\nThe examples include these popular large datasets:\n\n*   [ImageNet](https://www.image-net.org/)\n    , the image labeling dataset that kicked off the deep learning revolution\n*   [COCO](https://cocodataset.org/#download)\n    , the Common Objects in COntext dataset of densely-labeled images\n*   [LAION-400M](https://laion.ai/blog/laion-400-open-dataset/)\n    , the Stable Diffusion training dataset\n*   Data derived from the [Big “Fantastic” Database](https://bfd.mmseqs.com/)\n    , [Protein Data Bank](https://www.wwpdb.org/)\n    , and [UniProt Database](https://www.uniprot.org/)\n     used in training the [RoseTTAFold](https://github.com/RosettaCommons/RoseTTAFold)\n     protein structure model\n\n[Large dataset ingestion](#large-dataset-ingestion)\n [Configure your Function for heavy disk usage](#configure-your-function-for-heavy-disk-usage)\n [Use compressed archives on Modal Volumes](#use-compressed-archives-on-modal-volumes)\n [Experimentation](#experimentation)\n [Downloading](#downloading)\n [Transforming](#transforming)\n [Examples](#examples)",
    "markdown": "* * *\n\nLarge dataset ingestion\n=======================\n\nThis guide provides best practices for downloading, transforming, and storing large datasets within Modal. A dataset is considered large if it contains hundreds of thousands of files and/or is over 100 GiB in size.\n\nThese guidelines ensure that large datasets can be ingested fully and reliably.\n\nConfigure your Function for heavy disk usage\n--------------------------------------------\n\nLarge datasets should be downloaded and transformed using a `modal.Function` and stored into a `modal.CloudBucketMount`. We recommend backing the latter with a Cloudflare R2 bucket, because Cloudflare does not charge network egress fees and has lower GiB/month storage costs than AWS S3.\n\nThis `modal.Function` should specify a large `timeout` because large dataset processing can take hours, and it should request a larger ephemeral disk in cases where the dataset being downloaded and processed is hundreds of GiBs.\n\n    @app.function(\n        volumes={\n            \"/mnt\": modal.CloudBucketMount(\n                \"datasets\",\n                bucket_endpoint_url=\"https://abc123example.r2.cloudflarestorage.com\",\n                secret=modal.Secret.from_name(\"cloudflare-r2-datasets\"),\n            )\n        },\n        ephemeral_disk=1000 * 1000,  # 1 TiB\n        timeout=60 * 60 * 12,  # 12 hours\n    \n    )\n    def download_and_transform() -> None:\n        ...\n\nCopy\n\n### Use compressed archives on Modal Volumes\n\n`modal.Volume`s are designed for storing tens of thousands of individual files, but not for hundreds of thousands or millions of files. However they can be still be used for storing large datasets if files are first combined and compressed in a dataset transformation step before saving them into the Volume.\n\nSee the [transforming](#transforming)\n section below for more details.\n\nExperimentation\n---------------\n\nDownloading and transforming large datasets can be fiddly. While iterating on a reliable ingestion program it is recommended to start a long-running `modal.Function` serving a JupyterHub server so that you can maintain disk state in the face of application errors.\n\nSee the [running Jupyter server within a Modal function](https://github.com/modal-labs/modal-examples/blob/main/11_notebooks/jupyter_inside_modal.py)\n example as base code.\n\nDownloading\n-----------\n\nThe raw dataset data should be first downloaded into the container at `/tmp/` and not placed directly into the mounted volume. This serves a couple purposes.\n\n1.  Certain download libraries and tools (e.g. `wget`) perform filesystem operations not supported properly by `CloudBucketMount`.\n2.  The raw dataset data may need to be transformed before use, in which case it is wasteful to store it permanently.\n\nThis snippet shows the basic download-and-copy procedure:\n\n    tmp_path = pathlib.Path(\"/tmp/imagenet/\")\n    vol_path = pathlib.Path(\"/mnt/imagenet/\")\n    filename = \"imagenet-object-localization-challenge.zip\"\n    # 1. Download into /tmp/\n    subprocess.run(\n        f\"kaggle competitions download -c imagenet-object-localization-challenge --path {tmp_path}\",\n        shell=True,\n        check=True\n    )\n    vol_path.mkdir(exist_ok=True)\n    # 2. Copy (without transform) into mounted volume.\n    shutil.copy(tmp_path / filename, vol_path / filename)\n\nCopy\n\nTransforming\n------------\n\nWhen ingesting a large dataset it is sometimes necessary to tranform it before storage, so that it is in an optimal format for loading at runtime. A common kind of necessary transform is gzip decompression. Very large datasets are often gzipped for storage and network transmission efficiency, but gzip decompression (80 MiB/s) is hundreds of times slower than reading from a solid state drive and should be done once before storage to avoid decompressing on every read against the dataset.\n\nTransformations should be performed after storing the raw dataset in `/tmp/`. Performing transformations almost always increases container disk usage and this is where the [`ephemeral_disk` parameter](/docs/reference/modal.App#function)\n parameter becomes important. For example, a 100 GiB raw, compressed dataset may decompress to into 500 GiB, occupying 600 GiB of container disk space.\n\nTransformations should also typically be performed against `/tmp/`. This is because\n\n1.  transforms can be IO intensive and IO latency is lower against local SSD.\n2.  transforms can create temporary data which is wasteful to store permanently.\n\nExamples\n--------\n\nThe best practices offered in this guide are demonstrated in the [`modal-examples` repository](https://github.com/modal-labs/modal-examples/tree/main/12_datasets)\n.\n\nThe examples include these popular large datasets:\n\n*   [ImageNet](https://www.image-net.org/)\n    , the image labeling dataset that kicked off the deep learning revolution\n*   [COCO](https://cocodataset.org/#download)\n    , the Common Objects in COntext dataset of densely-labeled images\n*   [LAION-400M](https://laion.ai/blog/laion-400-open-dataset/)\n    , the Stable Diffusion training dataset\n*   Data derived from the [Big “Fantastic” Database](https://bfd.mmseqs.com/)\n    , [Protein Data Bank](https://www.wwpdb.org/)\n    , and [UniProt Database](https://www.uniprot.org/)\n     used in training the [RoseTTAFold](https://github.com/RosettaCommons/RoseTTAFold)\n     protein structure model\n\n[Large dataset ingestion](#large-dataset-ingestion)\n [Configure your Function for heavy disk usage](#configure-your-function-for-heavy-disk-usage)\n [Use compressed archives on Modal Volumes](#use-compressed-archives-on-modal-volumes)\n [Experimentation](#experimentation)\n [Downloading](#downloading)\n [Transforming](#transforming)\n [Examples](#examples)",
    "metadata": {
      "title": "Large dataset ingestion | Modal Docs",
      "description": "This guide provides best practices for downloading, transforming, and storing large datasets within Modal. A dataset is considered large if it contains hundreds of thousands of files and/or is over 100 GiB in size.",
      "ogTitle": "Large dataset ingestion",
      "ogDescription": "This guide provides best practices for downloading, transforming, and storing large datasets within Modal. A dataset is considered large if it contains hundreds of thousands of files and/or is over 100 GiB in size.",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/dataset-ingestion",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nCloud bucket mounts\n===================\n\nThe [`modal.CloudBucketMount`](/docs/reference/modal.CloudBucketMount)\n is a mutable volume that allows for both reading and writing files from a cloud bucket. It supports AWS S3, Cloudflare R2, and Google Cloud Storage buckets.\n\nCloud bucket mounts are built on top of AWS’ [`mountpoint`](https://github.com/awslabs/mountpoint-s3)\n technology and inherits its limitations.\n\nMounting Cloudflare R2 buckets\n------------------------------\n\n`CloudBucketMount` enables Cloudflare R2 buckets to be mounted as file system volumes. Because Cloudflare R2 is [S3-Compatible](https://developers.cloudflare.com/r2/api/s3/api/)\n the setup is very similar between R2 and S3. See [modal.CloudBucketMount](/docs/reference/modal.CloudBucketMount#modalcloudbucketmount)\n for usage instructions.\n\nWhen creating the R2 API token for use with the mount, you need to have the ability to read, write, and list objects in the specific buckets you will mount. You do _not_ need admin permissions, and you should _not_ use “Client IP Address Filtering”.\n\nMounting Google Cloud Storage buckets\n-------------------------------------\n\n`CloudBucketMount` enables Google Cloud Storage (GCS) buckets to be mounted as file system volumes. See [modal.CloudBucketMount](/docs/reference/modal.CloudBucketMount#modalcloudbucketmount)\n for GCS setup instructions.\n\nMounting S3 buckets\n-------------------\n\n`CloudBucketMount` enables S3 buckets to be mounted as file system volumes. To interact with a bucket, you must have the appropriate IAM permissions configured (refer to the section on [IAM Permissions](#iam-permissions)\n).\n\n    import modal\n    import subprocess\n    \n    app = modal.App()\n    \n    s3_bucket_name = \"s3-bucket-name\"  # Bucket name not ARN.\n    s3_access_credentials = modal.Secret.from_dict({\n        \"AWS_ACCESS_KEY_ID\": \"...\",\n        \"AWS_SECRET_ACCESS_KEY\": \"...\",\n        \"AWS_REGION\": \"...\"\n    })\n    \n    @app.function(\n        volumes={\n            \"/my-mount\": modal.CloudBucketMount(s3_bucket_name, secret=s3_access_credentials)\n        }\n    )\n    def f():\n        subprocess.run([\"ls\", \"/my-mount\"])\n\nCopy\n\n### Specifying S3 bucket region\n\nAmazon S3 buckets are associated with a single AWS Region. [`Mountpoint`](https://github.com/awslabs/mountpoint-s3)\n attempts to automatically detect the region for your S3 bucket at startup time and directs all S3 requests to that region. However, in certain scenarios, like if your container is running on an AWS worker in a certain region, while your bucket is in a different region, this automatic detection may fail.\n\nTo avoid this issue, you can specify the region of your S3 bucket by adding an `AWS_REGION` key to your Modal secrets, as in the code example above.\n\n### Using AWS temporary security credentials\n\n`CloudBucketMount`s also support AWS temporary security credentials by passing the additional environment variable `AWS_SESSION_TOKEN`. Temporary credentials will expire and will not get renewed automatically. You will need to update the corresponding Modal Secret in order to prevent failures.\n\nYou can get temporary credentials with the [AWS CLI](https://aws.amazon.com/cli/)\n with:\n\n    $ aws configure export-credentials --format env\n    export AWS_ACCESS_KEY_ID=XXX\n    export AWS_SECRET_ACCESS_KEY=XXX\n    export AWS_SESSION_TOKEN=XXX...\n\nCopy\n\nAll these values are required.\n\n### Mounting a path within a bucket\n\nTo mount only the files under a specific subdirectory, you can specify a path prefix using `key_prefix`. Since this prefix specifies a directory, it must end in a `/`. The entire bucket is mounted when no prefix is supplied.\n\n    import modal\n    import subprocess\n    \n    app = modal.App()\n    \n    s3_bucket_name = \"s3-bucket-name\"\n    prefix = 'path/to/dir/'\n    \n    s3_access_credentials = modal.Secret.from_dict({\n        \"AWS_ACCESS_KEY_ID\": \"...\",\n        \"AWS_SECRET_ACCESS_KEY\": \"...\",\n    })\n    \n    @app.function(\n        volumes={\n            \"/my-mount\": modal.CloudBucketMount(\n                bucket_name=s3_bucket_name,\n                key_prefix=prefix,\n                secret=s3_access_credentials\n            )\n        }\n    )\n    def f():\n        subprocess.run([\"ls\", \"/my-mount\"])\n\nCopy\n\nThis will only mount the files in the bucket `s3-bucket-name` that are prefixed by `path/to/dir/`.\n\n### Read-only mode\n\nTo mount a bucket in read-only mode, set `read_only=True` as an argument.\n\n    import modal\n    import subprocess\n    \n    app = modal.App()\n    \n    s3_bucket_name = \"s3-bucket-name\"  # Bucket name not ARN.\n    s3_access_credentials = modal.Secret.from_dict({\n        \"AWS_ACCESS_KEY_ID\": \"...\",\n        \"AWS_SECRET_ACCESS_KEY\": \"...\",\n    })\n    \n    @app.function(\n        volumes={\n            \"/my-mount\": modal.CloudBucketMount(s3_bucket_name, secret=s3_access_credentials, read_only=True)\n        }\n    )\n    def f():\n        subprocess.run([\"ls\", \"/my-mount\"])\n\nCopy\n\nWhile S3 mounts supports both write and read operations, they are optimized for reading large files sequentially. Certain file operations, such as renaming files, are not supported. For a comprehensive list of supported operations, consult the [Mountpoint documentation](https://github.com/awslabs/mountpoint-s3/blob/main/doc/SEMANTICS.md)\n.\n\n### IAM permissions\n\nTo utilize `CloudBucketMount` for reading and writing files from S3 buckets, your IAM policy must include permissions for `s3:PutObject`, `s3:AbortMultipartUpload`, and `s3:DeleteObject`. These permissions are not required for mounts configured with `read_only=True`.\n\n    {\n      \"Version\": \"2012-10-17\",\n      \"Statement\": [\\\n        {\\\n          \"Sid\": \"ModalBucketAccess\",\\\n          \"Effect\": \"Allow\",\\\n          \"Action\": [\"s3:ListBucket\"],\\\n          \"Resource\": [\"arn:aws:s3:::<MY-S3-BUCKET>\"]\\\n        },\\\n        {\\\n          \"Sid\": \"ModalBucketAccess\",\\\n          \"Effect\": \"Allow\",\\\n          \"Action\": [\\\n            \"s3:GetObject\",\\\n            \"s3:PutObject\",\\\n            \"s3:AbortMultipartUpload\",\\\n            \"s3:DeleteObject\"\\\n          ],\\\n          \"Resource\": [\"arn:aws:s3:::<MY-S3-BUCKET>/*\"]\\\n        }\\\n      ]\n    }\n\nCopy\n\n[Cloud bucket mounts](#cloud-bucket-mounts)\n [Mounting Cloudflare R2 buckets](#mounting-cloudflare-r2-buckets)\n [Mounting Google Cloud Storage buckets](#mounting-google-cloud-storage-buckets)\n [Mounting S3 buckets](#mounting-s3-buckets)\n [Specifying S3 bucket region](#specifying-s3-bucket-region)\n [Using AWS temporary security credentials](#using-aws-temporary-security-credentials)\n [Mounting a path within a bucket](#mounting-a-path-within-a-bucket)\n [Read-only mode](#read-only-mode)\n [IAM permissions](#iam-permissions)\n\nSee it in action\n\n[Mount S3 buckets in Modal apps](/docs/examples/s3_bucket_mount)\n\n[Create a LoRA Playground with Modal, Gradio, and S3](/docs/examples/cloud_bucket_mount_loras)",
    "markdown": "* * *\n\nCloud bucket mounts\n===================\n\nThe [`modal.CloudBucketMount`](/docs/reference/modal.CloudBucketMount)\n is a mutable volume that allows for both reading and writing files from a cloud bucket. It supports AWS S3, Cloudflare R2, and Google Cloud Storage buckets.\n\nCloud bucket mounts are built on top of AWS’ [`mountpoint`](https://github.com/awslabs/mountpoint-s3)\n technology and inherits its limitations.\n\nMounting Cloudflare R2 buckets\n------------------------------\n\n`CloudBucketMount` enables Cloudflare R2 buckets to be mounted as file system volumes. Because Cloudflare R2 is [S3-Compatible](https://developers.cloudflare.com/r2/api/s3/api/)\n the setup is very similar between R2 and S3. See [modal.CloudBucketMount](/docs/reference/modal.CloudBucketMount#modalcloudbucketmount)\n for usage instructions.\n\nWhen creating the R2 API token for use with the mount, you need to have the ability to read, write, and list objects in the specific buckets you will mount. You do _not_ need admin permissions, and you should _not_ use “Client IP Address Filtering”.\n\nMounting Google Cloud Storage buckets\n-------------------------------------\n\n`CloudBucketMount` enables Google Cloud Storage (GCS) buckets to be mounted as file system volumes. See [modal.CloudBucketMount](/docs/reference/modal.CloudBucketMount#modalcloudbucketmount)\n for GCS setup instructions.\n\nMounting S3 buckets\n-------------------\n\n`CloudBucketMount` enables S3 buckets to be mounted as file system volumes. To interact with a bucket, you must have the appropriate IAM permissions configured (refer to the section on [IAM Permissions](#iam-permissions)\n).\n\n    import modal\n    import subprocess\n    \n    app = modal.App()\n    \n    s3_bucket_name = \"s3-bucket-name\"  # Bucket name not ARN.\n    s3_access_credentials = modal.Secret.from_dict({\n        \"AWS_ACCESS_KEY_ID\": \"...\",\n        \"AWS_SECRET_ACCESS_KEY\": \"...\",\n        \"AWS_REGION\": \"...\"\n    })\n    \n    @app.function(\n        volumes={\n            \"/my-mount\": modal.CloudBucketMount(s3_bucket_name, secret=s3_access_credentials)\n        }\n    )\n    def f():\n        subprocess.run([\"ls\", \"/my-mount\"])\n\nCopy\n\n### Specifying S3 bucket region\n\nAmazon S3 buckets are associated with a single AWS Region. [`Mountpoint`](https://github.com/awslabs/mountpoint-s3)\n attempts to automatically detect the region for your S3 bucket at startup time and directs all S3 requests to that region. However, in certain scenarios, like if your container is running on an AWS worker in a certain region, while your bucket is in a different region, this automatic detection may fail.\n\nTo avoid this issue, you can specify the region of your S3 bucket by adding an `AWS_REGION` key to your Modal secrets, as in the code example above.\n\n### Using AWS temporary security credentials\n\n`CloudBucketMount`s also support AWS temporary security credentials by passing the additional environment variable `AWS_SESSION_TOKEN`. Temporary credentials will expire and will not get renewed automatically. You will need to update the corresponding Modal Secret in order to prevent failures.\n\nYou can get temporary credentials with the [AWS CLI](https://aws.amazon.com/cli/)\n with:\n\n    $ aws configure export-credentials --format env\n    export AWS_ACCESS_KEY_ID=XXX\n    export AWS_SECRET_ACCESS_KEY=XXX\n    export AWS_SESSION_TOKEN=XXX...\n\nCopy\n\nAll these values are required.\n\n### Mounting a path within a bucket\n\nTo mount only the files under a specific subdirectory, you can specify a path prefix using `key_prefix`. Since this prefix specifies a directory, it must end in a `/`. The entire bucket is mounted when no prefix is supplied.\n\n    import modal\n    import subprocess\n    \n    app = modal.App()\n    \n    s3_bucket_name = \"s3-bucket-name\"\n    prefix = 'path/to/dir/'\n    \n    s3_access_credentials = modal.Secret.from_dict({\n        \"AWS_ACCESS_KEY_ID\": \"...\",\n        \"AWS_SECRET_ACCESS_KEY\": \"...\",\n    })\n    \n    @app.function(\n        volumes={\n            \"/my-mount\": modal.CloudBucketMount(\n                bucket_name=s3_bucket_name,\n                key_prefix=prefix,\n                secret=s3_access_credentials\n            )\n        }\n    )\n    def f():\n        subprocess.run([\"ls\", \"/my-mount\"])\n\nCopy\n\nThis will only mount the files in the bucket `s3-bucket-name` that are prefixed by `path/to/dir/`.\n\n### Read-only mode\n\nTo mount a bucket in read-only mode, set `read_only=True` as an argument.\n\n    import modal\n    import subprocess\n    \n    app = modal.App()\n    \n    s3_bucket_name = \"s3-bucket-name\"  # Bucket name not ARN.\n    s3_access_credentials = modal.Secret.from_dict({\n        \"AWS_ACCESS_KEY_ID\": \"...\",\n        \"AWS_SECRET_ACCESS_KEY\": \"...\",\n    })\n    \n    @app.function(\n        volumes={\n            \"/my-mount\": modal.CloudBucketMount(s3_bucket_name, secret=s3_access_credentials, read_only=True)\n        }\n    )\n    def f():\n        subprocess.run([\"ls\", \"/my-mount\"])\n\nCopy\n\nWhile S3 mounts supports both write and read operations, they are optimized for reading large files sequentially. Certain file operations, such as renaming files, are not supported. For a comprehensive list of supported operations, consult the [Mountpoint documentation](https://github.com/awslabs/mountpoint-s3/blob/main/doc/SEMANTICS.md)\n.\n\n### IAM permissions\n\nTo utilize `CloudBucketMount` for reading and writing files from S3 buckets, your IAM policy must include permissions for `s3:PutObject`, `s3:AbortMultipartUpload`, and `s3:DeleteObject`. These permissions are not required for mounts configured with `read_only=True`.\n\n    {\n      \"Version\": \"2012-10-17\",\n      \"Statement\": [\\\n        {\\\n          \"Sid\": \"ModalBucketAccess\",\\\n          \"Effect\": \"Allow\",\\\n          \"Action\": [\"s3:ListBucket\"],\\\n          \"Resource\": [\"arn:aws:s3:::<MY-S3-BUCKET>\"]\\\n        },\\\n        {\\\n          \"Sid\": \"ModalBucketAccess\",\\\n          \"Effect\": \"Allow\",\\\n          \"Action\": [\\\n            \"s3:GetObject\",\\\n            \"s3:PutObject\",\\\n            \"s3:AbortMultipartUpload\",\\\n            \"s3:DeleteObject\"\\\n          ],\\\n          \"Resource\": [\"arn:aws:s3:::<MY-S3-BUCKET>/*\"]\\\n        }\\\n      ]\n    }\n\nCopy\n\n[Cloud bucket mounts](#cloud-bucket-mounts)\n [Mounting Cloudflare R2 buckets](#mounting-cloudflare-r2-buckets)\n [Mounting Google Cloud Storage buckets](#mounting-google-cloud-storage-buckets)\n [Mounting S3 buckets](#mounting-s3-buckets)\n [Specifying S3 bucket region](#specifying-s3-bucket-region)\n [Using AWS temporary security credentials](#using-aws-temporary-security-credentials)\n [Mounting a path within a bucket](#mounting-a-path-within-a-bucket)\n [Read-only mode](#read-only-mode)\n [IAM permissions](#iam-permissions)\n\nSee it in action\n\n[Mount S3 buckets in Modal apps](/docs/examples/s3_bucket_mount)\n\n[Create a LoRA Playground with Modal, Gradio, and S3](/docs/examples/cloud_bucket_mount_loras)",
    "metadata": {
      "title": "Cloud bucket mounts | Modal Docs",
      "description": "The modal.CloudBucketMount is a mutable volume that allows for both reading and writing files from a cloud bucket. It supports AWS S3, Cloudflare R2, and Google Cloud Storage buckets.",
      "ogTitle": "Cloud bucket mounts",
      "ogDescription": "The modal.CloudBucketMount is a mutable volume that allows for both reading and writing files from a cloud bucket. It supports AWS S3, Cloudflare R2, and Google Cloud Storage buckets.",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/cloud-bucket-mounts",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nNetwork file systems (superseded)\n=================================\n\nModal lets you create [writeable volumes](/docs/reference/modal.NetworkFileSystem)\n that can be simultaneously attached to multiple Modal functions. These are helpful for use cases such as:\n\n1.  Storing datasets\n2.  Keeping a shared cache for expensive computations\n3.  Leveraging POSIX filesystem APIs for both local and remote data storage\n\n**Note: `NetworkFileSystem`s have been superseded.** Modal `NetworkFileSystem`s are limited by the fact that they are located in only one cloud region. Since Modal compute runs in multiple regions, this causes variable latency and throughput issues when accessing the file system.\n\nTo address this, we have a new distributed storage primitive, [modal.Volume](/docs/guide/volumes)\n, that offers fast reads and writes across all regions. `NetworkFileSystem`s are still supported and useful in some circumstances, but we recommend trying out `Volume`s first for most new projects.\n\nBasic example\n-------------\n\nThe [modal.NetworkFileSystem.from\\_name](/docs/reference/modal.NetworkFileSystem#from_name)\n constructor. You can either create this network file system using the command\n\n    modal nfs create\n\nCopy\n\nOr you can also provide `create_if_missing=True` in the code.\n\nThis can be mounted within a function by providing a mapping between mount paths and `NetworkFileSystem` objects. For example, to use a `NetworkFileSystem` to initialize a shared [shelve](https://docs.python.org/3/library/shelve.html)\n disk cache:\n\n    import shelve\n    import modal\n    \n    volume = modal.NetworkFileSystem.from_name(\"my-cache\", create_if_missing=True)\n    \n    @app.function(network_file_systems={\"/root/cache\": volume})\n    def expensive_computation(key: str):\n        with shelve.open(\"/root/cache/shelve\") as cache:\n            cached_val = cache.get(key)\n    \n        if cached_val is not None:\n            return cached_val\n    \n        # cache miss; populate value\n        ...\n\nCopy\n\nThe above implements basic disk caching, but be aware that `shelve` does not [guarantee correctness](https://docs.python.org/3/library/shelve.html#restrictions)\n in the event of concurrent read/write operations. To protect against concurrent write conflicts, the [flufl.lock](https://flufllock.readthedocs.io/en/stable/)\n package is useful. An example of that library’s usage is in the [Datasette example](/docs/examples/covid_datasette)\n.\n\nDeleting volumes\n----------------\n\nTo remove a persisted network file system, deleting all its data, you must “stop” it. This can be done via the network file system’s dashboard app page or the CLI.\n\nFor example, a file system with the name `my-vol` that lives in the `e-corp` workspace could be stopped (i.e. deleted) by going to its dashboard page at [https://modal.com/apps/e-corp/my-vol](https://modal.com/apps/e-corp/my-vol)\n and clicking the trash icon. Alternatively, you can use the file system’s app ID with [`modal app stop`](/docs/reference/cli/app#modal-app-stop)\n.\n\n(Network File Systems are currently a specialized app type within Modal, which is why deleting one is done by stopping an app.)\n\nFurther examples\n----------------\n\n*   The [Modal Podcast Transcriber](/docs/examples/whisper-transcriber)\n     uses a persisted network file system to durably store raw audio, metadata, and finished transcriptions.\n\n[Network file systems (superseded)](#network-file-systems-superseded)\n [Basic example](#basic-example)\n [Deleting volumes](#deleting-volumes)\n [Further examples](#further-examples)",
    "markdown": "* * *\n\nNetwork file systems (superseded)\n=================================\n\nModal lets you create [writeable volumes](/docs/reference/modal.NetworkFileSystem)\n that can be simultaneously attached to multiple Modal functions. These are helpful for use cases such as:\n\n1.  Storing datasets\n2.  Keeping a shared cache for expensive computations\n3.  Leveraging POSIX filesystem APIs for both local and remote data storage\n\n**Note: `NetworkFileSystem`s have been superseded.** Modal `NetworkFileSystem`s are limited by the fact that they are located in only one cloud region. Since Modal compute runs in multiple regions, this causes variable latency and throughput issues when accessing the file system.\n\nTo address this, we have a new distributed storage primitive, [modal.Volume](/docs/guide/volumes)\n, that offers fast reads and writes across all regions. `NetworkFileSystem`s are still supported and useful in some circumstances, but we recommend trying out `Volume`s first for most new projects.\n\nBasic example\n-------------\n\nThe [modal.NetworkFileSystem.from\\_name](/docs/reference/modal.NetworkFileSystem#from_name)\n constructor. You can either create this network file system using the command\n\n    modal nfs create\n\nCopy\n\nOr you can also provide `create_if_missing=True` in the code.\n\nThis can be mounted within a function by providing a mapping between mount paths and `NetworkFileSystem` objects. For example, to use a `NetworkFileSystem` to initialize a shared [shelve](https://docs.python.org/3/library/shelve.html)\n disk cache:\n\n    import shelve\n    import modal\n    \n    volume = modal.NetworkFileSystem.from_name(\"my-cache\", create_if_missing=True)\n    \n    @app.function(network_file_systems={\"/root/cache\": volume})\n    def expensive_computation(key: str):\n        with shelve.open(\"/root/cache/shelve\") as cache:\n            cached_val = cache.get(key)\n    \n        if cached_val is not None:\n            return cached_val\n    \n        # cache miss; populate value\n        ...\n\nCopy\n\nThe above implements basic disk caching, but be aware that `shelve` does not [guarantee correctness](https://docs.python.org/3/library/shelve.html#restrictions)\n in the event of concurrent read/write operations. To protect against concurrent write conflicts, the [flufl.lock](https://flufllock.readthedocs.io/en/stable/)\n package is useful. An example of that library’s usage is in the [Datasette example](/docs/examples/covid_datasette)\n.\n\nDeleting volumes\n----------------\n\nTo remove a persisted network file system, deleting all its data, you must “stop” it. This can be done via the network file system’s dashboard app page or the CLI.\n\nFor example, a file system with the name `my-vol` that lives in the `e-corp` workspace could be stopped (i.e. deleted) by going to its dashboard page at [https://modal.com/apps/e-corp/my-vol](https://modal.com/apps/e-corp/my-vol)\n and clicking the trash icon. Alternatively, you can use the file system’s app ID with [`modal app stop`](/docs/reference/cli/app#modal-app-stop)\n.\n\n(Network File Systems are currently a specialized app type within Modal, which is why deleting one is done by stopping an app.)\n\nFurther examples\n----------------\n\n*   The [Modal Podcast Transcriber](/docs/examples/whisper-transcriber)\n     uses a persisted network file system to durably store raw audio, metadata, and finished transcriptions.\n\n[Network file systems (superseded)](#network-file-systems-superseded)\n [Basic example](#basic-example)\n [Deleting volumes](#deleting-volumes)\n [Further examples](#further-examples)",
    "metadata": {
      "title": "Network file systems (superseded) | Modal Docs",
      "description": "Modal lets you create writeable volumes that can be simultaneously attached to multiple Modal functions. These are helpful for use cases such as:",
      "ogTitle": "Network file systems (superseded)",
      "ogDescription": "Modal lets you create writeable volumes that can be simultaneously attached to multiple Modal functions. These are helpful for use cases such as:",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/network-file-systems",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nDynamic sandboxes\n=================\n\nIn addition to the function interface, Modal has a direct interface for defining containers _at runtime_ and running arbitrary code inside them.\n\nThis can be useful if, for example, you want to:\n\n*   Run code generated by a language model with a list of dynamically generated requirements.\n*   Check out a git repository and run a command against it, like a test suite, or `npm lint`.\n*   Use Modal to orchestrate containers that don’t have or use Python.\n\nEach individual job is called a **Sandbox**, and can be created using the [`spawn_sandbox`](/docs/reference/modal.App#spawn_sandbox)\n function on a running app:\n\n    @app.local_entrypoint()\n    def main():\n        sb = app.spawn_sandbox(\n            \"bash\",\n            \"-c\",\n            \"cd /repo && pytest .\",\n            image=modal.Image.debian_slim().pip_install(\"pandas\"),\n            mounts=[modal.Mount.from_local_dir(\"./my_repo\", remote_path=\"/repo\")],\n            timeout=600, # 10 minutes\n        )\n    \n        sb.wait()\n    \n        if sb.returncode != 0:\n            print(f\"Tests failed with code {sb.returncode}\")\n            print(sb.stderr.read())\n\nCopy\n\nIt’s useful to note that the [`Sandbox`](/docs/reference/modal.Sandbox#modalsandboxsandbox)\n object returned above has an interface similar to Python’s [asyncio.subprocess.Process](https://docs.python.org/3/library/asyncio-subprocess.html#asyncio.subprocess.Process)\n API, and can be used in a similar way.\n\nParameters\n----------\n\nSandboxes support nearly all configuration options found in regular `modal.Function`s. Refer to [`spawn_sandbox`](/docs/reference/modal.App#spawn_sandbox)\n for further documentation on Sandbox parameterization.\n\n### Dynamically defined environments\n\nNote that any valid `Image` or `Mount` can be used with a sandbox, even if those images or mounts have not previously been defined. This also means that images and mounts can be built from requirements at **runtime**. For example, you could use a language model to write some code and define your image, and then spawn a sandbox with it. Check out [devlooper](https://github.com/modal-labs/devlooper)\n for a concrete example of this.\n\n### Returning or persisting data\n\nModal [Volume](https://modal.com/docs/reference/modal.Volume)\ns or [CloudBucketMount](/docs/guide/cloud-bucket-mounts)\ns can be attached to sandboxes. If you want to give the caller access to files written by the sandbox, you could create an ephemeral `Volume` that will be garbage collected when the app finishes:\n\n    @app.local_entrypoint()\n    def main():\n        with modal.Volume.ephemeral() as vol:\n            sb = app.spawn_sandbox(\n                \"bash\",\n                \"-c\",\n                \"echo foo > /cache/a.txt\",\n                volumes={\"/cache\": vol},\n            )\n            sb.wait()\n            for data in vol.read_file(\"a.txt\"):\n                print(data)\n\nCopy\n\nAlternatively, if you want to persist files between sandbox invocations (useful if you’re building a stateful code interpreter, for example), you can use create a persisted `Volume` with a dynamically assigned label:\n\n    vol = modal.Volume.from_name(f\"vol-{session_id}\", create_if_missing=True)\n    sb = app.spawn_sandbox(\n        \"bash\",\n        \"-c\",\n        \"echo foo > /cache/a.txt\",\n        volumes={\"/cache\": vol},\n    )\n\nCopy\n\nIsolation and security\n----------------------\n\nSandboxes can be used to run untrusted code, such as code from third parties or generated by a language model. Unlike regular `Function` runners, `Sandbox` runners do not have the ability to spawn new containers, or otherwise perform operations in your workspace. These runners are also torn down after execution, and never reused across calls. A sandbox’s network access can also be blocked with `block_network=True`.\n\nInput\n-----\n\nSandboxes support accepting input via the `stdin` attribute on the sandbox object. The `stdin` handle is a [StreamWriter](/docs/reference/modal.Sandbox#modalsandboxstreamwriter)\n object.\n\n    sandbox = app.spawn_sandbox(\n        \"bash\",\n        \"-c\",\n        \"while read line; do echo $line; done\",\n    )\n    \n    with open(\"data.txt\", \"rb\") as f:\n        for line in f.readlines():\n            sandbox.stdin.write(line)\n            sandbox.stdin.drain()  # flush line\n        sandbox.stdin.write_eof()\n        sandbox.stdin.drain()  # flush EOF\n    sandbox.wait()\n\nCopy\n\nOutput\n------\n\nThe interface for accessing Sandbox output is via the `stdout` and `stderr` attributes on the Sandbox object. These are [`LogsReader`](/docs/reference/modal.Sandbox#modalsandboxlogsreader)\n objects, and they support streaming output or reading it all in one call.\n\nThe `read` method fetches all logs until EOF and returns the entire output stream.\n\n    sandbox = app.spawn_sandbox(\"echo\", \"hello\")\n    sandbox.wait()\n    \n    print(sandbox.stdout.read())\n\nCopy\n\nTo stream output take advantage of the fact that `stdout` and `stderr` are iterable. Note that this snippet uses an _asynchronous_ iteration.\n\n    async for line in sandbox.stdout:\n        print(line)\n\nCopy\n\n[Dynamic sandboxes](#dynamic-sandboxes)\n [Parameters](#parameters)\n [Dynamically defined environments](#dynamically-defined-environments)\n [Returning or persisting data](#returning-or-persisting-data)\n [Isolation and security](#isolation-and-security)\n [Input](#input)\n [Output](#output)\n\nSee it in action\n\n[Safe LLM code execution](https://github.com/modal-labs/devlooper)",
    "markdown": "* * *\n\nDynamic sandboxes\n=================\n\nIn addition to the function interface, Modal has a direct interface for defining containers _at runtime_ and running arbitrary code inside them.\n\nThis can be useful if, for example, you want to:\n\n*   Run code generated by a language model with a list of dynamically generated requirements.\n*   Check out a git repository and run a command against it, like a test suite, or `npm lint`.\n*   Use Modal to orchestrate containers that don’t have or use Python.\n\nEach individual job is called a **Sandbox**, and can be created using the [`spawn_sandbox`](/docs/reference/modal.App#spawn_sandbox)\n function on a running app:\n\n    @app.local_entrypoint()\n    def main():\n        sb = app.spawn_sandbox(\n            \"bash\",\n            \"-c\",\n            \"cd /repo && pytest .\",\n            image=modal.Image.debian_slim().pip_install(\"pandas\"),\n            mounts=[modal.Mount.from_local_dir(\"./my_repo\", remote_path=\"/repo\")],\n            timeout=600, # 10 minutes\n        )\n    \n        sb.wait()\n    \n        if sb.returncode != 0:\n            print(f\"Tests failed with code {sb.returncode}\")\n            print(sb.stderr.read())\n\nCopy\n\nIt’s useful to note that the [`Sandbox`](/docs/reference/modal.Sandbox#modalsandboxsandbox)\n object returned above has an interface similar to Python’s [asyncio.subprocess.Process](https://docs.python.org/3/library/asyncio-subprocess.html#asyncio.subprocess.Process)\n API, and can be used in a similar way.\n\nParameters\n----------\n\nSandboxes support nearly all configuration options found in regular `modal.Function`s. Refer to [`spawn_sandbox`](/docs/reference/modal.App#spawn_sandbox)\n for further documentation on Sandbox parameterization.\n\n### Dynamically defined environments\n\nNote that any valid `Image` or `Mount` can be used with a sandbox, even if those images or mounts have not previously been defined. This also means that images and mounts can be built from requirements at **runtime**. For example, you could use a language model to write some code and define your image, and then spawn a sandbox with it. Check out [devlooper](https://github.com/modal-labs/devlooper)\n for a concrete example of this.\n\n### Returning or persisting data\n\nModal [Volume](https://modal.com/docs/reference/modal.Volume)\ns or [CloudBucketMount](/docs/guide/cloud-bucket-mounts)\ns can be attached to sandboxes. If you want to give the caller access to files written by the sandbox, you could create an ephemeral `Volume` that will be garbage collected when the app finishes:\n\n    @app.local_entrypoint()\n    def main():\n        with modal.Volume.ephemeral() as vol:\n            sb = app.spawn_sandbox(\n                \"bash\",\n                \"-c\",\n                \"echo foo > /cache/a.txt\",\n                volumes={\"/cache\": vol},\n            )\n            sb.wait()\n            for data in vol.read_file(\"a.txt\"):\n                print(data)\n\nCopy\n\nAlternatively, if you want to persist files between sandbox invocations (useful if you’re building a stateful code interpreter, for example), you can use create a persisted `Volume` with a dynamically assigned label:\n\n    vol = modal.Volume.from_name(f\"vol-{session_id}\", create_if_missing=True)\n    sb = app.spawn_sandbox(\n        \"bash\",\n        \"-c\",\n        \"echo foo > /cache/a.txt\",\n        volumes={\"/cache\": vol},\n    )\n\nCopy\n\nIsolation and security\n----------------------\n\nSandboxes can be used to run untrusted code, such as code from third parties or generated by a language model. Unlike regular `Function` runners, `Sandbox` runners do not have the ability to spawn new containers, or otherwise perform operations in your workspace. These runners are also torn down after execution, and never reused across calls. A sandbox’s network access can also be blocked with `block_network=True`.\n\nInput\n-----\n\nSandboxes support accepting input via the `stdin` attribute on the sandbox object. The `stdin` handle is a [StreamWriter](/docs/reference/modal.Sandbox#modalsandboxstreamwriter)\n object.\n\n    sandbox = app.spawn_sandbox(\n        \"bash\",\n        \"-c\",\n        \"while read line; do echo $line; done\",\n    )\n    \n    with open(\"data.txt\", \"rb\") as f:\n        for line in f.readlines():\n            sandbox.stdin.write(line)\n            sandbox.stdin.drain()  # flush line\n        sandbox.stdin.write_eof()\n        sandbox.stdin.drain()  # flush EOF\n    sandbox.wait()\n\nCopy\n\nOutput\n------\n\nThe interface for accessing Sandbox output is via the `stdout` and `stderr` attributes on the Sandbox object. These are [`LogsReader`](/docs/reference/modal.Sandbox#modalsandboxlogsreader)\n objects, and they support streaming output or reading it all in one call.\n\nThe `read` method fetches all logs until EOF and returns the entire output stream.\n\n    sandbox = app.spawn_sandbox(\"echo\", \"hello\")\n    sandbox.wait()\n    \n    print(sandbox.stdout.read())\n\nCopy\n\nTo stream output take advantage of the fact that `stdout` and `stderr` are iterable. Note that this snippet uses an _asynchronous_ iteration.\n\n    async for line in sandbox.stdout:\n        print(line)\n\nCopy\n\n[Dynamic sandboxes](#dynamic-sandboxes)\n [Parameters](#parameters)\n [Dynamically defined environments](#dynamically-defined-environments)\n [Returning or persisting data](#returning-or-persisting-data)\n [Isolation and security](#isolation-and-security)\n [Input](#input)\n [Output](#output)\n\nSee it in action\n\n[Safe LLM code execution](https://github.com/modal-labs/devlooper)",
    "metadata": {
      "title": "Dynamic sandboxes | Modal Docs",
      "description": "In addition to the function interface, Modal has a direct interface for defining containers at runtime and running arbitrary code inside them.",
      "ogTitle": "Dynamic sandboxes",
      "ogDescription": "In addition to the function interface, Modal has a direct interface for defining containers at runtime and running arbitrary code inside them.",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/sandbox",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nCold start performance\n======================\n\nModal Functions are run in [containers](/docs/guide/custom-container)\n.\n\nIf a container is already ready to run your Function, it will be reused.\n\nIf not, Modal spins up a new container. This is known as a _cold start_, and it is often associated with higher latency.\n\nThere are two sources of increased latency during cold starts:\n\n1.  inputs may **spend more time waiting** in a queue for a container to become ready or “warm”.\n2.  when an input is handled by the container that just started, there may be **extra work that only needs to be done on the first invocation** (“amortized work”).\n\nThis guide presents techniques and Modal features for reducing the impact of both queueing and amortized work on observed latencies.\n\nIf you are invoking Functions with no warm containers or if you otherwise see inputs spending too much time in the “pending” state, you should [target queueing time for optimization](#reduce-time-spent-queueing-for-warm-containers)\n.\n\nIf you see some Function invocations taking much longer than others, and those invocations are the first handled by a new container, you should [target amortized work for optimization](#reduce-time-spent-on-amortized-work)\n.\n\nReduce time spent queueing for warm containers\n----------------------------------------------\n\nNew containers are booted when there are not enough other warm containers to to handle the current number of inputs.\n\nFor example, the first time you send an input to a Function, there are zero warm containers and there is one input, so a single container must be booted up. The total latency for the input will include the time it takes to boot a container.\n\nIf you send another input right after the first one finishes, there will be one warm container and one pending input, and no new container will be booted.\n\nGeneralizing, there are two factors that affect the time inputs spend queueing: the time it takes for a container to boot and become warm and the chance a warm container is available to handle an input.\n\n### Warm up containers faster\n\nThe time taken for a container to become warm and ready for inputs can range from seconds to minutes.\n\nModal’s custom container stack has been heavily optimized to reduce this time. Containers boot in about one second.\n\nBut before a container is considered warm and ready to handle inputs, we need to execute any logic in your code’s global scope (such as imports) or in any [`modal.enter` methods](/docs/guide/lifecycle-functions)\n. So if your boots are slow, these are the first places to work on optimization.\n\nFor example, you might be downloading a large model from a model server during the boot process. You can instead [download the model during the build phase](/docs/guides/model-weights)\n, which only runs when the container image changes, at most once per deployment.\n\nFor models in the tens of gigabytes, this can reduce boot times from minutes to seconds.\n\n### Run more warm containers\n\nIt is not always possible to speed up boots sufficiently. For example, seconds of added latency to load a model may not be acceptable in an interactive setting.\n\nIn this case, the only option is to have more warm containers running. This increases the chance that an input will be handled by a warm container, for example one that finishes an input while another container is booting.\n\nModal currently exposes two parameters to control how many containers will be warm: `container_idle_timeout` and `keep_warm`.\n\n#### Keep containers warm for longer with `container_idle_timeout`\n\nBy default, Modal containers spin down after 60 seconds of inactivity. You can configure this time by setting the `container_idle_timeout` value on the [`@function`](/docs/reference/modal.App#function)\n decorator. The timeout is measured in seconds and can be set to any value between two seconds and twenty minutes.\n\n    import modal\n    \n    app = modal.App()\n    \n    @app.function(container_idle_timeout=300)\n    def my_idle_greeting():\n        return {\"hello\": \"world\"}\n\nCopy\n\n#### Maintain a warm pool with `keep_warm`\n\nKeeping already warm containers around doesn’t help if there are no warm containers to begin with, as when Functions scale from zero.\n\nTo keep some containers warm and running at all times, set the `keep_warm` value on the [`@function`](/docs/reference/modal.App#function)\n decorator. This sets the minimum number of containers that will always be ready to run your Function. Modal will still scale up (and spin down) more containers if the demand for your Function exceeds the `keep_warm` value, as usual.\n\n    import modal\n    \n    app = modal.App()\n    \n    @app.function(keep_warm=3)\n    @modal.web_endpoint()\n    def my_warm_greeting():\n        return {\"hello\": \"world\"}\n\nCopy\n\nReduce time spent on amortized work\n-----------------------------------\n\nSome work is done only the first time that a function is invoked, but used on every subsequent invocation. This is [_amortized work_](https://www.cs.cornell.edu/courses/cs312/2006sp/lectures/lec18.html)\n.\n\nFor example, you may be using a large pre-trained model whose weights need to be loaded from disk to memory the first time it is used.\n\nThis results in longer latencies for the first invocation of a warm container, which shows up in the application as occasional slow calls (high tail latency).\n\n### Move amortized work to build or warm up\n\nAs with work at warm up time, some work done on the first invocation can be moved out to build time or to warm up time.\n\nAny work that can be saved to disk, like [downloading model weights](/docs/guide/model-weights)\n, should be done during the build phase.\n\nIf you can move the logic for the amortized work out of the function body and into a [container `enter` method](https://modal.com/docs/guide/lifecycle-functions#container-lifecycle-beta)\n, you can move work into the warm up period. Containers will not be considered warm until all `enter` methods have completed, so no inputs will have elevated latency inside the body of the function.\n\nFor more on how to use `enter` with machine learning model weights, see [this guide](/docs/guides/model-weights)\n.\n\nNote that `enter` doesn’t get rid of the latency — it just moves the latency to the warm up period, where it can be handled by [running more warm containers](#run-more-warm-containers)\n.\n\n### Amortize work across cold starts with memory snapshots (beta)\n\nTail latencies can be reduced by amortizing work across cold starts with memory snapshots.\n\nInvocations of a function after the first are faster in part because the memory is already populated with values that would otherwise need to be computed, like the contents of imported libraries.\n\nMemory snapshotting is a developer preview feature that captures the state of a container’s memory at user-controlled points after it has been warmed up and reuses that state in future boots.\n\nRefer to the [memory snapshot](/docs/guide/memory-snapshot)\n page for details.\n\n### Target amortized work for optimization\n\nSometimes, there is nothing to be done but to speed this work up.\n\nHere, we share specific patterns that show up in optimizing amortized work in Modal functions.\n\n#### Load multiple large files concurrently\n\nOften Modal applications need to read large files into memory (eg. model weights) before they can process inputs. Where feasible these large file reads should happen concurrently and not sequentially. Concurrent IO takes full advantage of our platform’s high disk and network bandwidth to reduce latency.\n\nOne common example of slow sequential IO is loading multiple independent Huggingface `transformers` models in series.\n\n    from transformers import CLIPProcessor, CLIPModel, BlipProcessor, BlipForConditionalGeneration\n    model_a = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n    processor_a = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n    model_b = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n    processor_b = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n\nCopy\n\nThe above snippet does four `.from_pretrained` loads sequentially. None of the components depend on another being already loaded in memory, so they can be loaded concurrently instead.\n\nThey could instead be loaded concurrently using a function like this:\n\n    from concurrent.futures import ThreadPoolExecutor, as_completed\n    from transformers import CLIPProcessor, CLIPModel, BlipProcessor, BlipForConditionalGeneration\n    \n    def load_models_concurrently(load_functions_map: dict) -> dict:\n        model_id_to_model = {}\n        with ThreadPoolExecutor(max_workers=len(load_functions_map)) as executor:\n            future_to_model_id = {\n                executor.submit(load_fn): model_id\n                for model_id, load_fn in load_functions_map.items()\n            }\n            for future in as_completed(future_to_model_id.keys()):\n                model_id_to_model[future_to_model_id[future]] = future.result()\n        return model_id_to_model\n    \n    components = load_models_concurrently({\n        \"clip_model\": lambda: CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\"),\n        \"clip_processor\": lambda: CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\"),\n        \"blip_model\": lambda: BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\"),\n        \"blip_processor\": lambda: BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n    })\n\nCopy\n\nIf performing concurrent IO on large file reads does _not_ speed up your cold starts, it’s possible that some part of your function’s code is holding the Python [GIL](https://wiki.python.org/moin/GlobalInterpreterLock)\n and reducing the efficacy of the multi-threaded executor.\n\n[Cold start performance](#cold-start-performance)\n [Reduce time spent queueing for warm containers](#reduce-time-spent-queueing-for-warm-containers)\n [Warm up containers faster](#warm-up-containers-faster)\n [Run more warm containers](#run-more-warm-containers)\n [Keep containers warm for longer with container\\_idle\\_timeout](#keep-containers-warm-for-longer-with-container_idle_timeout)\n [Maintain a warm pool with keep\\_warm](#maintain-a-warm-pool-with-keep_warm)\n [Reduce time spent on amortized work](#reduce-time-spent-on-amortized-work)\n [Move amortized work to build or warm up](#move-amortized-work-to-build-or-warm-up)\n [Amortize work across cold starts with memory snapshots (beta)](#amortize-work-across-cold-starts-with-memory-snapshots-beta)\n [Target amortized work for optimization](#target-amortized-work-for-optimization)\n [Load multiple large files concurrently](#load-multiple-large-files-concurrently)",
    "markdown": "* * *\n\nCold start performance\n======================\n\nModal Functions are run in [containers](/docs/guide/custom-container)\n.\n\nIf a container is already ready to run your Function, it will be reused.\n\nIf not, Modal spins up a new container. This is known as a _cold start_, and it is often associated with higher latency.\n\nThere are two sources of increased latency during cold starts:\n\n1.  inputs may **spend more time waiting** in a queue for a container to become ready or “warm”.\n2.  when an input is handled by the container that just started, there may be **extra work that only needs to be done on the first invocation** (“amortized work”).\n\nThis guide presents techniques and Modal features for reducing the impact of both queueing and amortized work on observed latencies.\n\nIf you are invoking Functions with no warm containers or if you otherwise see inputs spending too much time in the “pending” state, you should [target queueing time for optimization](#reduce-time-spent-queueing-for-warm-containers)\n.\n\nIf you see some Function invocations taking much longer than others, and those invocations are the first handled by a new container, you should [target amortized work for optimization](#reduce-time-spent-on-amortized-work)\n.\n\nReduce time spent queueing for warm containers\n----------------------------------------------\n\nNew containers are booted when there are not enough other warm containers to to handle the current number of inputs.\n\nFor example, the first time you send an input to a Function, there are zero warm containers and there is one input, so a single container must be booted up. The total latency for the input will include the time it takes to boot a container.\n\nIf you send another input right after the first one finishes, there will be one warm container and one pending input, and no new container will be booted.\n\nGeneralizing, there are two factors that affect the time inputs spend queueing: the time it takes for a container to boot and become warm and the chance a warm container is available to handle an input.\n\n### Warm up containers faster\n\nThe time taken for a container to become warm and ready for inputs can range from seconds to minutes.\n\nModal’s custom container stack has been heavily optimized to reduce this time. Containers boot in about one second.\n\nBut before a container is considered warm and ready to handle inputs, we need to execute any logic in your code’s global scope (such as imports) or in any [`modal.enter` methods](/docs/guide/lifecycle-functions)\n. So if your boots are slow, these are the first places to work on optimization.\n\nFor example, you might be downloading a large model from a model server during the boot process. You can instead [download the model during the build phase](/docs/guides/model-weights)\n, which only runs when the container image changes, at most once per deployment.\n\nFor models in the tens of gigabytes, this can reduce boot times from minutes to seconds.\n\n### Run more warm containers\n\nIt is not always possible to speed up boots sufficiently. For example, seconds of added latency to load a model may not be acceptable in an interactive setting.\n\nIn this case, the only option is to have more warm containers running. This increases the chance that an input will be handled by a warm container, for example one that finishes an input while another container is booting.\n\nModal currently exposes two parameters to control how many containers will be warm: `container_idle_timeout` and `keep_warm`.\n\n#### Keep containers warm for longer with `container_idle_timeout`\n\nBy default, Modal containers spin down after 60 seconds of inactivity. You can configure this time by setting the `container_idle_timeout` value on the [`@function`](/docs/reference/modal.App#function)\n decorator. The timeout is measured in seconds and can be set to any value between two seconds and twenty minutes.\n\n    import modal\n    \n    app = modal.App()\n    \n    @app.function(container_idle_timeout=300)\n    def my_idle_greeting():\n        return {\"hello\": \"world\"}\n\nCopy\n\n#### Maintain a warm pool with `keep_warm`\n\nKeeping already warm containers around doesn’t help if there are no warm containers to begin with, as when Functions scale from zero.\n\nTo keep some containers warm and running at all times, set the `keep_warm` value on the [`@function`](/docs/reference/modal.App#function)\n decorator. This sets the minimum number of containers that will always be ready to run your Function. Modal will still scale up (and spin down) more containers if the demand for your Function exceeds the `keep_warm` value, as usual.\n\n    import modal\n    \n    app = modal.App()\n    \n    @app.function(keep_warm=3)\n    @modal.web_endpoint()\n    def my_warm_greeting():\n        return {\"hello\": \"world\"}\n\nCopy\n\nReduce time spent on amortized work\n-----------------------------------\n\nSome work is done only the first time that a function is invoked, but used on every subsequent invocation. This is [_amortized work_](https://www.cs.cornell.edu/courses/cs312/2006sp/lectures/lec18.html)\n.\n\nFor example, you may be using a large pre-trained model whose weights need to be loaded from disk to memory the first time it is used.\n\nThis results in longer latencies for the first invocation of a warm container, which shows up in the application as occasional slow calls (high tail latency).\n\n### Move amortized work to build or warm up\n\nAs with work at warm up time, some work done on the first invocation can be moved out to build time or to warm up time.\n\nAny work that can be saved to disk, like [downloading model weights](/docs/guide/model-weights)\n, should be done during the build phase.\n\nIf you can move the logic for the amortized work out of the function body and into a [container `enter` method](https://modal.com/docs/guide/lifecycle-functions#container-lifecycle-beta)\n, you can move work into the warm up period. Containers will not be considered warm until all `enter` methods have completed, so no inputs will have elevated latency inside the body of the function.\n\nFor more on how to use `enter` with machine learning model weights, see [this guide](/docs/guides/model-weights)\n.\n\nNote that `enter` doesn’t get rid of the latency — it just moves the latency to the warm up period, where it can be handled by [running more warm containers](#run-more-warm-containers)\n.\n\n### Amortize work across cold starts with memory snapshots (beta)\n\nTail latencies can be reduced by amortizing work across cold starts with memory snapshots.\n\nInvocations of a function after the first are faster in part because the memory is already populated with values that would otherwise need to be computed, like the contents of imported libraries.\n\nMemory snapshotting is a developer preview feature that captures the state of a container’s memory at user-controlled points after it has been warmed up and reuses that state in future boots.\n\nRefer to the [memory snapshot](/docs/guide/memory-snapshot)\n page for details.\n\n### Target amortized work for optimization\n\nSometimes, there is nothing to be done but to speed this work up.\n\nHere, we share specific patterns that show up in optimizing amortized work in Modal functions.\n\n#### Load multiple large files concurrently\n\nOften Modal applications need to read large files into memory (eg. model weights) before they can process inputs. Where feasible these large file reads should happen concurrently and not sequentially. Concurrent IO takes full advantage of our platform’s high disk and network bandwidth to reduce latency.\n\nOne common example of slow sequential IO is loading multiple independent Huggingface `transformers` models in series.\n\n    from transformers import CLIPProcessor, CLIPModel, BlipProcessor, BlipForConditionalGeneration\n    model_a = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n    processor_a = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n    model_b = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n    processor_b = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n\nCopy\n\nThe above snippet does four `.from_pretrained` loads sequentially. None of the components depend on another being already loaded in memory, so they can be loaded concurrently instead.\n\nThey could instead be loaded concurrently using a function like this:\n\n    from concurrent.futures import ThreadPoolExecutor, as_completed\n    from transformers import CLIPProcessor, CLIPModel, BlipProcessor, BlipForConditionalGeneration\n    \n    def load_models_concurrently(load_functions_map: dict) -> dict:\n        model_id_to_model = {}\n        with ThreadPoolExecutor(max_workers=len(load_functions_map)) as executor:\n            future_to_model_id = {\n                executor.submit(load_fn): model_id\n                for model_id, load_fn in load_functions_map.items()\n            }\n            for future in as_completed(future_to_model_id.keys()):\n                model_id_to_model[future_to_model_id[future]] = future.result()\n        return model_id_to_model\n    \n    components = load_models_concurrently({\n        \"clip_model\": lambda: CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\"),\n        \"clip_processor\": lambda: CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\"),\n        \"blip_model\": lambda: BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\"),\n        \"blip_processor\": lambda: BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n    })\n\nCopy\n\nIf performing concurrent IO on large file reads does _not_ speed up your cold starts, it’s possible that some part of your function’s code is holding the Python [GIL](https://wiki.python.org/moin/GlobalInterpreterLock)\n and reducing the efficacy of the multi-threaded executor.\n\n[Cold start performance](#cold-start-performance)\n [Reduce time spent queueing for warm containers](#reduce-time-spent-queueing-for-warm-containers)\n [Warm up containers faster](#warm-up-containers-faster)\n [Run more warm containers](#run-more-warm-containers)\n [Keep containers warm for longer with container\\_idle\\_timeout](#keep-containers-warm-for-longer-with-container_idle_timeout)\n [Maintain a warm pool with keep\\_warm](#maintain-a-warm-pool-with-keep_warm)\n [Reduce time spent on amortized work](#reduce-time-spent-on-amortized-work)\n [Move amortized work to build or warm up](#move-amortized-work-to-build-or-warm-up)\n [Amortize work across cold starts with memory snapshots (beta)](#amortize-work-across-cold-starts-with-memory-snapshots-beta)\n [Target amortized work for optimization](#target-amortized-work-for-optimization)\n [Load multiple large files concurrently](#load-multiple-large-files-concurrently)",
    "metadata": {
      "title": "Cold start performance | Modal Docs",
      "description": "Modal Functions are run in containers.",
      "ogTitle": "Cold start performance",
      "ogDescription": "Modal Functions are run in containers.",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/cold-start",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nMemory Snapshot (beta)\n======================\n\nYou can improve cold-start performance for some functions using the Memory Snapshot feature.\n\nSnapshots happen after your function’s import sequence. During import, your app reads many files from the file system, which can potentially be expensive. For instance [`torch` is hundreds of MiB](https://pypi.org/project/torch/#files)\n.\n\nMemory snapshots are created after a function is done importing packages but before it is called. We save that snapshot as a file. Then, every time your function is invoked, we restore memory from the snapshot. The result is increased cold boot performance: functions with memory snapshots enabled **typically start 1.5-3x faster**.\n\nYou don’t need to modify your function to take advantage of snapshotting in most cases (see below).\n\nThis is a _beta_ feature. Let us know in [Modal Slack](https://modal.com/slack)\n if you find any issues.\n\nEnabling automatic snapshots\n----------------------------\n\nMemory Snapshot is a beta feature. It is available as a flag in the function decorator. You can enable it as follows:\n\n    import modal\n    \n    app = modal.App(\"example-memory-snapshot\")\n    \n    \n    @app.function(enable_memory_snapshot=True)\n    def my_func():\n        print(\"hello\")\n\nCopy\n\nThen deploy the app with `modal deploy`.\n\nKeep the following in mind when using Memory Snapshot:\n\n*   Modal may take a snapshot only after your function runs the first few times, not necessarily on the first run (see [Snapshot compatibility](#snapshot-compatibility)\n     section).\n*   Creating memory snapshots adds latency to a function start time, so expect your function to be slower to start during the first invocations. Subsequent runs should be faster.\n\nControlling snapshot memory\n---------------------------\n\nYou can also snapshot programs that use a lot of memory with a class interface. This allows you to start programs that use large model weights faster, among other things.\n\nWith memory snapshots enabled, you can load model weights into CPU memory _before_ creating the snapshot. On every subsequent cold boot, your function will resume from that point. Because we serialize the memory state—including the model weights—in an efficient format, it can start up in less time than it originally took to load the model.\n\nThis example loads BGE embeddigs into CPU memory and creates a memory snapshot. When your Function cold boots, it will move those weights onto a GPU in the function `move_to_gpu()`.\n\nYou can this use feature in classes by setting `enable_memory_snapshot=True` and then marking the methods that you want to run before saving the snapshot with `@enter(snap=True)`. Conversely, decorate methods with `@enter(snap=False)` when you want them to run on every cold boot after saving or resuming from the snapshot state.\n\n    import modal\n    \n    image = (\n        modal.Image.debian_slim()\n            .pip_install(\"sentence-transformers\")\n    )\n    app = modal.App(\"sentence-transformers\", image=image)\n    \n    \n    with image.imports():\n        from sentence_transformers import SentenceTransformer\n    \n    \n    @app.cls(\n        gpu=modal.gpu.A10G(),\n        enable_memory_snapshot=True,\n    )\n    class Embedder:\n    \n        # model_id = \"BAAI/bge-large-en-v1.5\"\n        model_id = \"BAAI/bge-small-en-v1.5\"\n    \n        @modal.build()\n        def build(self):\n            model = SentenceTransformer(self.model_id)\n            model.save(\"/model.bge\")\n    \n        @modal.enter(snap=True)\n        def load(self):\n            # Create a memory snapshot with the model loaded in CPU memory.\n            self.model = SentenceTransformer(\"/model.bge\", device=\"cpu\")\n    \n        @modal.enter(snap=False)\n        def setup(self):\n            # Move the model to a GPU before doing any work.\n            self.model.to(\"cuda\")\n    \n        @modal.method()\n        def run(self, sentences:list[str]):\n            embeddings = self.model.encode(sentences, normalize_embeddings=True)\n            print(embeddings)\n    \n    \n    @app.local_entrypoint()\n    def main():\n        sentences = [\"what is the meaning of life?\"]\n        Embedder().run.remote(sentences)\n    \n    \n    if __name__ == \"__main__\":\n        cls = modal.Cls.lookup(\"sentence-transformers\", \"Embedder\")\n    \n        sentences = [\"what is the meaning of life?\"]\n        cls().run.remote(sentences)\n\nCopy\n\nThis reduces the time it takes for our app to boot by about 3x, from ~14s to ~4.8s.\n\nSnapshot compatibility\n----------------------\n\nModal will create memory snapshots for every new version of your function. Changing your function or updating its dependencies will trigger a new snapshotting operation when you run your function anew.\n\nAdditionally, you may observe in application logs your function being memory snapshots multiple times during its first few invocations. This happens because Modal will create a memory snapshot for every CPU type and runtime version in our fleet. We typically need 3-5 snapshots to cover our entire fleet. The cold boot benefits should greatly outweigh the penalty of creating multiple snapshots.\n\nKnown limitations\n-----------------\n\nMemory Snapshot is still in _beta_. Please report any issues on our [community Slack server](/slack)\n.\n\n### No GPUs available during the snapshotting phase\n\nIt’s currently not possible to snapshot GPU memory. We avoid exposing GPU devices to your function during the snapshotting stage (e.g. when `@enter(snap=True)`). NVIDIA drivers are available but no GPU devices are. This can be a problem if you need the GPU — for example, you may need to compile a package. We suggest using the [`@build`](/docs/guide/lifecycle-functions#build)\n decorator and store outputs in disk as part of your image. You can then load these into CPU memory and successfully snapshot your function. Then, when invoking your function, you can move objects to GPU memory for more details.\n\nIf your program calls functions that check if GPUs are availale during snapshots and then in restore, it will get different results in each stage.\n\n    from modal import App, enter, method\n    \n    app = App()\n    \n    @app.cls(enable_memory_snapshots=True)\n    class GPUAvailability:\n    \n        @enter(snap=True)\n        def no_gpus_available_during_snapshots(self):\n            import torch\n            print(f\"GPUs available: {torch.cuda.is_available()}\")\n    \n        @enter(snap=False)\n        def gpus_available_during_restore(self):\n            import torch\n            print(f\"GPUs available: {torch.cuda.is_available()}\")\n\nCopy\n\nIn the example above, GPUs are not available when `no_gpus_available_during_snapshots()` is called but are available when your app is restored and `gpus_available_during_restore()` is called.\n\n### Network connectivity is not available during the snapshotting phase\n\nIt is on our roadmap to enable this, but currently network is blocked during snapshotting phase. If your application code attempts to connect to the internet during snapshotting it will throw exceptions related to connection issues and DNS, for example `urllib3.exceptions.NameResolutionError`.\n\nA workaround is download data during the `modal.Image` build instead.\n\n### Filesystem writes are not snapshotted\n\nCurrently only the container memory is snapshotted, but your function may modify the filesystem during the snapshotting phase and the lost writes can break the function code on restore.\n\nWe are actively working to incorporate filesystem modifications into snapshots so that this failure case is removed.\n\n### Cached GPU device queries\n\nPyTorch’s `torch.cuda.device_count()` function will [cache its result after first execution](https://github.com/pytorch/pytorch/issues/95073)\n, providing incorrect results when used with snapshotting because the GPU availability changes between snapshotting and restore (see [_No GPUs available during the snapshotting\\_phase_](#no-gpus-available-during-the-snapshotting-phase)\n section above).\n\nA workaround is to patch `torch` to use a non-caching device count query function:\n\n    torch.cuda.device_count = torch.cuda._device_count_nvml\n\nCopy\n\n### Randomness and uniqueness\n\nIf your applications depend on uniqueness of state, you must evaluate your function code and verify that it is resilient to snapshotting operations. For example, if a variable is randomly initialized and snapshotted, that variable will be identical after every restore, possibly breaking uniqueness expectations of the proceeding function code.\n\n[Memory Snapshot (beta)](#memory-snapshot-beta)\n [Enabling automatic snapshots](#enabling-automatic-snapshots)\n [Controlling snapshot memory](#controlling-snapshot-memory)\n [Snapshot compatibility](#snapshot-compatibility)\n [Known limitations](#known-limitations)\n [No GPUs available during the snapshotting phase](#no-gpus-available-during-the-snapshotting-phase)\n [Network connectivity is not available during the snapshotting phase](#network-connectivity-is-not-available-during-the-snapshotting-phase)\n [Filesystem writes are not snapshotted](#filesystem-writes-are-not-snapshotted)\n [Cached GPU device queries](#cached-gpu-device-queries)\n [Randomness and uniqueness](#randomness-and-uniqueness)",
    "markdown": "* * *\n\nMemory Snapshot (beta)\n======================\n\nYou can improve cold-start performance for some functions using the Memory Snapshot feature.\n\nSnapshots happen after your function’s import sequence. During import, your app reads many files from the file system, which can potentially be expensive. For instance [`torch` is hundreds of MiB](https://pypi.org/project/torch/#files)\n.\n\nMemory snapshots are created after a function is done importing packages but before it is called. We save that snapshot as a file. Then, every time your function is invoked, we restore memory from the snapshot. The result is increased cold boot performance: functions with memory snapshots enabled **typically start 1.5-3x faster**.\n\nYou don’t need to modify your function to take advantage of snapshotting in most cases (see below).\n\nThis is a _beta_ feature. Let us know in [Modal Slack](https://modal.com/slack)\n if you find any issues.\n\nEnabling automatic snapshots\n----------------------------\n\nMemory Snapshot is a beta feature. It is available as a flag in the function decorator. You can enable it as follows:\n\n    import modal\n    \n    app = modal.App(\"example-memory-snapshot\")\n    \n    \n    @app.function(enable_memory_snapshot=True)\n    def my_func():\n        print(\"hello\")\n\nCopy\n\nThen deploy the app with `modal deploy`.\n\nKeep the following in mind when using Memory Snapshot:\n\n*   Modal may take a snapshot only after your function runs the first few times, not necessarily on the first run (see [Snapshot compatibility](#snapshot-compatibility)\n     section).\n*   Creating memory snapshots adds latency to a function start time, so expect your function to be slower to start during the first invocations. Subsequent runs should be faster.\n\nControlling snapshot memory\n---------------------------\n\nYou can also snapshot programs that use a lot of memory with a class interface. This allows you to start programs that use large model weights faster, among other things.\n\nWith memory snapshots enabled, you can load model weights into CPU memory _before_ creating the snapshot. On every subsequent cold boot, your function will resume from that point. Because we serialize the memory state—including the model weights—in an efficient format, it can start up in less time than it originally took to load the model.\n\nThis example loads BGE embeddigs into CPU memory and creates a memory snapshot. When your Function cold boots, it will move those weights onto a GPU in the function `move_to_gpu()`.\n\nYou can this use feature in classes by setting `enable_memory_snapshot=True` and then marking the methods that you want to run before saving the snapshot with `@enter(snap=True)`. Conversely, decorate methods with `@enter(snap=False)` when you want them to run on every cold boot after saving or resuming from the snapshot state.\n\n    import modal\n    \n    image = (\n        modal.Image.debian_slim()\n            .pip_install(\"sentence-transformers\")\n    )\n    app = modal.App(\"sentence-transformers\", image=image)\n    \n    \n    with image.imports():\n        from sentence_transformers import SentenceTransformer\n    \n    \n    @app.cls(\n        gpu=modal.gpu.A10G(),\n        enable_memory_snapshot=True,\n    )\n    class Embedder:\n    \n        # model_id = \"BAAI/bge-large-en-v1.5\"\n        model_id = \"BAAI/bge-small-en-v1.5\"\n    \n        @modal.build()\n        def build(self):\n            model = SentenceTransformer(self.model_id)\n            model.save(\"/model.bge\")\n    \n        @modal.enter(snap=True)\n        def load(self):\n            # Create a memory snapshot with the model loaded in CPU memory.\n            self.model = SentenceTransformer(\"/model.bge\", device=\"cpu\")\n    \n        @modal.enter(snap=False)\n        def setup(self):\n            # Move the model to a GPU before doing any work.\n            self.model.to(\"cuda\")\n    \n        @modal.method()\n        def run(self, sentences:list[str]):\n            embeddings = self.model.encode(sentences, normalize_embeddings=True)\n            print(embeddings)\n    \n    \n    @app.local_entrypoint()\n    def main():\n        sentences = [\"what is the meaning of life?\"]\n        Embedder().run.remote(sentences)\n    \n    \n    if __name__ == \"__main__\":\n        cls = modal.Cls.lookup(\"sentence-transformers\", \"Embedder\")\n    \n        sentences = [\"what is the meaning of life?\"]\n        cls().run.remote(sentences)\n\nCopy\n\nThis reduces the time it takes for our app to boot by about 3x, from ~14s to ~4.8s.\n\nSnapshot compatibility\n----------------------\n\nModal will create memory snapshots for every new version of your function. Changing your function or updating its dependencies will trigger a new snapshotting operation when you run your function anew.\n\nAdditionally, you may observe in application logs your function being memory snapshots multiple times during its first few invocations. This happens because Modal will create a memory snapshot for every CPU type and runtime version in our fleet. We typically need 3-5 snapshots to cover our entire fleet. The cold boot benefits should greatly outweigh the penalty of creating multiple snapshots.\n\nKnown limitations\n-----------------\n\nMemory Snapshot is still in _beta_. Please report any issues on our [community Slack server](/slack)\n.\n\n### No GPUs available during the snapshotting phase\n\nIt’s currently not possible to snapshot GPU memory. We avoid exposing GPU devices to your function during the snapshotting stage (e.g. when `@enter(snap=True)`). NVIDIA drivers are available but no GPU devices are. This can be a problem if you need the GPU — for example, you may need to compile a package. We suggest using the [`@build`](/docs/guide/lifecycle-functions#build)\n decorator and store outputs in disk as part of your image. You can then load these into CPU memory and successfully snapshot your function. Then, when invoking your function, you can move objects to GPU memory for more details.\n\nIf your program calls functions that check if GPUs are availale during snapshots and then in restore, it will get different results in each stage.\n\n    from modal import App, enter, method\n    \n    app = App()\n    \n    @app.cls(enable_memory_snapshots=True)\n    class GPUAvailability:\n    \n        @enter(snap=True)\n        def no_gpus_available_during_snapshots(self):\n            import torch\n            print(f\"GPUs available: {torch.cuda.is_available()}\")\n    \n        @enter(snap=False)\n        def gpus_available_during_restore(self):\n            import torch\n            print(f\"GPUs available: {torch.cuda.is_available()}\")\n\nCopy\n\nIn the example above, GPUs are not available when `no_gpus_available_during_snapshots()` is called but are available when your app is restored and `gpus_available_during_restore()` is called.\n\n### Network connectivity is not available during the snapshotting phase\n\nIt is on our roadmap to enable this, but currently network is blocked during snapshotting phase. If your application code attempts to connect to the internet during snapshotting it will throw exceptions related to connection issues and DNS, for example `urllib3.exceptions.NameResolutionError`.\n\nA workaround is download data during the `modal.Image` build instead.\n\n### Filesystem writes are not snapshotted\n\nCurrently only the container memory is snapshotted, but your function may modify the filesystem during the snapshotting phase and the lost writes can break the function code on restore.\n\nWe are actively working to incorporate filesystem modifications into snapshots so that this failure case is removed.\n\n### Cached GPU device queries\n\nPyTorch’s `torch.cuda.device_count()` function will [cache its result after first execution](https://github.com/pytorch/pytorch/issues/95073)\n, providing incorrect results when used with snapshotting because the GPU availability changes between snapshotting and restore (see [_No GPUs available during the snapshotting\\_phase_](#no-gpus-available-during-the-snapshotting-phase)\n section above).\n\nA workaround is to patch `torch` to use a non-caching device count query function:\n\n    torch.cuda.device_count = torch.cuda._device_count_nvml\n\nCopy\n\n### Randomness and uniqueness\n\nIf your applications depend on uniqueness of state, you must evaluate your function code and verify that it is resilient to snapshotting operations. For example, if a variable is randomly initialized and snapshotted, that variable will be identical after every restore, possibly breaking uniqueness expectations of the proceeding function code.\n\n[Memory Snapshot (beta)](#memory-snapshot-beta)\n [Enabling automatic snapshots](#enabling-automatic-snapshots)\n [Controlling snapshot memory](#controlling-snapshot-memory)\n [Snapshot compatibility](#snapshot-compatibility)\n [Known limitations](#known-limitations)\n [No GPUs available during the snapshotting phase](#no-gpus-available-during-the-snapshotting-phase)\n [Network connectivity is not available during the snapshotting phase](#network-connectivity-is-not-available-during-the-snapshotting-phase)\n [Filesystem writes are not snapshotted](#filesystem-writes-are-not-snapshotted)\n [Cached GPU device queries](#cached-gpu-device-queries)\n [Randomness and uniqueness](#randomness-and-uniqueness)",
    "metadata": {
      "title": "Memory Snapshot (beta) | Modal Docs",
      "description": "You can improve cold-start performance for some functions using the Memory Snapshot feature.",
      "ogTitle": "Memory Snapshot (beta)",
      "ogDescription": "You can improve cold-start performance for some functions using the Memory Snapshot feature.",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/memory-snapshot",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nGeographic Latency\n==================\n\nModal’s worker cluster is multi-cloud and multi-region. The vast majority of workers are located in the continental USA, but we do run workers in Europe and Asia.\n\nModal’s control plane is hosted in Virginia, USA (`us-east-1`).\n\nAny time data needs to travel between the Modal client, our control plane servers, and our workers latency will be incurred. [Cloudping.com](https://www.cloudping.co/grid)\n provides good estimates on the significance of the latency between regions. For example, the roundtrip latency between AWS `us-east-1` (Virginia, USA) and `us-west-1` (California, USA) is around 60ms.\n\nYou can observe the location identifier of a container [via an environment variable](/docs/guide/environment_variables)\n. Logging this environment variable alongside latency information can reveal when geography is impacting your application performance.\n\nRegion selection\n----------------\n\nIn cases where low-latency communication is required between your container and a network dependency (e.g a database), it is useful to ensure that Modal schedules your container in only regions geographically proximate to that dependency. For example, if you have an AWS RDS database in Virginia, USA (`us-east-1`), ensuring your Modal containers are also scheduled in Virginia means that network latency between the container and the database will be less than 5 milliseconds.\n\nFor more information, please see [Region selection](/docs/guide/region-selection)\n.\n\n[Geographic Latency](#geographic-latency)\n [Region selection](#region-selection)",
    "markdown": "* * *\n\nGeographic Latency\n==================\n\nModal’s worker cluster is multi-cloud and multi-region. The vast majority of workers are located in the continental USA, but we do run workers in Europe and Asia.\n\nModal’s control plane is hosted in Virginia, USA (`us-east-1`).\n\nAny time data needs to travel between the Modal client, our control plane servers, and our workers latency will be incurred. [Cloudping.com](https://www.cloudping.co/grid)\n provides good estimates on the significance of the latency between regions. For example, the roundtrip latency between AWS `us-east-1` (Virginia, USA) and `us-west-1` (California, USA) is around 60ms.\n\nYou can observe the location identifier of a container [via an environment variable](/docs/guide/environment_variables)\n. Logging this environment variable alongside latency information can reveal when geography is impacting your application performance.\n\nRegion selection\n----------------\n\nIn cases where low-latency communication is required between your container and a network dependency (e.g a database), it is useful to ensure that Modal schedules your container in only regions geographically proximate to that dependency. For example, if you have an AWS RDS database in Virginia, USA (`us-east-1`), ensuring your Modal containers are also scheduled in Virginia means that network latency between the container and the database will be less than 5 milliseconds.\n\nFor more information, please see [Region selection](/docs/guide/region-selection)\n.\n\n[Geographic Latency](#geographic-latency)\n [Region selection](#region-selection)",
    "metadata": {
      "title": "Geographic Latency | Modal Docs",
      "description": "Modal’s worker cluster is multi-cloud and multi-region. The vast majority of workers are located in the continental USA, but we do run workers in Europe and Asia.",
      "ogTitle": "Geographic Latency",
      "ogDescription": "Modal’s worker cluster is multi-cloud and multi-region. The vast majority of workers are located in the continental USA, but we do run workers in Europe and Asia.",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/geographic-latency",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nFailures and retries\n====================\n\nWhen you call a function over a sequence of inputs with [Function.map()](/docs/guide/scale#parallel-execution-of-inputs)\n, sometimes errors can happen during function execution. Exceptions from within the remote function are propagated to the caller, so you can handle them with a `try-except` statement (refer to [section on custom types](https://modal.com/docs/guide/troubleshooting#custom-types-defined-in-__main__)\n for more on how to catch user-defined exceptions):\n\n    @app.function()\n    def f(i):\n        raise ValueError()\n    \n    @app.local_entrypoint()\n    def main():\n        try:\n            for _ in f.map([1, 2, 3]):\n                pass\n        except ValueError:\n            print(\"Exception handled\")\n\nCopy\n\nFunction retries\n----------------\n\nYou can configure Modal to automatically retry function failures if you set the `retries` option when declaring your function:\n\n    @app.function(retries=3)\n    def my_flaky_function():\n        pass\n\nCopy\n\nWhen used with `Function.map()`, each input is retried up to the max number of retries specified.\n\nThe basic configuration shown provides a fixed 1s delay between retry attempts. For fine-grained control over retry delays, including exponential backoff configuration, use [`modal.Retries`](/docs/reference/modal.Retries)\n.\n\nContainer crashes\n-----------------\n\nIn the case of a container crash on start-up (for example, while handling imports in global scope before the function can be run), the error will be propagated to the caller immediately, since it’s likely a user error.\n\nIf a container crashes after start-up (for example, due to an out of memory error), Modal will reschedule the container and any work it was currently assigned, unless the crash rate of the container exceeds a certain limit.\n\n[Failures and retries](#failures-and-retries)\n [Function retries](#function-retries)\n [Container crashes](#container-crashes)",
    "markdown": "* * *\n\nFailures and retries\n====================\n\nWhen you call a function over a sequence of inputs with [Function.map()](/docs/guide/scale#parallel-execution-of-inputs)\n, sometimes errors can happen during function execution. Exceptions from within the remote function are propagated to the caller, so you can handle them with a `try-except` statement (refer to [section on custom types](https://modal.com/docs/guide/troubleshooting#custom-types-defined-in-__main__)\n for more on how to catch user-defined exceptions):\n\n    @app.function()\n    def f(i):\n        raise ValueError()\n    \n    @app.local_entrypoint()\n    def main():\n        try:\n            for _ in f.map([1, 2, 3]):\n                pass\n        except ValueError:\n            print(\"Exception handled\")\n\nCopy\n\nFunction retries\n----------------\n\nYou can configure Modal to automatically retry function failures if you set the `retries` option when declaring your function:\n\n    @app.function(retries=3)\n    def my_flaky_function():\n        pass\n\nCopy\n\nWhen used with `Function.map()`, each input is retried up to the max number of retries specified.\n\nThe basic configuration shown provides a fixed 1s delay between retry attempts. For fine-grained control over retry delays, including exponential backoff configuration, use [`modal.Retries`](/docs/reference/modal.Retries)\n.\n\nContainer crashes\n-----------------\n\nIn the case of a container crash on start-up (for example, while handling imports in global scope before the function can be run), the error will be propagated to the caller immediately, since it’s likely a user error.\n\nIf a container crashes after start-up (for example, due to an out of memory error), Modal will reschedule the container and any work it was currently assigned, unless the crash rate of the container exceeds a certain limit.\n\n[Failures and retries](#failures-and-retries)\n [Function retries](#function-retries)\n [Container crashes](#container-crashes)",
    "metadata": {
      "title": "Failures and retries | Modal Docs",
      "description": "When you call a function over a sequence of inputs with Function.map(), sometimes errors can happen during function execution. Exceptions from within the remote function are propagated to the caller, so you can handle them with a try-except statement (refer to section on custom types for more on how to catch user-defined exceptions):",
      "ogTitle": "Failures and retries",
      "ogDescription": "When you call a function over a sequence of inputs with Function.map(), sometimes errors can happen during function execution. Exceptions from within the remote function are propagated to the caller, so you can handle them with a try-except statement (refer to section on custom types for more on how to catch user-defined exceptions):",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/retries",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nPreemption\n==========\n\nAll Modal Functions are subject to preemption. If a preemption event interrupts a running Function, Modal will gracefully terminate the Function and restart it on the same input.\n\nPreemptions are rare, but it is always possible that your Function is interrupted. Long-running Functions such as model training Functions should take particular care to tolerate interruptions, as likelihood of interruption increases with Function run duration.\n\nPreparing for interruptions\n---------------------------\n\nDesign your applications to be fault and preemption tolerant. Modal will send an interrupt signal (`SIGINT`) to your application code when preemption occurs. In Python applications, this signal is by default propogated as a `KeyboardInterrupt`, which you can handle in your code to perform cleanup.\n\nOther best practices for handling preemptions include:\n\n*   Divide long-running operations into small tasks or use checkpoints so that you can save your work frequently.\n*   Ensure preemptible operations are safely retryable (ie. idempotent).\n\nRunning uninterruptible Functions\n---------------------------------\n\nWe currently don’t have a way for Functions to avoid the possibility of interruption, but it’s a planned feature. If you require Functions guaranteed to run without interruption, please reach out!\n\n[Preemption](#preemption)\n [Preparing for interruptions](#preparing-for-interruptions)\n [Running uninterruptible Functions](#running-uninterruptible-functions)",
    "markdown": "* * *\n\nPreemption\n==========\n\nAll Modal Functions are subject to preemption. If a preemption event interrupts a running Function, Modal will gracefully terminate the Function and restart it on the same input.\n\nPreemptions are rare, but it is always possible that your Function is interrupted. Long-running Functions such as model training Functions should take particular care to tolerate interruptions, as likelihood of interruption increases with Function run duration.\n\nPreparing for interruptions\n---------------------------\n\nDesign your applications to be fault and preemption tolerant. Modal will send an interrupt signal (`SIGINT`) to your application code when preemption occurs. In Python applications, this signal is by default propogated as a `KeyboardInterrupt`, which you can handle in your code to perform cleanup.\n\nOther best practices for handling preemptions include:\n\n*   Divide long-running operations into small tasks or use checkpoints so that you can save your work frequently.\n*   Ensure preemptible operations are safely retryable (ie. idempotent).\n\nRunning uninterruptible Functions\n---------------------------------\n\nWe currently don’t have a way for Functions to avoid the possibility of interruption, but it’s a planned feature. If you require Functions guaranteed to run without interruption, please reach out!\n\n[Preemption](#preemption)\n [Preparing for interruptions](#preparing-for-interruptions)\n [Running uninterruptible Functions](#running-uninterruptible-functions)",
    "metadata": {
      "title": "Preemption | Modal Docs",
      "description": "All Modal Functions are subject to preemption. If a preemption event interrupts a running Function, Modal will gracefully terminate the Function and restart it on the same input.",
      "ogTitle": "Preemption",
      "ogDescription": "All Modal Functions are subject to preemption. If a preemption event interrupts a running Function, Modal will gracefully terminate the Function and restart it on the same input.",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/preemption",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nTimeouts\n========\n\nAll Modal [Function](/docs/reference/modal.Function)\n executions have a default execution timeout of 300 seconds (5 minutes), but users may specify timeout durations between 10 seconds and 24 hours.\n\n    import time\n    \n    \n    @app.function()\n    def f():\n        time.sleep(599)  # Timeout!\n    \n    \n    @app.function(timeout=600)\n    def g():\n        time.sleep(599)\n        print(\"*Just* made it!\")\n\nCopy\n\nThe timeout duration is a measure of a Function’s _execution_ time. It does not include scheduling time or any other period besides the time your code is executing in Modal. This duration is also per execution attempt, meaning Functions configured with [`modal.Retries`](/docs/reference/modal.Retries)\n will start new execution timeouts on each retry. For example, an infinite-looping Function with a 100 second timeout and 3 allowed retries will run for least 400 seconds within Modal.\n\nHandling timeouts\n-----------------\n\nAfter exhausting any specified retries, a timeout in a Function will produce a `modal.exception.FunctionTimeoutError` which you may catch in your code.\n\n    import modal.exception\n    \n    \n    @app.function(timeout=100)\n    def f():\n        time.sleep(200)  # Timeout!\n    \n    \n    @app.local_entrypoint()\n    def main():\n        try:\n            f.remote()\n        except modal.exception.FunctionTimeoutError:\n            ... # Handle the timeout.\n\nCopy\n\nTimeout accuracy\n----------------\n\nFunctions will run for _at least_ as long as their timeout allows, but they may run a handful of seconds longer. If you require accurate and precise timeout durations on your Function executions, it is recommended that you implement timeout logic in your user code.\n\n[Timeouts](#timeouts)\n [Handling timeouts](#handling-timeouts)\n [Timeout accuracy](#timeout-accuracy)",
    "markdown": "* * *\n\nTimeouts\n========\n\nAll Modal [Function](/docs/reference/modal.Function)\n executions have a default execution timeout of 300 seconds (5 minutes), but users may specify timeout durations between 10 seconds and 24 hours.\n\n    import time\n    \n    \n    @app.function()\n    def f():\n        time.sleep(599)  # Timeout!\n    \n    \n    @app.function(timeout=600)\n    def g():\n        time.sleep(599)\n        print(\"*Just* made it!\")\n\nCopy\n\nThe timeout duration is a measure of a Function’s _execution_ time. It does not include scheduling time or any other period besides the time your code is executing in Modal. This duration is also per execution attempt, meaning Functions configured with [`modal.Retries`](/docs/reference/modal.Retries)\n will start new execution timeouts on each retry. For example, an infinite-looping Function with a 100 second timeout and 3 allowed retries will run for least 400 seconds within Modal.\n\nHandling timeouts\n-----------------\n\nAfter exhausting any specified retries, a timeout in a Function will produce a `modal.exception.FunctionTimeoutError` which you may catch in your code.\n\n    import modal.exception\n    \n    \n    @app.function(timeout=100)\n    def f():\n        time.sleep(200)  # Timeout!\n    \n    \n    @app.local_entrypoint()\n    def main():\n        try:\n            f.remote()\n        except modal.exception.FunctionTimeoutError:\n            ... # Handle the timeout.\n\nCopy\n\nTimeout accuracy\n----------------\n\nFunctions will run for _at least_ as long as their timeout allows, but they may run a handful of seconds longer. If you require accurate and precise timeout durations on your Function executions, it is recommended that you implement timeout logic in your user code.\n\n[Timeouts](#timeouts)\n [Handling timeouts](#handling-timeouts)\n [Timeout accuracy](#timeout-accuracy)",
    "metadata": {
      "title": "Timeouts | Modal Docs",
      "description": "All Modal Function executions have a default execution timeout of 300 seconds (5 minutes), but users may specify timeout durations between 10 seconds and 24 hours.",
      "ogTitle": "Timeouts",
      "ogDescription": "All Modal Function executions have a default execution timeout of 300 seconds (5 minutes), but users may specify timeout durations between 10 seconds and 24 hours.",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/timeouts",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nTroubleshooting\n===============\n\n“Command not found” errors\n--------------------------\n\nIf you installed Modal but you’re seeing an error like `modal: command not found` when trying to run the CLI, this means that the installation location of Python package executables (“binaries”) are not present on your system path. This is a common problem; you need to reconfigure your system’s environment variables to fix it.\n\nOne workaround is to use `python -m modal.cli` instead of `modal`. However, this is just a patch. There’s no single solution for the problem because Python installs dependencies on different locations depending on your environment. See this [popular StackOverflow question](https://stackoverflow.com/q/35898734)\n for pointers on how to resolve your system path issue.\n\nCustom types defined in `__main__`\n----------------------------------\n\nModal currently uses [cloudpickle](https://github.com/cloudpipe/cloudpickle)\n to transfer objects returned or exceptions raised by functions that are executed in Modal. This gives a lot of flexibility and support for custom data types.\n\nHowever, any types that are declared in your Python entrypoint file (The one you call on the command line) will currently be _redeclared_ if they are returned from Modal functions, and will therefore have the same structure and type name but not maintain class object identity with your local types. This means that you _can’t_ catch specific custom exception classes:\n\n    import modal\n    app = modal.App()\n    \n    class MyException(Exception):\n        pass\n    \n    @app.function()\n    def raise_custom():\n        raise MyException()\n    \n    @app.local_entrypoint()\n    def main():\n        try:\n            raise_custom.remote()\n        except MyException:  # this will not catch the remote exception\n            pass\n        except Exception:  # this will catch it instead, as it's still a subclass of Exception\n            pass\n\nCopy\n\nNor can you do object equality checks on `dataclasses`, or `isinstance` checks:\n\n    import modal\n    import dataclasses\n    \n    @dataclasses.dataclass\n    class MyType:\n        foo: int\n    \n    app = modal.App()\n    \n    @app.function()\n    def return_custom():\n        return MyType(foo=10)\n    \n    \n    @app.local_entrypoint()\n    def main():\n        data = return_custom.remote()\n        assert data == MyType(foo=10)  # false!\n        assert data.foo == 10  # true!, the type still has the same fields etc.\n        assert isinstance(data, MyType)  # false!\n\nCopy\n\nIf this is a problem for you, you can easily solve it by moving your custom type definitions to a separate Python file from the one you trigger to run your Modal code, and import that file instead.\n\n    # File: my_types.py\n    import dataclasses\n    \n    @dataclasses.dataclass\n    class MyType:\n        foo: int\n\nCopy\n\n    # File: modal_script.py\n    import modal\n    from my_types import MyType\n    \n    app = modal.App()\n    \n    @app.function()\n    def return_custom():\n        return MyType(foo=10)\n    \n    @app.local_entrypoint()\n    def main():\n        data = return_custom.remote()\n        assert data == MyType(foo=10)  # true!\n        assert isinstance(data, MyType)  # true!\n\nCopy\n\nFunction side effects\n---------------------\n\nThe same container _can_ be reused for multiple invocations of the same function within an app. This means that if your function has side effects like modifying files on disk, they may or may not be present for subsequent calls to that function. You should not rely on the side effects to be present, but you might have to be careful so they don’t cause problems.\n\nFor example, if you create a disk-backed database using sqlite3:\n\n    import modal\n    import sqlite3\n    \n    app = modal.App()\n    \n    @app.function()\n    def db_op():\n        db = sqlite3(\"db_file.sqlite3\")\n        db.execute(\"CREATE TABLE example (col_1 TEXT)\")\n        ...\n\nCopy\n\nThis function _can_ (but will not necessarily) fail on the second invocation with an\n\n`OperationalError: table foo already exists`\n\nTo get around this, take care to either clean up your side effects (e.g. deleting the db file at the end your function call above) or make your functions take them into consideration (e.g. adding an `if os.path.exists(\"db_file.sqlite\")` condition or randomize the filename above).\n\n`413 Content Too Large` errors\n------------------------------\n\nIf you receive a `413 Content Too Large` error, this might be because you are hitting our gRPC payload size limits.\n\nThe size limit is currently 100MB.\n\n`403` errors when connecting to GCP services.\n---------------------------------------------\n\nGCP will sometimes return 403 errors to Modal when connecting directly to GCP cloud services like Google Cloud Storage. This is a known issue.\n\nThe workaround is to pin the `cloud` parameter in the [`@app.function`](https://modal.com/docs/reference/modal.App#function)\n or [`@app.cls`](https://modal.com/docs/reference/modal.App#cls)\n.\n\nFor example:\n\n    @app.function(cloud=\"gcp\")\n\nCopy\n\n    @app.cls(cloud=\"gcp\")\n\nCopy\n\n[Troubleshooting](#troubleshooting)\n [“Command not found” errors](#command-not-found-errors)\n [Custom types defined in \\_\\_main\\_\\_](#custom-types-defined-in-__main__)\n [Function side effects](#function-side-effects)\n [413 Content Too Large errors](#413-content-too-large-errors)\n [403 errors when connecting to GCP services.](#403-errors-when-connecting-to-gcp-services)",
    "markdown": "* * *\n\nTroubleshooting\n===============\n\n“Command not found” errors\n--------------------------\n\nIf you installed Modal but you’re seeing an error like `modal: command not found` when trying to run the CLI, this means that the installation location of Python package executables (“binaries”) are not present on your system path. This is a common problem; you need to reconfigure your system’s environment variables to fix it.\n\nOne workaround is to use `python -m modal.cli` instead of `modal`. However, this is just a patch. There’s no single solution for the problem because Python installs dependencies on different locations depending on your environment. See this [popular StackOverflow question](https://stackoverflow.com/q/35898734)\n for pointers on how to resolve your system path issue.\n\nCustom types defined in `__main__`\n----------------------------------\n\nModal currently uses [cloudpickle](https://github.com/cloudpipe/cloudpickle)\n to transfer objects returned or exceptions raised by functions that are executed in Modal. This gives a lot of flexibility and support for custom data types.\n\nHowever, any types that are declared in your Python entrypoint file (The one you call on the command line) will currently be _redeclared_ if they are returned from Modal functions, and will therefore have the same structure and type name but not maintain class object identity with your local types. This means that you _can’t_ catch specific custom exception classes:\n\n    import modal\n    app = modal.App()\n    \n    class MyException(Exception):\n        pass\n    \n    @app.function()\n    def raise_custom():\n        raise MyException()\n    \n    @app.local_entrypoint()\n    def main():\n        try:\n            raise_custom.remote()\n        except MyException:  # this will not catch the remote exception\n            pass\n        except Exception:  # this will catch it instead, as it's still a subclass of Exception\n            pass\n\nCopy\n\nNor can you do object equality checks on `dataclasses`, or `isinstance` checks:\n\n    import modal\n    import dataclasses\n    \n    @dataclasses.dataclass\n    class MyType:\n        foo: int\n    \n    app = modal.App()\n    \n    @app.function()\n    def return_custom():\n        return MyType(foo=10)\n    \n    \n    @app.local_entrypoint()\n    def main():\n        data = return_custom.remote()\n        assert data == MyType(foo=10)  # false!\n        assert data.foo == 10  # true!, the type still has the same fields etc.\n        assert isinstance(data, MyType)  # false!\n\nCopy\n\nIf this is a problem for you, you can easily solve it by moving your custom type definitions to a separate Python file from the one you trigger to run your Modal code, and import that file instead.\n\n    # File: my_types.py\n    import dataclasses\n    \n    @dataclasses.dataclass\n    class MyType:\n        foo: int\n\nCopy\n\n    # File: modal_script.py\n    import modal\n    from my_types import MyType\n    \n    app = modal.App()\n    \n    @app.function()\n    def return_custom():\n        return MyType(foo=10)\n    \n    @app.local_entrypoint()\n    def main():\n        data = return_custom.remote()\n        assert data == MyType(foo=10)  # true!\n        assert isinstance(data, MyType)  # true!\n\nCopy\n\nFunction side effects\n---------------------\n\nThe same container _can_ be reused for multiple invocations of the same function within an app. This means that if your function has side effects like modifying files on disk, they may or may not be present for subsequent calls to that function. You should not rely on the side effects to be present, but you might have to be careful so they don’t cause problems.\n\nFor example, if you create a disk-backed database using sqlite3:\n\n    import modal\n    import sqlite3\n    \n    app = modal.App()\n    \n    @app.function()\n    def db_op():\n        db = sqlite3(\"db_file.sqlite3\")\n        db.execute(\"CREATE TABLE example (col_1 TEXT)\")\n        ...\n\nCopy\n\nThis function _can_ (but will not necessarily) fail on the second invocation with an\n\n`OperationalError: table foo already exists`\n\nTo get around this, take care to either clean up your side effects (e.g. deleting the db file at the end your function call above) or make your functions take them into consideration (e.g. adding an `if os.path.exists(\"db_file.sqlite\")` condition or randomize the filename above).\n\n`413 Content Too Large` errors\n------------------------------\n\nIf you receive a `413 Content Too Large` error, this might be because you are hitting our gRPC payload size limits.\n\nThe size limit is currently 100MB.\n\n`403` errors when connecting to GCP services.\n---------------------------------------------\n\nGCP will sometimes return 403 errors to Modal when connecting directly to GCP cloud services like Google Cloud Storage. This is a known issue.\n\nThe workaround is to pin the `cloud` parameter in the [`@app.function`](https://modal.com/docs/reference/modal.App#function)\n or [`@app.cls`](https://modal.com/docs/reference/modal.App#cls)\n.\n\nFor example:\n\n    @app.function(cloud=\"gcp\")\n\nCopy\n\n    @app.cls(cloud=\"gcp\")\n\nCopy\n\n[Troubleshooting](#troubleshooting)\n [“Command not found” errors](#command-not-found-errors)\n [Custom types defined in \\_\\_main\\_\\_](#custom-types-defined-in-__main__)\n [Function side effects](#function-side-effects)\n [413 Content Too Large errors](#413-content-too-large-errors)\n [403 errors when connecting to GCP services.](#403-errors-when-connecting-to-gcp-services)",
    "metadata": {
      "title": "Troubleshooting | Modal Docs",
      "description": "If you installed Modal but you’re seeing an error like modal: command not found when trying to run the CLI, this means that the installation location of Python package executables (“binaries”) are not present on your system path. This is a common problem; you need to reconfigure your system’s environment variables to fix it.",
      "ogTitle": "Troubleshooting",
      "ogDescription": "If you installed Modal but you’re seeing an error like modal: command not found when trying to run the CLI, this means that the installation location of Python package executables (“binaries”) are not present on your system path. This is a common problem; you need to reconfigure your system’s environment variables to fix it.",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/troubleshooting",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nSecurity and privacy at Modal\n=============================\n\nThe document outlines Modal’s security and privacy commitments.\n\nApplication security (AppSec)\n-----------------------------\n\nAppSec is the practice of building software that is secure by design, secured during development, secured with testing and review, and deployed securely.\n\n*   We build our software using memory-safe programming languages, including Rust (for our worker runtime and storage infrastructure) and Python (for our API servers and Modal client).\n*   Software dependencies are audited by Github’s Dependabot.\n*   We make decisions that minimize our attack surface. Most interactions with Modal are well-described in a gRPC API, and occur through [`modal`](https://pypi.org/project/modal)\n    , our open-source command-line tool and Python client library.\n*   We have automated synthetic monitoring test applications that continously check for network and application isolation within our runtime.\n*   We use HTTPS for secure connections. Modal forces HTTPS for all services using TLS (SSL), including our public website and the Dashboard to ensure secure connections. Modal’s [client library](https://pypi.org/project/modal)\n     connects to Modal’s servers over TLS and verify TLS certificates on each connection.\n*   All user data is encrypted in transit and at rest.\n*   All public Modal APIs use [TLS 1.3](https://datatracker.ietf.org/doc/html/rfc8446)\n    , the latest and safest version of the TLS protocol.\n*   Internal code reviews are performed using a modern, PR-based development workflow (Github), and engage external penetration testing firms to assess our software security.\n\nCorporate security (CorpSec)\n----------------------------\n\nCorpSec is the practice of making sure Modal employees have secure access to Modal company infrastructure, and also that exposed channels to Modal are secured. CorpSec controls are the primary concern of standards such as SOC2.\n\n*   Access to our services and applications is gated on a SSO Identity Provider (IdP).\n*   We mandata phishing-resistant multi-factor authentication (MFA) in all enrolled IdP accounts.\n*   We regularly audit access to internal systems.\n*   Employee laptops are protected by full disk encryption using FileVault2, and managed by Secureframe MDM.\n\nNetwork and infrastructure security (InfraSec)\n----------------------------------------------\n\nInfraSec is the practice of ensuring a hardened, minimal attack surface for components we deploy on our network.\n\n*   Modal uses logging and metrics observability providers, including Datadog and Sentry.io.\n*   Compute jobs at Modal are containerized and virtualized using [gVisor](https://github.com/google/gvisor)\n    , the sandboxing technology developed at Google and used in their _Google Cloud Run_ and _Google Kubernetes Engine_ cloud services.\n*   We conduct annual business continuity and security incident exercises.\n\nVulnerability remediation\n-------------------------\n\nSecurity vulnerabilities directly affecting Modal’s systems and services will be patched or otherwise remediated within a timeframe appropriate for the severity of the vulnerability, subject to the public availability of a patch or other remediation mechanisms.\n\nIf there is a CVSS severity rating accompanying a vulnerability disclosure, we rely on that as a starting point, but may upgrade or downgrade the severity using our best judgement.\n\n### Severity timeframes\n\n*   **Critical:** 24 hours\n*   **High:** 1 week\n*   **Medium:** 1 month\n*   **Low:** 3 months\n*   **Informational:** 3 months or longer\n\nSOC 2\n-----\n\nWe have successfully completed a System and Organization Controls (SOC) 2 Type 1 audit. Contact us at [security@modal.com](mailto:security@modal.com)\n for more details or access to the report.\n\nPCI\n---\n\n_Payment Card Industry Data Security Standard_ (PCI) is a standard that defines the security and privacy requirements for payment card processing.\n\nModal uses [Stripe](https://stripe.com)\n to securely process transactions and trusts their commitment to best-in-class security. We do not store personal credit card information for any of our customers. Stripe is certified as “PCI Service Provider Level 1”, which is the highest level of certification in the payments industry.\n\nBug bounty program\n------------------\n\nKeeping user data secure is a top priority at Modal. We welcome contributions from the security community to identify vulnerabilities in our product and disclose them to us in a responsible manner. We offer rewards ranging from $100 to $1000+ depending on the severity of the issue discovered. To participate, please send a report of the vulnerability to [security@modal.com](mailto:security@modal.com)\n.\n\nData privacy\n------------\n\nModal will never access or use:\n\n*   your source code.\n*   the inputs or outputs to your Modal Functions.\n*   any data you store in Modal, such as in Images or Volumes.\n\nInput and output data for a function are deleted from our system once the output has been retrieved. If the output has not been retrieved after a max TTL of 24 hours the input and output are deleted automatically.\n\nApp logs and metadata are stored on Modal. Modal will not access this data unless permission is granted by the user to help with troubleshooting.\n\nQuestions?\n----------\n\n[Email us!](mailto:security@modal.com)\n\n[Security and privacy at Modal](#security-and-privacy-at-modal)\n [Application security (AppSec)](#application-security-appsec)\n [Corporate security (CorpSec)](#corporate-security-corpsec)\n [Network and infrastructure security (InfraSec)](#network-and-infrastructure-security-infrasec)\n [Vulnerability remediation](#vulnerability-remediation)\n [Severity timeframes](#severity-timeframes)\n [SOC 2](#soc-2)\n [PCI](#pci)\n [Bug bounty program](#bug-bounty-program)\n [Data privacy](#data-privacy)\n [Questions?](#questions)",
    "markdown": "* * *\n\nSecurity and privacy at Modal\n=============================\n\nThe document outlines Modal’s security and privacy commitments.\n\nApplication security (AppSec)\n-----------------------------\n\nAppSec is the practice of building software that is secure by design, secured during development, secured with testing and review, and deployed securely.\n\n*   We build our software using memory-safe programming languages, including Rust (for our worker runtime and storage infrastructure) and Python (for our API servers and Modal client).\n*   Software dependencies are audited by Github’s Dependabot.\n*   We make decisions that minimize our attack surface. Most interactions with Modal are well-described in a gRPC API, and occur through [`modal`](https://pypi.org/project/modal)\n    , our open-source command-line tool and Python client library.\n*   We have automated synthetic monitoring test applications that continously check for network and application isolation within our runtime.\n*   We use HTTPS for secure connections. Modal forces HTTPS for all services using TLS (SSL), including our public website and the Dashboard to ensure secure connections. Modal’s [client library](https://pypi.org/project/modal)\n     connects to Modal’s servers over TLS and verify TLS certificates on each connection.\n*   All user data is encrypted in transit and at rest.\n*   All public Modal APIs use [TLS 1.3](https://datatracker.ietf.org/doc/html/rfc8446)\n    , the latest and safest version of the TLS protocol.\n*   Internal code reviews are performed using a modern, PR-based development workflow (Github), and engage external penetration testing firms to assess our software security.\n\nCorporate security (CorpSec)\n----------------------------\n\nCorpSec is the practice of making sure Modal employees have secure access to Modal company infrastructure, and also that exposed channels to Modal are secured. CorpSec controls are the primary concern of standards such as SOC2.\n\n*   Access to our services and applications is gated on a SSO Identity Provider (IdP).\n*   We mandata phishing-resistant multi-factor authentication (MFA) in all enrolled IdP accounts.\n*   We regularly audit access to internal systems.\n*   Employee laptops are protected by full disk encryption using FileVault2, and managed by Secureframe MDM.\n\nNetwork and infrastructure security (InfraSec)\n----------------------------------------------\n\nInfraSec is the practice of ensuring a hardened, minimal attack surface for components we deploy on our network.\n\n*   Modal uses logging and metrics observability providers, including Datadog and Sentry.io.\n*   Compute jobs at Modal are containerized and virtualized using [gVisor](https://github.com/google/gvisor)\n    , the sandboxing technology developed at Google and used in their _Google Cloud Run_ and _Google Kubernetes Engine_ cloud services.\n*   We conduct annual business continuity and security incident exercises.\n\nVulnerability remediation\n-------------------------\n\nSecurity vulnerabilities directly affecting Modal’s systems and services will be patched or otherwise remediated within a timeframe appropriate for the severity of the vulnerability, subject to the public availability of a patch or other remediation mechanisms.\n\nIf there is a CVSS severity rating accompanying a vulnerability disclosure, we rely on that as a starting point, but may upgrade or downgrade the severity using our best judgement.\n\n### Severity timeframes\n\n*   **Critical:** 24 hours\n*   **High:** 1 week\n*   **Medium:** 1 month\n*   **Low:** 3 months\n*   **Informational:** 3 months or longer\n\nSOC 2\n-----\n\nWe have successfully completed a System and Organization Controls (SOC) 2 Type 1 audit. Contact us at [security@modal.com](mailto:security@modal.com)\n for more details or access to the report.\n\nPCI\n---\n\n_Payment Card Industry Data Security Standard_ (PCI) is a standard that defines the security and privacy requirements for payment card processing.\n\nModal uses [Stripe](https://stripe.com)\n to securely process transactions and trusts their commitment to best-in-class security. We do not store personal credit card information for any of our customers. Stripe is certified as “PCI Service Provider Level 1”, which is the highest level of certification in the payments industry.\n\nBug bounty program\n------------------\n\nKeeping user data secure is a top priority at Modal. We welcome contributions from the security community to identify vulnerabilities in our product and disclose them to us in a responsible manner. We offer rewards ranging from $100 to $1000+ depending on the severity of the issue discovered. To participate, please send a report of the vulnerability to [security@modal.com](mailto:security@modal.com)\n.\n\nData privacy\n------------\n\nModal will never access or use:\n\n*   your source code.\n*   the inputs or outputs to your Modal Functions.\n*   any data you store in Modal, such as in Images or Volumes.\n\nInput and output data for a function are deleted from our system once the output has been retrieved. If the output has not been retrieved after a max TTL of 24 hours the input and output are deleted automatically.\n\nApp logs and metadata are stored on Modal. Modal will not access this data unless permission is granted by the user to help with troubleshooting.\n\nQuestions?\n----------\n\n[Email us!](mailto:security@modal.com)\n\n[Security and privacy at Modal](#security-and-privacy-at-modal)\n [Application security (AppSec)](#application-security-appsec)\n [Corporate security (CorpSec)](#corporate-security-corpsec)\n [Network and infrastructure security (InfraSec)](#network-and-infrastructure-security-infrasec)\n [Vulnerability remediation](#vulnerability-remediation)\n [Severity timeframes](#severity-timeframes)\n [SOC 2](#soc-2)\n [PCI](#pci)\n [Bug bounty program](#bug-bounty-program)\n [Data privacy](#data-privacy)\n [Questions?](#questions)",
    "metadata": {
      "title": "Security and privacy at Modal | Modal Docs",
      "description": "The document outlines Modal’s security and privacy commitments.",
      "ogTitle": "Security and privacy at Modal",
      "ogDescription": "The document outlines Modal’s security and privacy commitments.",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/security",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nConnecting Modal to your Vercel account\n=======================================\n\nYou can use the Modal + Vercel integration to access Modal’s [Instant Endpoints](#modal-instant-endpoints)\n from Vercel projects. You’ll find the Modal Vercel integration available for install in the Vercel AI marketplace.\n\nWhat this integration does\n--------------------------\n\nThis integration allows you to:\n\n1.  Easily synchronize your Modal API keys to one or more Vercel projects\n2.  Call Modal’s [Instant Endpoints](#modal-instant-endpoints)\n     over HTTP in connected Vercel projects\n\n### Authentication\n\nThe integration will set the following environment variables against the user’s selected Vercel projects:\n\n*   `MODAL_TOKEN_ID` (starts with `ak-*`)\n*   `MODAL_TOKEN_SECRET` (starts with `as-*`)\n\nThe environment variables will be set in the “preview” and “production” project targets. You can read more about environment variables within Vercel [in the documentation](https://vercel.com/docs/concepts/projects/environment-variables#environments)\n.\n\nInstalling the integration\n--------------------------\n\n1.  Click “Add integration” on the [Vercel integrations page](https://vercel.com/integrations/modal)\n    \n2.  Select the Vercel account you want to connect with\n3.  (If logged out) Sign into an existing Modal workspace, or create a new Modal workspace\n4.  Select the Vercel projects that you wish to connect to your Modal workspace\n5.  Click “Continue”\n6.  Back in your Vercel dashboard, confirm the environment variables were added by going to your Vercel `project > \"Settings\" > \"Environment variables\"`\n\nUninstalling the integration\n----------------------------\n\nThe Modal Vercel integration is managed under the user’s Vercel dashboard under the “Integrations” tab. From there they can remove the specific integration installation from their Vercel account.\n\n**Important:** removing an integration will delete the corresponding API token set by Modal in your Vercel project(s).\n\n* * *\n\nModal Instant Endpoints\n-----------------------\n\n_Instant Endpoints_ are a fast and scalable API for integrating open-source AI models into your Vercel app.\n\nAll available endpoints are listed below, along with example code suitable for use with the Javascript `fetch` API.\n\n### Stable Diffusion XL\n\n> [https://modal-labs—instant-stable-diffusion-xl.modal.run](https://modal-labs--instant-stable-diffusion-xl.modal.run/)\n\nStable Diffusion is a latent text-to-image diffusion model able to generate photo-realistic images given any text prompt.\n\nThis endpoint uses a fast version of [Stable Diffusion XL](https://huggingface.co/papers/2307.01952)\n to create variably sized images up to 1024h x 1024w.\n\n#### Example code\n\n    // pages/api/modal.ts\n    const requestData = {\n      prompt: \"need for speed supercar. unreal engine\",\n      width: 768,\n      height: 768,\n      num_outputs: 1,\n    };\n    const result = await fetch(\n      \"https://modal-labs--instant-stable-diffusion-xl.modal.run/v1/inference\",\n      {\n        headers: {\n          Authorization: `Token ${process.env.MODAL_TOKEN_ID}:${process.env.MODAL_TOKEN_SECRET}`,\n          \"Content-Type\": \"application/json\",\n        },\n        method: \"POST\",\n        body: JSON.stringify(requestData),\n      },\n    );\n    const imageData = await result.blob();\n\nCopy\n\n#### Input schema\n\n*   **prompt `string`**\n    *   Input prompt\n*   **height `integer`**\n    *   Height of generated image in pixels. Needs to be a multiple of 64\n    *   One of: `64, 128, 192, 256, 320, 384, 448, 512, 576, 640, 704, 768, 832, 896, 960, 1024`\n    *   Default: `768`\n*   **width `integer`**\n    *   Width of generated image in pixels. Needs to be a multiple of 64\n    *   One of: `64, 128, 192, 256, 320, 384, 448, 512, 576, 640, 704, 768, 832, 896, 960, 1024`\n    *   Default: `768`\n\n#### Output schema\n\nThis endpoint outputs `bytes` for a single image with [media type](https://developer.mozilla.org/en-US/docs/Web/HTTP/Basics_of_HTTP/MIME_types#image)\n `\"image/png\"`.\n\n#### Pricing\n\nRequests to this endpoint use duration based pricing, billed at 1ms granularity. The exact cost per millisecond is based on the underlying GPU hardware. This endpoint use a single NVIDIA A10G device to serve each request.\n\nSee our [pricing page](/pricing)\n for current GPU prices.\n\nInferences usually complete within 15-30 seconds.\n\n### Transcribe speech with vaibhavs10/insanely-fast-whisper\n\nThis endpoint hosts [vaibhavs10/insanely-fast-whisper](https://github.com/chenxwh/insanely-fast-whisper)\n to transcribe and diarize audio.\n\n#### Example code\n\n    // pages/api/modal.ts\n    const data = {\n      audio: dataUrl,\n      diarize_audio: false,\n    };\n    \n    const response = await fetch(\"https://modal-labs--instant-whisper.modal.run\", {\n      headers: {\n        Authorization: `Token ${process.env.MODAL_TOKEN_ID}:${process.env.MODAL_TOKEN_SECRET}`,\n      },\n      method: \"POST\",\n      body: JSON.stringify(requestData),\n    });\n    \n    const output = await response.json();\n\nCopy\n\n#### Input schema\n\n*   **audio `string`**\n    *   Input audio file as a [Data URL](https://developer.mozilla.org/en-US/docs/Web/HTTP/Basics_of_HTTP/Data_URLs)\n        .\n*   **language `string`**\n    *   Language of the input text. Whisper auto-detects the language if not provided. See the full list of options [here](https://github.com/huggingface/transformers/blob/3d2900e829ab16757632f9dde891f1947cfc4be0/src/transformers/models/whisper/tokenization_whisper.py#L95)\n        \n    *   Default: “\n*   **diarize\\_audio `Boolean`**\n    *   Whether to diarize the audio.\n    *   Default: `false`\n*   **batch\\_size `integer`**\n    *   Number of parallel batches.\n    *   Default: `24`\n\n#### Output schema\n\nThis endpoint outputs a JSON with two fields:\n\n*   **text `string`**\n*   **chunks `Chunk[]`**\n\nHere, `Chunk` is a JSON object with the following fields:\n\n*   **speaker `string`**\n    *   \\[Optional\\] only present if `diarize_audio` is `true`\n*   **text `string`**\n*   **timestamp `[float, float]`**\n\n### Stream text-to-speech with coqui-ai/TTS\n\nXTTS v2 is a fast and high-quality text-to-speech model.\n\nThis endpoint uses a streaming version of [coqui-ai/TTS](https://github.com/coqui-ai/TTS)\n that streams wav audio back as it’s generated in real-time.\n\n#### Example code\n\n    // pages/api/modal.ts\n    const requestData = {\n      text: \"It is a mistake to think you can solve any major problems just with potatoes.\",\n      language: \"en\",\n    };\n    const result = await fetch(\"https://modal-labs--instant-xtts-v2.modal.run\", {\n      headers: {\n        Authorization: `Token ${process.env.MODAL_TOKEN_ID}:${process.env.MODAL_TOKEN_SECRET}`,\n        \"Content-Type\": \"application/json\",\n      },\n      method: \"POST\",\n      body: JSON.stringify(requestData),\n    });\n    const audioBuffer = await response.buffer();\n\nCopy\n\n#### Input schema\n\n*   **text `string`**\n    *   Input text\n*   **language `string`**\n    *   Language of the input text\n    *   One of: `en, es, fr, de, it, pt, pl, tr, ru, nl, cs, ar, zh, hu, ko, hi`\n    *   Default: `en`\n\n#### Output schema\n\nThis endpoint streams `bytes` for a single audio file with [media type](https://developer.mozilla.org/en-US/docs/Web/HTTP/Basics_of_HTTP/MIME_types#image)\n `\"audio/wav\"`.\n\n### Want more?\n\nIf a popular open-source AI model API is not listed here, you can [either implement it in Python](/docs/guide/webhooks)\n and host it on Modal or [ask us in Slack](/slack)\n to add it as an Instant Endpoint!\n\n[Connecting Modal to your Vercel account](#connecting-modal-to-your-vercel-account)\n [What this integration does](#what-this-integration-does)\n [Authentication](#authentication)\n [Installing the integration](#installing-the-integration)\n [Uninstalling the integration](#uninstalling-the-integration)\n [Modal Instant Endpoints](#modal-instant-endpoints)\n [Stable Diffusion XL](#stable-diffusion-xl)\n [Example code](#example-code)\n [Input schema](#input-schema)\n [Output schema](#output-schema)\n [Pricing](#pricing)\n [Transcribe speech with vaibhavs10/insanely-fast-whisper](#transcribe-speech-with-vaibhavs10insanely-fast-whisper)\n [Example code](#example-code-1)\n [Input schema](#input-schema-1)\n [Output schema](#output-schema-1)\n [Stream text-to-speech with coqui-ai/TTS](#stream-text-to-speech-withcoqui-aitts)\n [Example code](#example-code-2)\n [Input schema](#input-schema-2)\n [Output schema](#output-schema-2)\n [Want more?](#want-more)",
    "markdown": "* * *\n\nConnecting Modal to your Vercel account\n=======================================\n\nYou can use the Modal + Vercel integration to access Modal’s [Instant Endpoints](#modal-instant-endpoints)\n from Vercel projects. You’ll find the Modal Vercel integration available for install in the Vercel AI marketplace.\n\nWhat this integration does\n--------------------------\n\nThis integration allows you to:\n\n1.  Easily synchronize your Modal API keys to one or more Vercel projects\n2.  Call Modal’s [Instant Endpoints](#modal-instant-endpoints)\n     over HTTP in connected Vercel projects\n\n### Authentication\n\nThe integration will set the following environment variables against the user’s selected Vercel projects:\n\n*   `MODAL_TOKEN_ID` (starts with `ak-*`)\n*   `MODAL_TOKEN_SECRET` (starts with `as-*`)\n\nThe environment variables will be set in the “preview” and “production” project targets. You can read more about environment variables within Vercel [in the documentation](https://vercel.com/docs/concepts/projects/environment-variables#environments)\n.\n\nInstalling the integration\n--------------------------\n\n1.  Click “Add integration” on the [Vercel integrations page](https://vercel.com/integrations/modal)\n    \n2.  Select the Vercel account you want to connect with\n3.  (If logged out) Sign into an existing Modal workspace, or create a new Modal workspace\n4.  Select the Vercel projects that you wish to connect to your Modal workspace\n5.  Click “Continue”\n6.  Back in your Vercel dashboard, confirm the environment variables were added by going to your Vercel `project > \"Settings\" > \"Environment variables\"`\n\nUninstalling the integration\n----------------------------\n\nThe Modal Vercel integration is managed under the user’s Vercel dashboard under the “Integrations” tab. From there they can remove the specific integration installation from their Vercel account.\n\n**Important:** removing an integration will delete the corresponding API token set by Modal in your Vercel project(s).\n\n* * *\n\nModal Instant Endpoints\n-----------------------\n\n_Instant Endpoints_ are a fast and scalable API for integrating open-source AI models into your Vercel app.\n\nAll available endpoints are listed below, along with example code suitable for use with the Javascript `fetch` API.\n\n### Stable Diffusion XL\n\n> [https://modal-labs—instant-stable-diffusion-xl.modal.run](https://modal-labs--instant-stable-diffusion-xl.modal.run/)\n\nStable Diffusion is a latent text-to-image diffusion model able to generate photo-realistic images given any text prompt.\n\nThis endpoint uses a fast version of [Stable Diffusion XL](https://huggingface.co/papers/2307.01952)\n to create variably sized images up to 1024h x 1024w.\n\n#### Example code\n\n    // pages/api/modal.ts\n    const requestData = {\n      prompt: \"need for speed supercar. unreal engine\",\n      width: 768,\n      height: 768,\n      num_outputs: 1,\n    };\n    const result = await fetch(\n      \"https://modal-labs--instant-stable-diffusion-xl.modal.run/v1/inference\",\n      {\n        headers: {\n          Authorization: `Token ${process.env.MODAL_TOKEN_ID}:${process.env.MODAL_TOKEN_SECRET}`,\n          \"Content-Type\": \"application/json\",\n        },\n        method: \"POST\",\n        body: JSON.stringify(requestData),\n      },\n    );\n    const imageData = await result.blob();\n\nCopy\n\n#### Input schema\n\n*   **prompt `string`**\n    *   Input prompt\n*   **height `integer`**\n    *   Height of generated image in pixels. Needs to be a multiple of 64\n    *   One of: `64, 128, 192, 256, 320, 384, 448, 512, 576, 640, 704, 768, 832, 896, 960, 1024`\n    *   Default: `768`\n*   **width `integer`**\n    *   Width of generated image in pixels. Needs to be a multiple of 64\n    *   One of: `64, 128, 192, 256, 320, 384, 448, 512, 576, 640, 704, 768, 832, 896, 960, 1024`\n    *   Default: `768`\n\n#### Output schema\n\nThis endpoint outputs `bytes` for a single image with [media type](https://developer.mozilla.org/en-US/docs/Web/HTTP/Basics_of_HTTP/MIME_types#image)\n `\"image/png\"`.\n\n#### Pricing\n\nRequests to this endpoint use duration based pricing, billed at 1ms granularity. The exact cost per millisecond is based on the underlying GPU hardware. This endpoint use a single NVIDIA A10G device to serve each request.\n\nSee our [pricing page](/pricing)\n for current GPU prices.\n\nInferences usually complete within 15-30 seconds.\n\n### Transcribe speech with vaibhavs10/insanely-fast-whisper\n\nThis endpoint hosts [vaibhavs10/insanely-fast-whisper](https://github.com/chenxwh/insanely-fast-whisper)\n to transcribe and diarize audio.\n\n#### Example code\n\n    // pages/api/modal.ts\n    const data = {\n      audio: dataUrl,\n      diarize_audio: false,\n    };\n    \n    const response = await fetch(\"https://modal-labs--instant-whisper.modal.run\", {\n      headers: {\n        Authorization: `Token ${process.env.MODAL_TOKEN_ID}:${process.env.MODAL_TOKEN_SECRET}`,\n      },\n      method: \"POST\",\n      body: JSON.stringify(requestData),\n    });\n    \n    const output = await response.json();\n\nCopy\n\n#### Input schema\n\n*   **audio `string`**\n    *   Input audio file as a [Data URL](https://developer.mozilla.org/en-US/docs/Web/HTTP/Basics_of_HTTP/Data_URLs)\n        .\n*   **language `string`**\n    *   Language of the input text. Whisper auto-detects the language if not provided. See the full list of options [here](https://github.com/huggingface/transformers/blob/3d2900e829ab16757632f9dde891f1947cfc4be0/src/transformers/models/whisper/tokenization_whisper.py#L95)\n        \n    *   Default: “\n*   **diarize\\_audio `Boolean`**\n    *   Whether to diarize the audio.\n    *   Default: `false`\n*   **batch\\_size `integer`**\n    *   Number of parallel batches.\n    *   Default: `24`\n\n#### Output schema\n\nThis endpoint outputs a JSON with two fields:\n\n*   **text `string`**\n*   **chunks `Chunk[]`**\n\nHere, `Chunk` is a JSON object with the following fields:\n\n*   **speaker `string`**\n    *   \\[Optional\\] only present if `diarize_audio` is `true`\n*   **text `string`**\n*   **timestamp `[float, float]`**\n\n### Stream text-to-speech with coqui-ai/TTS\n\nXTTS v2 is a fast and high-quality text-to-speech model.\n\nThis endpoint uses a streaming version of [coqui-ai/TTS](https://github.com/coqui-ai/TTS)\n that streams wav audio back as it’s generated in real-time.\n\n#### Example code\n\n    // pages/api/modal.ts\n    const requestData = {\n      text: \"It is a mistake to think you can solve any major problems just with potatoes.\",\n      language: \"en\",\n    };\n    const result = await fetch(\"https://modal-labs--instant-xtts-v2.modal.run\", {\n      headers: {\n        Authorization: `Token ${process.env.MODAL_TOKEN_ID}:${process.env.MODAL_TOKEN_SECRET}`,\n        \"Content-Type\": \"application/json\",\n      },\n      method: \"POST\",\n      body: JSON.stringify(requestData),\n    });\n    const audioBuffer = await response.buffer();\n\nCopy\n\n#### Input schema\n\n*   **text `string`**\n    *   Input text\n*   **language `string`**\n    *   Language of the input text\n    *   One of: `en, es, fr, de, it, pt, pl, tr, ru, nl, cs, ar, zh, hu, ko, hi`\n    *   Default: `en`\n\n#### Output schema\n\nThis endpoint streams `bytes` for a single audio file with [media type](https://developer.mozilla.org/en-US/docs/Web/HTTP/Basics_of_HTTP/MIME_types#image)\n `\"audio/wav\"`.\n\n### Want more?\n\nIf a popular open-source AI model API is not listed here, you can [either implement it in Python](/docs/guide/webhooks)\n and host it on Modal or [ask us in Slack](/slack)\n to add it as an Instant Endpoint!\n\n[Connecting Modal to your Vercel account](#connecting-modal-to-your-vercel-account)\n [What this integration does](#what-this-integration-does)\n [Authentication](#authentication)\n [Installing the integration](#installing-the-integration)\n [Uninstalling the integration](#uninstalling-the-integration)\n [Modal Instant Endpoints](#modal-instant-endpoints)\n [Stable Diffusion XL](#stable-diffusion-xl)\n [Example code](#example-code)\n [Input schema](#input-schema)\n [Output schema](#output-schema)\n [Pricing](#pricing)\n [Transcribe speech with vaibhavs10/insanely-fast-whisper](#transcribe-speech-with-vaibhavs10insanely-fast-whisper)\n [Example code](#example-code-1)\n [Input schema](#input-schema-1)\n [Output schema](#output-schema-1)\n [Stream text-to-speech with coqui-ai/TTS](#stream-text-to-speech-withcoqui-aitts)\n [Example code](#example-code-2)\n [Input schema](#input-schema-2)\n [Output schema](#output-schema-2)\n [Want more?](#want-more)",
    "metadata": {
      "title": "Connecting Modal to your Vercel account | Modal Docs",
      "description": "You can use the Modal + Vercel integration to access Modal’s Instant Endpoints from Vercel projects. You’ll find the Modal Vercel integration available for install in the Vercel AI marketplace.",
      "ogTitle": "Connecting Modal to your Vercel account",
      "ogDescription": "You can use the Modal + Vercel integration to access Modal’s Instant Endpoints from Vercel projects. You’ll find the Modal Vercel integration available for install in the Vercel AI marketplace.",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/vercel-integration",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nOkta SSO\n========\n\nPrerequisites\n-------------\n\n*   A Workspace that’s on an [Enterprise](/pricing)\n     plan\n*   Admin access to the Workspace you want to configure with Okta Single-Sign-On (SSO)\n*   Admin privileges for your Okta Organization\n\nSupported features\n------------------\n\n*   IdP-initiated SSO\n*   SP-initiated SSO\n*   Just-In-Time account provisioning\n\nFor more information on the listed features, visit the [Okta Glossary](https://help.okta.com/okta_help.htm?type=oie&id=ext_glossary)\n.\n\nConfiguration\n-------------\n\n### Read this before you enable “Require SSO”\n\nEnabling “Require SSO” will force all users to sign in via Okta. Ensure that you have admin access to your Modal Workspace through an Okta account before enabling.\n\n### Configuration steps\n\n#### Step 1: Add Modal app to Okta Applications\n\n1.  Sign in to your Okta admin dashboard\n    \n2.  Navigate to the Applications tab and click “Browse App Catalog”. ![Okta browse application](https://modal.com/_app/immutable/assets/okta-browse-applications.db6ce98e.png)\n    \n3.  Select “Modal” and click “Done”.\n    \n4.  Select the “Sign On” tab and click “Edit”. ![Okta sign on edit](https://modal.com/_app/immutable/assets/okta-sign-on-edit.7dd90268.png)\n    \n5.  Fill out Workspace field to configure for your specific Modal workspace. See [Step 2](/docs/guide/okta-sso#step-2-link-your-workspace-to-okta-modal-application)\n     if you’re unsure what this is. ![Okta add workspace](https://modal.com/_app/immutable/assets/okta-add-workspace-username.fdac3995.png)\n    \n\n#### Step 2: Link your Workspace to Okta Modal application\n\n1.  Navigate to your application on the Okta Admin page.\n    \n2.  Copy the Metadata URL from the Okta Admin Console (It’s under the “Sign On” tab). ![Okta metadata url](https://modal.com/_app/immutable/assets/okta-metadata-url.496416f7.png)\n    \n3.  Sign in to [https://modal.com](https://modal.com)\n     and visit your Workspace SSO Management page (e.g. `https://modal.com/settings/[workspace name]/sso-management`)\n    \n4.  Paste the Metadata URL in the input and click “Save Changes”\n    \n\n#### Step 3: Assign users / groups and test the integration\n\n1.  Navigate back to your Okta application on the Okta Admin dashboard\n2.  Click on the “Assignments” tab and add the appropriate people or groups.\n\n![Okta Assign Users](https://modal.com/_app/immutable/assets/okta-assign-people.e9c1aa27.png)\n\n3.  To test the integration login sign in to one of the people assigned to the Okta Modal application\n4.  Click on the Modal application on the Okta Dashboard to Single Sign-On\n\n#### Notes\n\nThe following SAML attributes are used by the integration:\n\n| Name | Value |\n| --- | --- |\n| email | user.email |\n| firstName | user.firstName |\n| lastName | user.lastName |\n\nSP-initiated SSO\n----------------\n\nThe sign-in process is initiated from [https://modal.com/login/sso](https://modal.com/login/sso)\n\n1.  Enter your workspace name in the input\n2.  Click “continue with SSO” to authenticate with Okta\n\n[Okta SSO](#okta-sso)\n [Prerequisites](#prerequisites)\n [Supported features](#supported-features)\n [Configuration](#configuration)\n [Read this before you enable “Require SSO”](#read-this-before-you-enable-require-sso)\n [Configuration steps](#configuration-steps)\n [Step 1: Add Modal app to Okta Applications](#step-1-add-modal-app-to-okta-applications)\n [Step 2: Link your Workspace to Okta Modal application](#step-2-link-your-workspace-to-okta-modal-application)\n [Step 3: Assign users / groups and test the integration](#step-3-assign-users--groups-and-test-the-integration)\n [Notes](#notes)\n [SP-initiated SSO](#sp-initiated-sso)",
    "markdown": "* * *\n\nOkta SSO\n========\n\nPrerequisites\n-------------\n\n*   A Workspace that’s on an [Enterprise](/pricing)\n     plan\n*   Admin access to the Workspace you want to configure with Okta Single-Sign-On (SSO)\n*   Admin privileges for your Okta Organization\n\nSupported features\n------------------\n\n*   IdP-initiated SSO\n*   SP-initiated SSO\n*   Just-In-Time account provisioning\n\nFor more information on the listed features, visit the [Okta Glossary](https://help.okta.com/okta_help.htm?type=oie&id=ext_glossary)\n.\n\nConfiguration\n-------------\n\n### Read this before you enable “Require SSO”\n\nEnabling “Require SSO” will force all users to sign in via Okta. Ensure that you have admin access to your Modal Workspace through an Okta account before enabling.\n\n### Configuration steps\n\n#### Step 1: Add Modal app to Okta Applications\n\n1.  Sign in to your Okta admin dashboard\n    \n2.  Navigate to the Applications tab and click “Browse App Catalog”. ![Okta browse application](https://modal.com/_app/immutable/assets/okta-browse-applications.db6ce98e.png)\n    \n3.  Select “Modal” and click “Done”.\n    \n4.  Select the “Sign On” tab and click “Edit”. ![Okta sign on edit](https://modal.com/_app/immutable/assets/okta-sign-on-edit.7dd90268.png)\n    \n5.  Fill out Workspace field to configure for your specific Modal workspace. See [Step 2](/docs/guide/okta-sso#step-2-link-your-workspace-to-okta-modal-application)\n     if you’re unsure what this is. ![Okta add workspace](https://modal.com/_app/immutable/assets/okta-add-workspace-username.fdac3995.png)\n    \n\n#### Step 2: Link your Workspace to Okta Modal application\n\n1.  Navigate to your application on the Okta Admin page.\n    \n2.  Copy the Metadata URL from the Okta Admin Console (It’s under the “Sign On” tab). ![Okta metadata url](https://modal.com/_app/immutable/assets/okta-metadata-url.496416f7.png)\n    \n3.  Sign in to [https://modal.com](https://modal.com)\n     and visit your Workspace SSO Management page (e.g. `https://modal.com/settings/[workspace name]/sso-management`)\n    \n4.  Paste the Metadata URL in the input and click “Save Changes”\n    \n\n#### Step 3: Assign users / groups and test the integration\n\n1.  Navigate back to your Okta application on the Okta Admin dashboard\n2.  Click on the “Assignments” tab and add the appropriate people or groups.\n\n![Okta Assign Users](https://modal.com/_app/immutable/assets/okta-assign-people.e9c1aa27.png)\n\n3.  To test the integration login sign in to one of the people assigned to the Okta Modal application\n4.  Click on the Modal application on the Okta Dashboard to Single Sign-On\n\n#### Notes\n\nThe following SAML attributes are used by the integration:\n\n| Name | Value |\n| --- | --- |\n| email | user.email |\n| firstName | user.firstName |\n| lastName | user.lastName |\n\nSP-initiated SSO\n----------------\n\nThe sign-in process is initiated from [https://modal.com/login/sso](https://modal.com/login/sso)\n\n1.  Enter your workspace name in the input\n2.  Click “continue with SSO” to authenticate with Okta\n\n[Okta SSO](#okta-sso)\n [Prerequisites](#prerequisites)\n [Supported features](#supported-features)\n [Configuration](#configuration)\n [Read this before you enable “Require SSO”](#read-this-before-you-enable-require-sso)\n [Configuration steps](#configuration-steps)\n [Step 1: Add Modal app to Okta Applications](#step-1-add-modal-app-to-okta-applications)\n [Step 2: Link your Workspace to Okta Modal application](#step-2-link-your-workspace-to-okta-modal-application)\n [Step 3: Assign users / groups and test the integration](#step-3-assign-users--groups-and-test-the-integration)\n [Notes](#notes)\n [SP-initiated SSO](#sp-initiated-sso)",
    "metadata": {
      "title": "Okta SSO | Modal Docs",
      "description": "A Workspace that’s on an Enterprise plan",
      "ogTitle": "Okta SSO",
      "ogDescription": "A Workspace that’s on an Enterprise plan",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/okta-sso",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nFile and project structure\n==========================\n\nApps spanning multiple files\n----------------------------\n\nIf you have a project spanning multiple files, you can either use a single Modal [`App`](/docs/guide/apps)\n to create Modal resources across all of them or compose multiple apps using [`app.include(other_app)`](/docs/reference/modal.App#include)\n into a single app at deploy time.\n\nIn this guide we’ll show you how to use composition of multiple smaller files with their own “apps” in order to cleanly separate different parts of your app into multiple files. You can see a realistic instance of a single app use in our [LLM + TTS example](https://github.com/modal-labs/quillman/)\n.\n\nAssume we have a package named `pkg` with files `a.py` and `b.py` that contain functions we want to deploy:\n\n    pkg/\n    ├── __init__.py\n    ├── a.py\n    └── b.py\n\nCopy\n\n    # pkg/a.py\n    a_app = modal.App(\"a\")\n    image_1 = modal.Image.debian_slim().pip_install(\"some_package\")\n    \n    @a_app.function(image=image_1)\n    def f():\n        ...\n\nCopy\n\n    # pkg/b.py\n    b_app = modal.App(\"b\")\n    image_2 = modal.Image.debian_slim().pip_install(\"other_package\")\n    \n    @b_app.function(image=image_2)\n    def g():\n        ...\n\nCopy\n\nTo [deploy](/docs/guide/managing-deployments)\n these resources together, make a single _deployment file_, perhaps `deploy.py` (the name itself doesn’t matter), that imports the apps from each of the sub-modules and includes them in a common parent app that represents your entire app:\n\n    # pkg/deploy.py\n    from .a import a_app\n    from .b import b_app\n    \n    app = modal.App(\"multi-file-app\")\n    app.include(a_app)\n    app.include(b_app)\n\nCopy\n\nNow you can deploy your app by running `modal deploy pkg.deploy` from above the `pkg` directory. Your deployed Modal app will have both the `f` and `g` functions.\n\nThe final file structure now looks like this:\n\n    pkg/\n    ├── __init__.py\n    ├── a.py\n    ├── b.py\n    └── deploy.py\n\nCopy\n\nOne advantage of splitting up apps this way is that you can opt to run only part of your larger app during development. For example, running `modal run a.py` to test some functionality in that part without having to process any changes to the rest of the app.\n\n_Tip: you can also make `__init__.py` your deployment file, which makes deploying a package slightly more convenient. With this, you can deploy your entire project using just `modal deploy pkg`._\n\n**Note:** Since the multi-file app still has a single namespace for all functions, it’s important to name your Modal functions uniquely across the project even when splitting it up across files - otherwise you risk some functions “shadowing” others with the same name.\n\n[File and project structure](#file-and-project-structure)\n [Apps spanning multiple files](#apps-spanning-multiple-files)\n\nSee it in action\n\n[QuiLLMan - Voice Chat with LLMs](https://github.com/modal-labs/quillman)",
    "markdown": "* * *\n\nFile and project structure\n==========================\n\nApps spanning multiple files\n----------------------------\n\nIf you have a project spanning multiple files, you can either use a single Modal [`App`](/docs/guide/apps)\n to create Modal resources across all of them or compose multiple apps using [`app.include(other_app)`](/docs/reference/modal.App#include)\n into a single app at deploy time.\n\nIn this guide we’ll show you how to use composition of multiple smaller files with their own “apps” in order to cleanly separate different parts of your app into multiple files. You can see a realistic instance of a single app use in our [LLM + TTS example](https://github.com/modal-labs/quillman/)\n.\n\nAssume we have a package named `pkg` with files `a.py` and `b.py` that contain functions we want to deploy:\n\n    pkg/\n    ├── __init__.py\n    ├── a.py\n    └── b.py\n\nCopy\n\n    # pkg/a.py\n    a_app = modal.App(\"a\")\n    image_1 = modal.Image.debian_slim().pip_install(\"some_package\")\n    \n    @a_app.function(image=image_1)\n    def f():\n        ...\n\nCopy\n\n    # pkg/b.py\n    b_app = modal.App(\"b\")\n    image_2 = modal.Image.debian_slim().pip_install(\"other_package\")\n    \n    @b_app.function(image=image_2)\n    def g():\n        ...\n\nCopy\n\nTo [deploy](/docs/guide/managing-deployments)\n these resources together, make a single _deployment file_, perhaps `deploy.py` (the name itself doesn’t matter), that imports the apps from each of the sub-modules and includes them in a common parent app that represents your entire app:\n\n    # pkg/deploy.py\n    from .a import a_app\n    from .b import b_app\n    \n    app = modal.App(\"multi-file-app\")\n    app.include(a_app)\n    app.include(b_app)\n\nCopy\n\nNow you can deploy your app by running `modal deploy pkg.deploy` from above the `pkg` directory. Your deployed Modal app will have both the `f` and `g` functions.\n\nThe final file structure now looks like this:\n\n    pkg/\n    ├── __init__.py\n    ├── a.py\n    ├── b.py\n    └── deploy.py\n\nCopy\n\nOne advantage of splitting up apps this way is that you can opt to run only part of your larger app during development. For example, running `modal run a.py` to test some functionality in that part without having to process any changes to the rest of the app.\n\n_Tip: you can also make `__init__.py` your deployment file, which makes deploying a package slightly more convenient. With this, you can deploy your entire project using just `modal deploy pkg`._\n\n**Note:** Since the multi-file app still has a single namespace for all functions, it’s important to name your Modal functions uniquely across the project even when splitting it up across files - otherwise you risk some functions “shadowing” others with the same name.\n\n[File and project structure](#file-and-project-structure)\n [Apps spanning multiple files](#apps-spanning-multiple-files)\n\nSee it in action\n\n[QuiLLMan - Voice Chat with LLMs](https://github.com/modal-labs/quillman)",
    "metadata": {
      "title": "File and project structure | Modal Docs",
      "description": "If you have a project spanning multiple files, you can either use a single Modal App to create Modal resources across all of them or compose multiple apps using app.include(other_app) into a single app at deploy time.",
      "ogTitle": "File and project structure",
      "ogDescription": "If you have a project spanning multiple files, you can either use a single Modal App to create Modal resources across all of them or compose multiple apps using app.include(other_app) into a single app at deploy time.",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/project-structure",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nDeveloping and debugging\n========================\n\nModal makes it easy to run apps in the cloud, try code changes in the cloud, and debug remotely executing code as if it were right there on your laptop. To speed boost your inner dev loop, this guide provides a rundown of tools and techniques for developing and debugging software in Modal.\n\nInteractivity\n-------------\n\nYou can launch a Modal app interactively and have it drop you right into the middle of the action, at an interesting callsite or the site of a runtime detonation.\n\n### Interactive functions\n\nIt is possible to start the interactive Python debugger or start an `IPython` REPL right in the middle of your Modal app.\n\nTo do so, you first need to run your app in “interactive” mode by using the `--interactive` / `-i` flag. In interactive mode, you can establish a connection to the calling terminal by calling `interact()` from within your function.\n\nFor a simple example, you can accept user input with the built-in Python `input` function:\n\n    @app.function()\n    def my_fn(x):\n        modal.interact()\n    \n        print(\"Enter a number:\", end=\" \")\n        x = input()\n        print(f\"Your number is {x}\")\n\nCopy\n\nNow when you run your app with the `--interactive` flag, you’re able to send inputs to your app, even though it’s running in a remote container!\n\n    modal run -i guess_number.py\n    Enter a number: 5\n    Your number is 5\n\nCopy\n\nFor a more interesting example, you can start an `IPython` REPL dynamically anywhere in your code:\n\n    @app.function()\n    def f():\n        model = expensive_function()\n        # play around with model\n        modal.interact()\n        import IPython\n        IPython.embed()\n\nCopy\n\nThe Python debugger can be initiated with the language built-in `breakpoint()` function. For convenience, breakpoints call `interact` automatically.\n\n    @app.function()\n    def f():\n        x = \"10point3\"\n        breakpoint()\n        answer = float(x)\n\nCopy\n\n### Interactive shell\n\nModal lets you run interactive commands on your running containers from the terminal.\n\n#### `modal container exec`\n\nTo run a command inside a running container, you first need to get the container ID. You can view all running containers and their container IDs with [`modal container list`](/docs/reference/cli/container)\n.\n\nAfter you obtain the container ID, you can run commands with `modal container exec [container-id] [command...]`. For example, to run a bash shell, you can run `modal container exec [container-id] /bin/bash`.\n\nNote that your executed command will terminate immediately once your container has finished running.\n\nBy default, commands will be run within a [pseudoterminal (PTY)](https://en.wikipedia.org/wiki/Pseudoterminal)\n, but this can be disabled with the `--no-pty` flag.\n\n#### `modal shell`\n\nYou can also launch an interactive shell in a new container with the same environment as your function. This is handy for debugging issues with your image, interactively refining build commands, and exploring the contents of [`Volume`](/docs/reference/modal.Volume)\ns, [`NetworkFileSystem`](/docs/reference/modal.NetworkFileSystem)\ns, and [`Mount`](/docs/reference/modal.Mount)\ns.\n\nThe primary interface for accessing this feature is the [`modal shell`](/docs/reference/cli/shell)\n CLI command, which accepts a function name in your app (or prompts you to select one, if none is provided), and runs an interactive command on the same image as the function, with the same [`Secret`](/docs/reference/modal.Secret)\ns, [`NetworkFileSystem`](/docs/reference/modal.NetworkFileSystem)\ns and [`Mount`](/docs/reference/modal.Mount)\ns attached as the selected function.\n\nThe default command is `/bin/bash`, but you can override this with any other command of your choice using the `--cmd` flag.\n\nNote that this command does not attach a shell to an existing instance of the function, but instead creates a fresh instance of the underlying image. We might support the former soon - please reach out to us if that would be useful to you.\n\nLive updating\n-------------\n\n### Hot reloading with `modal serve`\n\nModal has the command `modal serve <filename.py>`, which creates a loop that live updates an app when any of the supporting files change.\n\nLive updating works with web endpoints, syncing your changes as you make them, and it also works well with cron schedules and job queues.\n\n    from modal import App, web_endpoint\n    \n    app = App()\n    \n    @app.function()\n    @web_endpoint()\n    def f():\n        return \"I update on file edit!\"\n    \n    @app.function(schedule=modal.Period(seconds=5))\n    def run_me():\n        print(\"I also update on file edit!\")\n\nCopy\n\nIf you edit this file, the `modal serve` command will detect the change and update the code, without having to restart the command.\n\nObservability\n-------------\n\nEach running Modal app, including all ephemeral apps, streams logs and resource metrics back to you for viewing.\n\nOn start, an app will log a dashboard link that will take you its app page.\n\n    $ python3 main.py\n    ✓ Initialized. View app page at https://modal.com/apps/ap-XYZ1234.\n    ...\n\nCopy\n\nFrom this page you can access the following:\n\n*   logs, both from your application and system-level logs from Modal\n*   compute resource metrics (CPU, RAM, GPU)\n*   function call history, including historical success/failure counts\n\n[Developing and debugging](#developing-and-debugging)\n [Interactivity](#interactivity)\n [Interactive functions](#interactive-functions)\n [Interactive shell](#interactive-shell)\n [modal container exec](#modal-container-exec)\n [modal shell](#modal-shell)\n [Live updating](#live-updating)\n [Hot reloading with modal serve](#hot-reloading-with-modal-serve)\n [Observability](#observability)",
    "markdown": "* * *\n\nDeveloping and debugging\n========================\n\nModal makes it easy to run apps in the cloud, try code changes in the cloud, and debug remotely executing code as if it were right there on your laptop. To speed boost your inner dev loop, this guide provides a rundown of tools and techniques for developing and debugging software in Modal.\n\nInteractivity\n-------------\n\nYou can launch a Modal app interactively and have it drop you right into the middle of the action, at an interesting callsite or the site of a runtime detonation.\n\n### Interactive functions\n\nIt is possible to start the interactive Python debugger or start an `IPython` REPL right in the middle of your Modal app.\n\nTo do so, you first need to run your app in “interactive” mode by using the `--interactive` / `-i` flag. In interactive mode, you can establish a connection to the calling terminal by calling `interact()` from within your function.\n\nFor a simple example, you can accept user input with the built-in Python `input` function:\n\n    @app.function()\n    def my_fn(x):\n        modal.interact()\n    \n        print(\"Enter a number:\", end=\" \")\n        x = input()\n        print(f\"Your number is {x}\")\n\nCopy\n\nNow when you run your app with the `--interactive` flag, you’re able to send inputs to your app, even though it’s running in a remote container!\n\n    modal run -i guess_number.py\n    Enter a number: 5\n    Your number is 5\n\nCopy\n\nFor a more interesting example, you can start an `IPython` REPL dynamically anywhere in your code:\n\n    @app.function()\n    def f():\n        model = expensive_function()\n        # play around with model\n        modal.interact()\n        import IPython\n        IPython.embed()\n\nCopy\n\nThe Python debugger can be initiated with the language built-in `breakpoint()` function. For convenience, breakpoints call `interact` automatically.\n\n    @app.function()\n    def f():\n        x = \"10point3\"\n        breakpoint()\n        answer = float(x)\n\nCopy\n\n### Interactive shell\n\nModal lets you run interactive commands on your running containers from the terminal.\n\n#### `modal container exec`\n\nTo run a command inside a running container, you first need to get the container ID. You can view all running containers and their container IDs with [`modal container list`](/docs/reference/cli/container)\n.\n\nAfter you obtain the container ID, you can run commands with `modal container exec [container-id] [command...]`. For example, to run a bash shell, you can run `modal container exec [container-id] /bin/bash`.\n\nNote that your executed command will terminate immediately once your container has finished running.\n\nBy default, commands will be run within a [pseudoterminal (PTY)](https://en.wikipedia.org/wiki/Pseudoterminal)\n, but this can be disabled with the `--no-pty` flag.\n\n#### `modal shell`\n\nYou can also launch an interactive shell in a new container with the same environment as your function. This is handy for debugging issues with your image, interactively refining build commands, and exploring the contents of [`Volume`](/docs/reference/modal.Volume)\ns, [`NetworkFileSystem`](/docs/reference/modal.NetworkFileSystem)\ns, and [`Mount`](/docs/reference/modal.Mount)\ns.\n\nThe primary interface for accessing this feature is the [`modal shell`](/docs/reference/cli/shell)\n CLI command, which accepts a function name in your app (or prompts you to select one, if none is provided), and runs an interactive command on the same image as the function, with the same [`Secret`](/docs/reference/modal.Secret)\ns, [`NetworkFileSystem`](/docs/reference/modal.NetworkFileSystem)\ns and [`Mount`](/docs/reference/modal.Mount)\ns attached as the selected function.\n\nThe default command is `/bin/bash`, but you can override this with any other command of your choice using the `--cmd` flag.\n\nNote that this command does not attach a shell to an existing instance of the function, but instead creates a fresh instance of the underlying image. We might support the former soon - please reach out to us if that would be useful to you.\n\nLive updating\n-------------\n\n### Hot reloading with `modal serve`\n\nModal has the command `modal serve <filename.py>`, which creates a loop that live updates an app when any of the supporting files change.\n\nLive updating works with web endpoints, syncing your changes as you make them, and it also works well with cron schedules and job queues.\n\n    from modal import App, web_endpoint\n    \n    app = App()\n    \n    @app.function()\n    @web_endpoint()\n    def f():\n        return \"I update on file edit!\"\n    \n    @app.function(schedule=modal.Period(seconds=5))\n    def run_me():\n        print(\"I also update on file edit!\")\n\nCopy\n\nIf you edit this file, the `modal serve` command will detect the change and update the code, without having to restart the command.\n\nObservability\n-------------\n\nEach running Modal app, including all ephemeral apps, streams logs and resource metrics back to you for viewing.\n\nOn start, an app will log a dashboard link that will take you its app page.\n\n    $ python3 main.py\n    ✓ Initialized. View app page at https://modal.com/apps/ap-XYZ1234.\n    ...\n\nCopy\n\nFrom this page you can access the following:\n\n*   logs, both from your application and system-level logs from Modal\n*   compute resource metrics (CPU, RAM, GPU)\n*   function call history, including historical success/failure counts\n\n[Developing and debugging](#developing-and-debugging)\n [Interactivity](#interactivity)\n [Interactive functions](#interactive-functions)\n [Interactive shell](#interactive-shell)\n [modal container exec](#modal-container-exec)\n [modal shell](#modal-shell)\n [Live updating](#live-updating)\n [Hot reloading with modal serve](#hot-reloading-with-modal-serve)\n [Observability](#observability)",
    "metadata": {
      "title": "Developing and debugging | Modal Docs",
      "description": "Modal makes it easy to run apps in the cloud, try code changes in the cloud, and debug remotely executing code as if it were right there on your laptop. To speed boost your inner dev loop, this guide provides a rundown of tools and techniques for developing and debugging software in Modal.",
      "ogTitle": "Developing and debugging",
      "ogDescription": "Modal makes it easy to run apps in the cloud, try code changes in the cloud, and debug remotely executing code as if it were right there on your laptop. To speed boost your inner dev loop, this guide provides a rundown of tools and techniques for developing and debugging software in Modal.",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/developing-debugging",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nModal user account setup\n========================\n\nTo run and deploy applications on Modal you’ll need to sign up and create a user account.\n\nYou can visit the [signup](/signup)\n page to begin the process or execute [`modal setup`](/docs/reference/cli/setup#modal-setup)\n on the command line.\n\nUsers can also be provisoned through [Okta SSO](/docs/guide/okta-sso)\n, which is an enterprise feature that you can request. For the typical user you’ll sign-up using an existing GitHub account. If you’re interested in authenticating with other identity providers let us know at [support@modal.com](mailto:support@modal.com)\n.\n\nWhat GitHub permissions does signing up require?\n------------------------------------------------\n\n*   `user:email` — gives us the emails associated with the GitHub account.\n*   `read:org` (invites only) — needed for Modal workspace invites. Note: this only allows us to see what organization memberships you have ([GitHub docs](https://docs.github.com/en/apps/oauth-apps/building-oauth-apps/scopes-for-oauth-apps)\n    ). We won’t be able to access any code repositories or other details.\n\nHow can I change my email?\n--------------------------\n\nYou can change your email on the [settings](/settings)\n page.\n\n[Modal user account setup](#modal-user-account-setup)\n [What GitHub permissions does signing up require?](#what-github-permissions-does-signing-up-require)\n [How can I change my email?](#how-can-i-change-my-email)",
    "markdown": "* * *\n\nModal user account setup\n========================\n\nTo run and deploy applications on Modal you’ll need to sign up and create a user account.\n\nYou can visit the [signup](/signup)\n page to begin the process or execute [`modal setup`](/docs/reference/cli/setup#modal-setup)\n on the command line.\n\nUsers can also be provisoned through [Okta SSO](/docs/guide/okta-sso)\n, which is an enterprise feature that you can request. For the typical user you’ll sign-up using an existing GitHub account. If you’re interested in authenticating with other identity providers let us know at [support@modal.com](mailto:support@modal.com)\n.\n\nWhat GitHub permissions does signing up require?\n------------------------------------------------\n\n*   `user:email` — gives us the emails associated with the GitHub account.\n*   `read:org` (invites only) — needed for Modal workspace invites. Note: this only allows us to see what organization memberships you have ([GitHub docs](https://docs.github.com/en/apps/oauth-apps/building-oauth-apps/scopes-for-oauth-apps)\n    ). We won’t be able to access any code repositories or other details.\n\nHow can I change my email?\n--------------------------\n\nYou can change your email on the [settings](/settings)\n page.\n\n[Modal user account setup](#modal-user-account-setup)\n [What GitHub permissions does signing up require?](#what-github-permissions-does-signing-up-require)\n [How can I change my email?](#how-can-i-change-my-email)",
    "metadata": {
      "title": "Modal user account setup | Modal Docs",
      "description": "To run and deploy applications on Modal you’ll need to sign up and create a user account.",
      "ogTitle": "Modal user account setup",
      "ogDescription": "To run and deploy applications on Modal you’ll need to sign up and create a user account.",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/modal-user-account-setup",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nWorkspaces\n==========\n\nA **workspace** is an area where a user can deploy Modal apps and other resources. There are two types of workspaces: personal and shared. After a new user has signed up to Modal, a personal workspace is automatically created for them. The name of the personal workspace is based on your GitHub username, but it might be randomly generated if already taken or invalid.\n\nTo collaborate with others, a new shared workspace needs to be created.\n\nCreate a Workspace\n------------------\n\nAll additional workspaces are shared workspaces, meaning you can invite others by email to collaborate with you. There are two ways to create a Modal workspace on the [settings](/settings/workspaces)\n page.\n\n![view of workspaces creation interface](https://modal.com/_app/immutable/assets/create-workspace-view.1cf5eeea.png)\n\n1.  Create from [GitHub organization](https://docs.github.com/en/organizations)\n    . Note: Invitees must also be part of the GitHub organization to join.\n    \n2.  Create from scatch. You can invite anyone to your workspace.\n    \n\nIf you’re interested in having a workspace associated with your Okta organization, then check out our [Okta SSO docs](/docs/guide/okta-sso)\n.\n\nInviting new Workspace members\n------------------------------\n\nTo invite a new Workspace member, you can visit the [settings](/settings)\n page and navigate to the members tab for the appropriate workspace.\n\nYou can either send an email invite or share an invite link. Both existing Modal users and non-existing users can use the links to join your workspace. If they are a new user a Modal account will be created for them.\n\n![invite member section](https://modal.com/_app/immutable/assets/invite-member.bb277e60.png)\n\nCreate a token for a Workspace\n------------------------------\n\nTo interact with a Workspace’s resources programmatically, you need to add an API token for that Workspace. Your existing API tokens are displayed on [the settings page](/settings/tokens)\n and new API tokens can be added for a particular Workspace.\n\nAfter adding a token for a Workspace to your Modal config file you can activate that Workspace’s profile using the CLI (see below).\n\nAs an manager or workspace owner you can manage active tokens for a workspace on [the member tokens page](/settings/member-tokens)\n. For more information on API token management see the [documentation about configuration](/docs/reference/modal.config)\n.\n\nSwitching active Workspace\n--------------------------\n\nWhen on the dashboard or using the CLI, the active profile determines which personal or organizational Workspace is associated with your actions.\n\n### Dashboard\n\nYou can switch between organization Workspaces and your Personal Workspace by using the workspace selector at the top of [the dashboard](/home)\n.\n\n### CLI\n\nTo switch the Workspace associated with CLI commands, use `modal profile activate`.\n\nAdministrating workspace members\n--------------------------------\n\nWorkspaces have three different levels of access privileges:\n\n*   Owner\n*   Manager\n*   User\n\nThe user that creates a workspace is automatically set as the **Owner** for that workspace. The owner can assign any other roles within the workspace, as well as disable other members of the workspace.\n\nA **Manager** within a workspace can assign all roles except **Owner** and can also disable other members of the workspace.\n\nA **User** of a workspace can not assign any access privileges within the workspace but can otherwise perform any action like running and deploying apps and modify Secrets.\n\nAs an Owner or Manager you can administrate the access privileges of other members on the members tab in [settings](/settings)\n.\n\nLeaving a Workspace\n-------------------\n\nTo leave a workspace, navigate to [the settings page](/settings/workspaces)\n and click “Leave” on a listed Workspace. There must be at least one owner assigned to a workspace.\n\n[Workspaces](#workspaces)\n [Create a Workspace](#create-a-workspace)\n [Inviting new Workspace members](#inviting-new-workspace-members)\n [Create a token for a Workspace](#create-a-token-for-a-workspace)\n [Switching active Workspace](#switching-active-workspace)\n [Dashboard](#dashboard)\n [CLI](#cli)\n [Administrating workspace members](#administrating-workspace-members)\n [Leaving a Workspace](#leaving-a-workspace)",
    "markdown": "* * *\n\nWorkspaces\n==========\n\nA **workspace** is an area where a user can deploy Modal apps and other resources. There are two types of workspaces: personal and shared. After a new user has signed up to Modal, a personal workspace is automatically created for them. The name of the personal workspace is based on your GitHub username, but it might be randomly generated if already taken or invalid.\n\nTo collaborate with others, a new shared workspace needs to be created.\n\nCreate a Workspace\n------------------\n\nAll additional workspaces are shared workspaces, meaning you can invite others by email to collaborate with you. There are two ways to create a Modal workspace on the [settings](/settings/workspaces)\n page.\n\n![view of workspaces creation interface](https://modal.com/_app/immutable/assets/create-workspace-view.1cf5eeea.png)\n\n1.  Create from [GitHub organization](https://docs.github.com/en/organizations)\n    . Note: Invitees must also be part of the GitHub organization to join.\n    \n2.  Create from scatch. You can invite anyone to your workspace.\n    \n\nIf you’re interested in having a workspace associated with your Okta organization, then check out our [Okta SSO docs](/docs/guide/okta-sso)\n.\n\nInviting new Workspace members\n------------------------------\n\nTo invite a new Workspace member, you can visit the [settings](/settings)\n page and navigate to the members tab for the appropriate workspace.\n\nYou can either send an email invite or share an invite link. Both existing Modal users and non-existing users can use the links to join your workspace. If they are a new user a Modal account will be created for them.\n\n![invite member section](https://modal.com/_app/immutable/assets/invite-member.bb277e60.png)\n\nCreate a token for a Workspace\n------------------------------\n\nTo interact with a Workspace’s resources programmatically, you need to add an API token for that Workspace. Your existing API tokens are displayed on [the settings page](/settings/tokens)\n and new API tokens can be added for a particular Workspace.\n\nAfter adding a token for a Workspace to your Modal config file you can activate that Workspace’s profile using the CLI (see below).\n\nAs an manager or workspace owner you can manage active tokens for a workspace on [the member tokens page](/settings/member-tokens)\n. For more information on API token management see the [documentation about configuration](/docs/reference/modal.config)\n.\n\nSwitching active Workspace\n--------------------------\n\nWhen on the dashboard or using the CLI, the active profile determines which personal or organizational Workspace is associated with your actions.\n\n### Dashboard\n\nYou can switch between organization Workspaces and your Personal Workspace by using the workspace selector at the top of [the dashboard](/home)\n.\n\n### CLI\n\nTo switch the Workspace associated with CLI commands, use `modal profile activate`.\n\nAdministrating workspace members\n--------------------------------\n\nWorkspaces have three different levels of access privileges:\n\n*   Owner\n*   Manager\n*   User\n\nThe user that creates a workspace is automatically set as the **Owner** for that workspace. The owner can assign any other roles within the workspace, as well as disable other members of the workspace.\n\nA **Manager** within a workspace can assign all roles except **Owner** and can also disable other members of the workspace.\n\nA **User** of a workspace can not assign any access privileges within the workspace but can otherwise perform any action like running and deploying apps and modify Secrets.\n\nAs an Owner or Manager you can administrate the access privileges of other members on the members tab in [settings](/settings)\n.\n\nLeaving a Workspace\n-------------------\n\nTo leave a workspace, navigate to [the settings page](/settings/workspaces)\n and click “Leave” on a listed Workspace. There must be at least one owner assigned to a workspace.\n\n[Workspaces](#workspaces)\n [Create a Workspace](#create-a-workspace)\n [Inviting new Workspace members](#inviting-new-workspace-members)\n [Create a token for a Workspace](#create-a-token-for-a-workspace)\n [Switching active Workspace](#switching-active-workspace)\n [Dashboard](#dashboard)\n [CLI](#cli)\n [Administrating workspace members](#administrating-workspace-members)\n [Leaving a Workspace](#leaving-a-workspace)",
    "metadata": {
      "title": "Workspaces | Modal Docs",
      "description": "A workspace is an area where a user can deploy Modal apps and other resources. There are two types of workspaces: personal and shared. After a new user has signed up to Modal, a personal workspace is automatically created for them. The name of the personal workspace is based on your GitHub username, but it might be randomly generated if already taken or invalid.",
      "ogTitle": "Workspaces",
      "ogDescription": "A workspace is an area where a user can deploy Modal apps and other resources. There are two types of workspaces: personal and shared. After a new user has signed up to Modal, a personal workspace is automatically created for them. The name of the personal workspace is based on your GitHub username, but it might be randomly generated if already taken or invalid.",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/workspaces",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nEnvironments\n============\n\nEnvironments are sub-divisons of workspaces, allowing you to deploy the same app (or set of apps) in multiple instances for different purposes without changing your code. Typical use cases for environments include having one `dev` environment and one `prod` environment, preventing overwriting production apps when developing new features, while still being able to deploy changes to a “live” and potentially complex structure of apps.\n\nEach environment has its own set of [Secrets](/docs/guide/secrets)\n and any object lookups performed from an app in an environment will by default look for objects in the same environment.\n\nBy default, every workspace has a single Environment called “main”. New Environments can be created on the CLI:\n\n    modal environment create dev\n\nCopy\n\n(You can run `modal environment --help` for more info)\n\nOnce created, Environments show up as a dropdown menu in the navbar of the [Modal dashboard](/home)\n, letting you set browse all Modal Apps and Secrets filtered by which Environment they were deployed to.\n\nMost CLI commands also support an `--env` flag letting you specify which Environment you intend to interact with, e.g.:\n\n    modal run --env=dev app.py\n    modal nfs create --env=dev storage\n\nCopy\n\nNote that if you have multiple Environments in your workspace and try to interact with it without specifying an Environment, an error will be raised.\n\nTo set a default Environment for your current CLI profile you can use `modal config set-environment`, e.g.:\n\n    modal config set-environment dev\n\nCopy\n\nAlternatively, you can set the `MODAL_ENVIRONMENT` environment variable.\n\nEnvironment web suffixes\n------------------------\n\nEnvironments have a ‘web suffix’ which is used to make [web endpoint URLs](/docs/guide/webhook-urls)\n unique across your workspace. One Environment is allowed to have no suffix (`\"\"`).\n\nCross environment lookups\n-------------------------\n\nIt’s possible to explicitly look up objects in Environments other than the Environment your app runs within:\n\n    production_secret = modal.Secret.from_name(\n        \"my-secret\",\n        environment_name=\"main\"\n    )\n    \n    modal.Function.lookup(\n        \"my_app\",\n        \"some_function\",\n        environment_name=\"dev\"\n    )\n\nCopy\n\nHowever, the `environment_name` argument is optional and omitting it will use the Environment from the object’s associated App or calling context.\n\n[Environments](#environments)\n [Environment web suffixes](#environment-web-suffixes)\n [Cross environment lookups](#cross-environment-lookups)",
    "markdown": "* * *\n\nEnvironments\n============\n\nEnvironments are sub-divisons of workspaces, allowing you to deploy the same app (or set of apps) in multiple instances for different purposes without changing your code. Typical use cases for environments include having one `dev` environment and one `prod` environment, preventing overwriting production apps when developing new features, while still being able to deploy changes to a “live” and potentially complex structure of apps.\n\nEach environment has its own set of [Secrets](/docs/guide/secrets)\n and any object lookups performed from an app in an environment will by default look for objects in the same environment.\n\nBy default, every workspace has a single Environment called “main”. New Environments can be created on the CLI:\n\n    modal environment create dev\n\nCopy\n\n(You can run `modal environment --help` for more info)\n\nOnce created, Environments show up as a dropdown menu in the navbar of the [Modal dashboard](/home)\n, letting you set browse all Modal Apps and Secrets filtered by which Environment they were deployed to.\n\nMost CLI commands also support an `--env` flag letting you specify which Environment you intend to interact with, e.g.:\n\n    modal run --env=dev app.py\n    modal nfs create --env=dev storage\n\nCopy\n\nNote that if you have multiple Environments in your workspace and try to interact with it without specifying an Environment, an error will be raised.\n\nTo set a default Environment for your current CLI profile you can use `modal config set-environment`, e.g.:\n\n    modal config set-environment dev\n\nCopy\n\nAlternatively, you can set the `MODAL_ENVIRONMENT` environment variable.\n\nEnvironment web suffixes\n------------------------\n\nEnvironments have a ‘web suffix’ which is used to make [web endpoint URLs](/docs/guide/webhook-urls)\n unique across your workspace. One Environment is allowed to have no suffix (`\"\"`).\n\nCross environment lookups\n-------------------------\n\nIt’s possible to explicitly look up objects in Environments other than the Environment your app runs within:\n\n    production_secret = modal.Secret.from_name(\n        \"my-secret\",\n        environment_name=\"main\"\n    )\n    \n    modal.Function.lookup(\n        \"my_app\",\n        \"some_function\",\n        environment_name=\"dev\"\n    )\n\nCopy\n\nHowever, the `environment_name` argument is optional and omitting it will use the Environment from the object’s associated App or calling context.\n\n[Environments](#environments)\n [Environment web suffixes](#environment-web-suffixes)\n [Cross environment lookups](#cross-environment-lookups)",
    "metadata": {
      "title": "Environments | Modal Docs",
      "description": "Environments are sub-divisons of workspaces, allowing you to deploy the same app (or set of apps) in multiple instances for different purposes without changing your code. Typical use cases for environments include having one dev environment and one prod environment, preventing overwriting production apps when developing new features, while still being able to deploy changes to a “live” and potentially complex structure of apps.",
      "ogTitle": "Environments",
      "ogDescription": "Environments are sub-divisons of workspaces, allowing you to deploy the same app (or set of apps) in multiple instances for different purposes without changing your code. Typical use cases for environments include having one dev environment and one prod environment, preventing overwriting production apps when developing new features, while still being able to deploy changes to a “live” and potentially complex structure of apps.",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/environments",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nJupyter notebooks\n=================\n\nYou can use the Modal client library in notebook environments like Jupyter! Just `import modal` and use as normal. However, there are some limitations when using Modal within notebooks.\n\nKnown issues\n------------\n\n*   **Interactive shell and interactive functions are not supported.**\n    \n    These can only be run within a live terminal session, so they are not supported in notebooks.\n    \n\nIf you encounter issues not documented above, first check your Modal client version is `>=0.49.2142`. Also, try restarting the notebook kernel, as it may be in a broken state, which is common in notebook development.\n\nIf the issue persists, contact us [in our Slack](https://modal.com/slack)\n.\n\nWe are working on removing these known issues so that writing Modal applications in a notebook feels just like developing in regular Python modules and scripts.\n\nJupyter inside Modal\n--------------------\n\nYou can run Jupyter in Modal using the `modal launch` command. For example:\n\n    $ modal launch jupyter --gpu a10g\n\nCopy\n\nThat will start a Jupyter instance with an A10G GPU attached. You’ll be able to access the app with via a [Modal Tunnel URL](https://modal.com/docs/guide/tunnels#tunnels-beta)\n. Jupyter will stop running whenever you stop Modal call in your terminal.\n\nSee `--help` for additional options.\n\nFurther examples\n----------------\n\n*   [Basic demonstration of running Modal in a notebook](https://github.com/modal-labs/modal-examples/blob/main/11_notebooks/basic.ipynb)\n    \n*   [Running Jupyter server within a Modal function](https://github.com/modal-labs/modal-examples/blob/main/11_notebooks/jupyter_inside_modal.py)\n    \n\n[Jupyter notebooks](#jupyter-notebooks)\n [Known issues](#known-issues)\n [Jupyter inside Modal](#jupyter-inside-modal)\n [Further examples](#further-examples)",
    "markdown": "* * *\n\nJupyter notebooks\n=================\n\nYou can use the Modal client library in notebook environments like Jupyter! Just `import modal` and use as normal. However, there are some limitations when using Modal within notebooks.\n\nKnown issues\n------------\n\n*   **Interactive shell and interactive functions are not supported.**\n    \n    These can only be run within a live terminal session, so they are not supported in notebooks.\n    \n\nIf you encounter issues not documented above, first check your Modal client version is `>=0.49.2142`. Also, try restarting the notebook kernel, as it may be in a broken state, which is common in notebook development.\n\nIf the issue persists, contact us [in our Slack](https://modal.com/slack)\n.\n\nWe are working on removing these known issues so that writing Modal applications in a notebook feels just like developing in regular Python modules and scripts.\n\nJupyter inside Modal\n--------------------\n\nYou can run Jupyter in Modal using the `modal launch` command. For example:\n\n    $ modal launch jupyter --gpu a10g\n\nCopy\n\nThat will start a Jupyter instance with an A10G GPU attached. You’ll be able to access the app with via a [Modal Tunnel URL](https://modal.com/docs/guide/tunnels#tunnels-beta)\n. Jupyter will stop running whenever you stop Modal call in your terminal.\n\nSee `--help` for additional options.\n\nFurther examples\n----------------\n\n*   [Basic demonstration of running Modal in a notebook](https://github.com/modal-labs/modal-examples/blob/main/11_notebooks/basic.ipynb)\n    \n*   [Running Jupyter server within a Modal function](https://github.com/modal-labs/modal-examples/blob/main/11_notebooks/jupyter_inside_modal.py)\n    \n\n[Jupyter notebooks](#jupyter-notebooks)\n [Known issues](#known-issues)\n [Jupyter inside Modal](#jupyter-inside-modal)\n [Further examples](#further-examples)",
    "metadata": {
      "title": "Jupyter notebooks | Modal Docs",
      "description": "You can use the Modal client library in notebook environments like Jupyter! Just import modal and use as normal. However, there are some limitations when using Modal within notebooks.",
      "ogTitle": "Jupyter notebooks",
      "ogDescription": "You can use the Modal client library in notebook environments like Jupyter! Just import modal and use as normal. However, there are some limitations when using Modal within notebooks.",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/notebooks",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nAsynchronous API usage\n======================\n\nAll of the functions in Modal are available in both standard (blocking) and asynchronous variants. The async interface can be accessed by appending `.aio` to any function in the Modal API.\n\nFor example, instead of `my_modal_funcion.remote(\"hello\")` in a blocking context, you can use `await my_modal_function.remote.aio(\"hello\")` to get an asynchronous coroutine response, for use with Python’s `asyncio` library.\n\n    import asyncio\n    import modal\n    \n    app = modal.App()\n    \n    \n    @app.function()\n    async def myfunc():\n        ...\n    \n    \n    @app.local_entrypoint()\n    async def main():\n        # execute 100 remote calls to myfunc in parallel\n        await asyncio.gather(*[myfunc.remote.aio() for i in range(100)])\n\nCopy\n\nThis is an advanced feature. If you are comfortable with asynchronous programming, you can use this to create arbitrary parallel execution patterns, with the added benefit that any Modal functions will be executed remotely.\n\nAsync functions\n---------------\n\nRegardless if you use an async runtime (like `asyncio`) in your usage of _Modal itself_, you are free to define your `app.function`\\-decorated function bodies as either async or blocking. Both kinds of definitions will work for remote Modal function calls from both any context.\n\nAn async function can call a blocking function, and vice versa.\n\n    @app.function()\n    def blocking_function():\n        return 42\n    \n    \n    @app.function()\n    async def async_function():\n        x = await blocking_function.remote.aio()\n        return x * 10\n    \n    \n    @app.local_entrypoint()\n    def blocking_main():\n        print(async_function.remote())  # => 420\n\nCopy\n\nIf a function is configured to support multiple concurrent inputs per container, the behavior varies slightly between blocking and async contexts:\n\n*   In a blocking context, concurrent inputs will run on separate Python threads. These are subject to the GIL, but they can still lead to race conditions if used with non-threadsafe objects.\n*   In an async context, concurrent inputs are simply scheduled as coroutines on the executor thread. Everything remains single-threaded.\n\n[Asynchronous API usage](#asynchronous-api-usage)\n [Async functions](#async-functions)",
    "markdown": "* * *\n\nAsynchronous API usage\n======================\n\nAll of the functions in Modal are available in both standard (blocking) and asynchronous variants. The async interface can be accessed by appending `.aio` to any function in the Modal API.\n\nFor example, instead of `my_modal_funcion.remote(\"hello\")` in a blocking context, you can use `await my_modal_function.remote.aio(\"hello\")` to get an asynchronous coroutine response, for use with Python’s `asyncio` library.\n\n    import asyncio\n    import modal\n    \n    app = modal.App()\n    \n    \n    @app.function()\n    async def myfunc():\n        ...\n    \n    \n    @app.local_entrypoint()\n    async def main():\n        # execute 100 remote calls to myfunc in parallel\n        await asyncio.gather(*[myfunc.remote.aio() for i in range(100)])\n\nCopy\n\nThis is an advanced feature. If you are comfortable with asynchronous programming, you can use this to create arbitrary parallel execution patterns, with the added benefit that any Modal functions will be executed remotely.\n\nAsync functions\n---------------\n\nRegardless if you use an async runtime (like `asyncio`) in your usage of _Modal itself_, you are free to define your `app.function`\\-decorated function bodies as either async or blocking. Both kinds of definitions will work for remote Modal function calls from both any context.\n\nAn async function can call a blocking function, and vice versa.\n\n    @app.function()\n    def blocking_function():\n        return 42\n    \n    \n    @app.function()\n    async def async_function():\n        x = await blocking_function.remote.aio()\n        return x * 10\n    \n    \n    @app.local_entrypoint()\n    def blocking_main():\n        print(async_function.remote())  # => 420\n\nCopy\n\nIf a function is configured to support multiple concurrent inputs per container, the behavior varies slightly between blocking and async contexts:\n\n*   In a blocking context, concurrent inputs will run on separate Python threads. These are subject to the GIL, but they can still lead to race conditions if used with non-threadsafe objects.\n*   In an async context, concurrent inputs are simply scheduled as coroutines on the executor thread. Everything remains single-threaded.\n\n[Asynchronous API usage](#asynchronous-api-usage)\n [Async functions](#async-functions)",
    "metadata": {
      "title": "Asynchronous API usage | Modal Docs",
      "description": "All of the functions in Modal are available in both standard (blocking) and asynchronous variants. The async interface can be accessed by appending .aio to any function in the Modal API.",
      "ogTitle": "Asynchronous API usage",
      "ogDescription": "All of the functions in Modal are available in both standard (blocking) and asynchronous variants. The async interface can be accessed by appending .aio to any function in the Modal API.",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/async",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nGlobal variables\n================\n\nThere are cases where you might want objects or data available in **global** scope. For example:\n\n*   You need to use the data in a scheduled function (scheduled functions don’t accept arguments)\n*   You need to construct objects (e.g. Secrets) in global scope to use as function annotations\n*   You don’t want to clutter many function signatures with some common arguments they all use, and pass the same arguments through many layers of function calls.\n\nFor these cases, you can use the `modal.is_local` function, which returns `True` if the app is running locally (initializing) or `False` if the app is executing in the cloud.\n\nFor instance, you can use a [`modal.Dict`](/docs/reference/modal.Dict)\n object to store one or multiple objects by key (similar to a Python dict) for later access in Modal functions:\n\n    import json\n    import random\n    \n    if modal.is_local():\n        with open(\"list.json\", \"r\") as f:\n            foo_list = json.load(f)  # reads from local disk on the development machine\n        app.data_dict = modal.Dict({\"foo\": foo_list})\n    \n    \n    @app.function(schedule=modal.Period(days=1))\n    def daily_random_entry():\n        print(random.choice(app.data_dict[\"foo\"]))\n\nCopy\n\nSimilarly, to create a [`modal.Secret`](/docs/guide/secrets)\n that you can pass to your function decorators to create environment variables, you can run:\n\n    import os\n    \n    if modal.is_local():\n        pg_password = modal.Secret.from_dict({\"PGPASS\": os.environ[\"MY_LOCAL_PASSWORD\"]})\n    else:\n        pg_password = modal.Secret.from_dict({})\n    \n    \n    @app.function(secrets=[pg_password])\n    def get_secret_data():\n        connection = psycopg2.connect(password=os.environ[\"PGPASS\"])\n        ...\n\nCopy\n\nWarning about regular module globals\n------------------------------------\n\nIf you try to construct a global in module scope using some local data _without_ using something like `modal.is_local`, it might have unexpected effects since your Python modules will be not only be loaded on your local machine, but also on the remote worker.\n\nE.g., this will typically not work:\n\n    # blob.json doesn't exist on the remote worker, so this will cause an error there\n    data_blob = open(\"blob.json\", \"r\").read()\n    \n    @app.function()\n    def foo():\n        print(data_blob)\n\nCopy\n\n[Global variables](#global-variables)\n [Warning about regular module globals](#warning-about-regular-module-globals)",
    "markdown": "* * *\n\nGlobal variables\n================\n\nThere are cases where you might want objects or data available in **global** scope. For example:\n\n*   You need to use the data in a scheduled function (scheduled functions don’t accept arguments)\n*   You need to construct objects (e.g. Secrets) in global scope to use as function annotations\n*   You don’t want to clutter many function signatures with some common arguments they all use, and pass the same arguments through many layers of function calls.\n\nFor these cases, you can use the `modal.is_local` function, which returns `True` if the app is running locally (initializing) or `False` if the app is executing in the cloud.\n\nFor instance, you can use a [`modal.Dict`](/docs/reference/modal.Dict)\n object to store one or multiple objects by key (similar to a Python dict) for later access in Modal functions:\n\n    import json\n    import random\n    \n    if modal.is_local():\n        with open(\"list.json\", \"r\") as f:\n            foo_list = json.load(f)  # reads from local disk on the development machine\n        app.data_dict = modal.Dict({\"foo\": foo_list})\n    \n    \n    @app.function(schedule=modal.Period(days=1))\n    def daily_random_entry():\n        print(random.choice(app.data_dict[\"foo\"]))\n\nCopy\n\nSimilarly, to create a [`modal.Secret`](/docs/guide/secrets)\n that you can pass to your function decorators to create environment variables, you can run:\n\n    import os\n    \n    if modal.is_local():\n        pg_password = modal.Secret.from_dict({\"PGPASS\": os.environ[\"MY_LOCAL_PASSWORD\"]})\n    else:\n        pg_password = modal.Secret.from_dict({})\n    \n    \n    @app.function(secrets=[pg_password])\n    def get_secret_data():\n        connection = psycopg2.connect(password=os.environ[\"PGPASS\"])\n        ...\n\nCopy\n\nWarning about regular module globals\n------------------------------------\n\nIf you try to construct a global in module scope using some local data _without_ using something like `modal.is_local`, it might have unexpected effects since your Python modules will be not only be loaded on your local machine, but also on the remote worker.\n\nE.g., this will typically not work:\n\n    # blob.json doesn't exist on the remote worker, so this will cause an error there\n    data_blob = open(\"blob.json\", \"r\").read()\n    \n    @app.function()\n    def foo():\n        print(data_blob)\n\nCopy\n\n[Global variables](#global-variables)\n [Warning about regular module globals](#warning-about-regular-module-globals)",
    "metadata": {
      "title": "Global variables | Modal Docs",
      "description": "There are cases where you might want objects or data available in global scope. For example:",
      "ogTitle": "Global variables",
      "ogDescription": "There are cases where you might want objects or data available in global scope. For example:",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/global-variables",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nRegion selection\n================\n\nModal allows you to specify which cloud region you would like to run a function in. This may be useful if:\n\n*   you are required (for regulatory reasons or by your customers) to process data within certain regions.\n*   you want to reduce egress fees that result from reading data from a dependency like S3.\n*   you have a latency-sensitive app where app endpoints need to run near your users and/or near an external DB.\n\nOur [Enterprise plan](/pricing)\n offers full access to this feature while our [Team plan](/pricing)\n provides a more [limited](/settings/plans)\n version. Please contact [support@modal.com](mailto:support@modal.com)\n to get access.\n\nSpecifying a region\n-------------------\n\nTo run your Modal function in a specific region, pass a `region=` argument to the function decorator.\n\n    import os\n    import modal\n    \n    app = modal.App(\"...\")\n    \n    @app.function(region=\"us-east\") # also supports a list of options, for example region=[\"us-central\", \"us-east\"]\n    def function():\n        print(f\"running in {os.environ['MODAL_REGION']}\") # us-east-1, us-east-2, us-ashburn-1, etc.\n\nCopy\n\nYou can specify a region in addition to the underlying cloud, `@app.function(cloud=\"aws\", region=\"us-east\")` would run your function only in `\"us-east-1\"` or `\"us-east-2\"` for instance.\n\nRegion options\n--------------\n\nModal offers varying levels of granularity for regions. Use broader regions when possible, as this increases the pool of available resources your function can be assigned to, which improves cold-start time and availability.\n\n### United States (“us”)\n\nUse `region=\"us\"` to select any region in the United States.\n\n         Broad            Specific             Description\n     ==============================================================\n      \"us-east\"           \"us-east-1\"          AWS Virginia\n                          \"us-east-2\"          AWS Ohio\n                          \"us-east1\"           GCP South Carolina\n                          \"us-east4\"           GCP Virginia\n                          \"us-ashburn-1\"       OCI Virginia\n     --------------------------------------------------------------\n      \"us-central\"        \"us-central1\"        GCP Iowa\n                          \"us-chicago-1\"       OCI Chicago\n     --------------------------------------------------------------\n      \"us-west\"           \"us-west-1\"          AWS California\n                          \"us-west-2\"          AWS Oregon\n                          \"us-west1\"           GCP Oregon\n                          \"us-sanjose-1\"       OCI San Jose\n\nCopy\n\n### Europe (“eu”)\n\nUse `region=\"eu\"` to select any region in Europe.\n\n         Broad            Specific             Description\n     ==============================================================\n      \"eu-west\"           \"eu-central-1\"       AWS Frankfurt\n                          \"eu-west-1\"          AWS Ireland\n                          \"eu-west-2\"          AWS London\n                          \"eu-west-3\"          AWS Paris\n                          \"europe-west4\"       GCP Netherlands\n                          \"eu-frankfurt-1\"     OCI Frankfurt\n                          \"eu-paris-1\"         OCI Paris\n                          \"uk-london-1\"        OCI London\n     --------------------------------------------------------------\n      \"eu-north\"          \"eu-north-1\"         AWS Stockholm\n\nCopy\n\n### Asia–Pacific (“ap”)\n\nUse `region=\"ap\"` to select any region in Asia–Pacific.\n\n         Broad            Specific             Description\n     ==============================================================\n      \"ap-northeast\"      \"asia-northeast3\"    GCP Seoul\n     --------------------------------------------------------------\n      \"ap-southeast\"      \"asia-southeast1\"    GCP Singapore\n                          \"ap-southeast-3\"     OCI Singapore\n     --------------------------------------------------------------\n      \"ap-melbourne\"      \"ap-melbourne-1\"     OCI Melbourne\n\nCopy\n\n### Other regions\n\n         Broad            Specific             Description\n     ==============================================================\n      \"ca\"                \"ca-central-1\"       AWS Montreal\n                          \"ca-toronto-1\"       OCI Toronto\n     --------------------------------------------------------------\n      \"me\"                \"me-west1\"           OCI Bahrain\n     --------------------------------------------------------------\n      \"sa\"                \"sa-east-1\"          AWS São Paulo\n\nCopy\n\n[Region selection](#region-selection)\n [Specifying a region](#specifying-a-region)\n [Region options](#region-options)\n [United States (“us”)](#united-states-us)\n [Europe (“eu”)](#europe-eu)\n [Asia–Pacific (“ap”)](#asiapacific-ap)\n [Other regions](#other-regions)",
    "markdown": "* * *\n\nRegion selection\n================\n\nModal allows you to specify which cloud region you would like to run a function in. This may be useful if:\n\n*   you are required (for regulatory reasons or by your customers) to process data within certain regions.\n*   you want to reduce egress fees that result from reading data from a dependency like S3.\n*   you have a latency-sensitive app where app endpoints need to run near your users and/or near an external DB.\n\nOur [Enterprise plan](/pricing)\n offers full access to this feature while our [Team plan](/pricing)\n provides a more [limited](/settings/plans)\n version. Please contact [support@modal.com](mailto:support@modal.com)\n to get access.\n\nSpecifying a region\n-------------------\n\nTo run your Modal function in a specific region, pass a `region=` argument to the function decorator.\n\n    import os\n    import modal\n    \n    app = modal.App(\"...\")\n    \n    @app.function(region=\"us-east\") # also supports a list of options, for example region=[\"us-central\", \"us-east\"]\n    def function():\n        print(f\"running in {os.environ['MODAL_REGION']}\") # us-east-1, us-east-2, us-ashburn-1, etc.\n\nCopy\n\nYou can specify a region in addition to the underlying cloud, `@app.function(cloud=\"aws\", region=\"us-east\")` would run your function only in `\"us-east-1\"` or `\"us-east-2\"` for instance.\n\nRegion options\n--------------\n\nModal offers varying levels of granularity for regions. Use broader regions when possible, as this increases the pool of available resources your function can be assigned to, which improves cold-start time and availability.\n\n### United States (“us”)\n\nUse `region=\"us\"` to select any region in the United States.\n\n         Broad            Specific             Description\n     ==============================================================\n      \"us-east\"           \"us-east-1\"          AWS Virginia\n                          \"us-east-2\"          AWS Ohio\n                          \"us-east1\"           GCP South Carolina\n                          \"us-east4\"           GCP Virginia\n                          \"us-ashburn-1\"       OCI Virginia\n     --------------------------------------------------------------\n      \"us-central\"        \"us-central1\"        GCP Iowa\n                          \"us-chicago-1\"       OCI Chicago\n     --------------------------------------------------------------\n      \"us-west\"           \"us-west-1\"          AWS California\n                          \"us-west-2\"          AWS Oregon\n                          \"us-west1\"           GCP Oregon\n                          \"us-sanjose-1\"       OCI San Jose\n\nCopy\n\n### Europe (“eu”)\n\nUse `region=\"eu\"` to select any region in Europe.\n\n         Broad            Specific             Description\n     ==============================================================\n      \"eu-west\"           \"eu-central-1\"       AWS Frankfurt\n                          \"eu-west-1\"          AWS Ireland\n                          \"eu-west-2\"          AWS London\n                          \"eu-west-3\"          AWS Paris\n                          \"europe-west4\"       GCP Netherlands\n                          \"eu-frankfurt-1\"     OCI Frankfurt\n                          \"eu-paris-1\"         OCI Paris\n                          \"uk-london-1\"        OCI London\n     --------------------------------------------------------------\n      \"eu-north\"          \"eu-north-1\"         AWS Stockholm\n\nCopy\n\n### Asia–Pacific (“ap”)\n\nUse `region=\"ap\"` to select any region in Asia–Pacific.\n\n         Broad            Specific             Description\n     ==============================================================\n      \"ap-northeast\"      \"asia-northeast3\"    GCP Seoul\n     --------------------------------------------------------------\n      \"ap-southeast\"      \"asia-southeast1\"    GCP Singapore\n                          \"ap-southeast-3\"     OCI Singapore\n     --------------------------------------------------------------\n      \"ap-melbourne\"      \"ap-melbourne-1\"     OCI Melbourne\n\nCopy\n\n### Other regions\n\n         Broad            Specific             Description\n     ==============================================================\n      \"ca\"                \"ca-central-1\"       AWS Montreal\n                          \"ca-toronto-1\"       OCI Toronto\n     --------------------------------------------------------------\n      \"me\"                \"me-west1\"           OCI Bahrain\n     --------------------------------------------------------------\n      \"sa\"                \"sa-east-1\"          AWS São Paulo\n\nCopy\n\n[Region selection](#region-selection)\n [Specifying a region](#specifying-a-region)\n [Region options](#region-options)\n [United States (“us”)](#united-states-us)\n [Europe (“eu”)](#europe-eu)\n [Asia–Pacific (“ap”)](#asiapacific-ap)\n [Other regions](#other-regions)",
    "metadata": {
      "title": "Region selection | Modal Docs",
      "description": "Modal allows you to specify which cloud region you would like to run a function in. This may be useful if:",
      "ogTitle": "Region selection",
      "ogDescription": "Modal allows you to specify which cloud region you would like to run a function in. This may be useful if:",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/region-selection",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nContainer lifecycle hooks and parameters\n========================================\n\nSince Modal [reuses the same container for multiple inputs](/docs/guide/cold-start#keep-containers-warm-for-longer-with-container_idle_timeout)\n, sometimes you might want to run some code exactly once when the container starts or exits. In addition, you might want to pass some parameters to the startup function that do not change between invocations (e.g. the name of a model that’s slow to load).\n\nTo accomplish any of these things, you need to use Modal’s class syntax and the [`@app.cls`](/docs/reference/modal.App#cls)\n decorator. Specifically, you’ll need to:\n\n1.  Convert your function to a method by making it a member of a class.\n2.  Decorate the class with `@app.cls(...)` with same arguments you previously had for `@app.xyz(...)`.\n3.  Instead of the `@app.function` decorator on the original method, use `@method` or the appropriate decorator for a [web endpoint](#lifecycle-hooks-for-web-endpoints)\n    .\n4.  Add the correct method “hooks” to your class based on your need:\n    *   `@enter` for one-time initialization (remote)\n    *   `@exit` for one-time cleanup (remote)\n    *   `@build` to run the function during image build and snapshot the results\n    *   A constructor (`__init__`) for [parametrized container pools](/docs/guide/lifecycle-functions#parametrized-functions)\n        \n\n`@enter`\n--------\n\nThe container entry handler is called when a new container is started. This is useful for doing one-time initialization, such as loading model weights or importing packages that are only present in that image.\n\nTo use, make your function a member of a class, and apply the `@enter()` decorator to one or more class methods:\n\n    from modal import App, enter, method\n    \n    app = App()\n    \n    @app.cls(cpu=8)\n    class Model:\n        @enter()\n        def run_this_on_container_startup(self):\n            self.model = pickle.load(open(\"model.pickle\"))\n    \n        @method()\n        def predict(self, x):\n            return self.model.predict(x)\n    \n    \n    @app.local_entrypoint()\n    def main():\n        Model().predict.remote(x)\n\nCopy\n\nWhen working with an [asynchronous Modal](/docs/guide/async)\n app, you may use an async method instead:\n\n    from modal import App, enter, method\n    \n    app = App()\n    \n    @app.cls(memory=1024)\n    class Processor:\n        @enter()\n        async def my_enter_method(self):\n            self.cache = await load_cache()\n    \n        @method()\n        async def run(self, x):\n            return await do_some_async_stuff(x, self.cache)\n    \n    \n    @app.local_entrypoint()\n    async def main():\n        await Processor().run.remote(x)\n\nCopy\n\nNote: The `@enter()` decorator replaces the earlier `__enter__` syntax, which has been deprecated.\n\n`@exit`\n-------\n\nThe container exit handler is called when a container is about to exit. It is useful for doing one-time cleanup, such as closing a database connection or saving intermediate results. To use, make your function a member of a class, and apply the `@exit()` decorator:\n\n    from modal import App, enter, exit, method\n    \n    app = App()\n    \n    @app.cls()\n    class ETLPipeline:\n        @enter()\n        def open_connection(self):\n            import psycopg2\n            self.connection = psycopg2.connect(os.environ[\"DATABASE_URI\"])\n    \n        @method()\n        def run(self):\n            # Run some queries\n            pass\n    \n        @exit()\n        def close_connection(self):\n            self.connection.close()\n    \n    \n    @app.local_entrypoint()\n    def main():\n        ETLPipeline().run.remote()\n\nCopy\n\nNote that the exit handler is given a grace period of 30 seconds to exit, and it will be killed if it takes longer than that to complete.\n\nNote: The `@exit()` decorator replaces the earlier `__exit__` syntax, which has been deprecated. Like `__exit__`, the method decorated by `@exit` previously needed to accept arguments containing exception information, but this is no longer supported.\n\n`@build`\n--------\n\nThe `@build()` decorator lets us define code that runs as a part of building the container image. This might be useful for downloading model weights and storing it as a part of the image:\n\n    from modal import App, build, enter, method\n    \n    app = App()\n    \n    @app.cls()\n    class Model:\n        @build()\n        def download_model(self):\n            download_model_to_disk()\n    \n        @enter()\n        def load_model(self):\n            load_model_from_disk()\n    \n        @method()\n        def predict(self, x):\n            ...\n\nCopy\n\nThe `@build` and `@enter` decorators can be stacked. This can be useful with tools like `tranformers` which lets you download model weights over the network but caches the weights locally. By making the initialization method run during image build, we make sure the model weights are cached in the image, which makes containers start faster.\n\n    from modal import App, build, enter, method\n    \n    app = App()\n    \n    @app.cls()\n    class Model:\n        @build()\n        @enter()\n        def load_model(self):\n            load_model_from_network(local_cache_dir=\"/\")\n    \n        @method()\n        def predict(self, x):\n            ...\n\nCopy\n\nParametrized functions\n----------------------\n\nImagine this scenario: you want to run different variants of a model based on some argument (say the size of the model), but still share the same code for all of these variants.\n\nIn other words, instead of defining a single Modal function, you want to define a family of functions parametrized by a set of arguments.\n\nTo do this, you can define an `__init__` (constructor) method on your class that accepts some arguments and performs the necessary initialization:\n\n    from modal import App, method\n    \n    app = App()\n    \n    @app.cls(gpu=\"A100\")\n    class Model():\n        def __init__(self, model_name: str, size: int) -> None:\n            self.model = load_model(model_name, size)\n    \n        @method()\n        def generate(self):\n            self.model.generate(...)\n\nCopy\n\nThen, you can construct a remote object with the desired parameters, and call the method on it:\n\n    @app.local_entrypoint()\n    def main():\n        m1 = Model(\"hedgehog\", size=7)\n        m1.generate.remote()\n    \n        m2 = Model(\"fox\", size=13)\n        m2.generate.remote()\n\nCopy\n\nEach variant of the model will behave like an independent Modal function. In addition, each pool is uniquely identified by a hash of the parameters. This means that if you constructed a `Model` with the same parameters in a different context, the calls to `generate` would be routed to the same set of containers as before.\n\nNote that any method annotated with `@enter` will still run remotely after `__init__`.\n\nArguments to `__init__` have a maximum size limit of 16 KiB.\n\n### Looking up a parametrized function\n\nIf you want to call your parametrized function from a Python script running anywhere, you can use `Cls.lookup`:\n\n    from modal import Cls\n    \n    Model = Cls.lookup(\"cls-app\", \"Model\")  # returns a class-like object\n    m = Model(\"snake\", size=12)\n    m.generate.remote()\n\nCopy\n\nWeb endpoints for parametrized functions is not supported at this point.\n\nLifecycle hooks for web endpoints\n---------------------------------\n\nModal `@function`s that are [web endpoints](/docs/guide/webhooks)\n can be converted to the class syntax as well. Instead of `@modal.method`, simply use whichever of the web endpoint decorators (`@modal.web_endpoint`, `@modal.asgi_app` or `@modal.wsgi_app`) you were using before.\n\n    from fastapi import Request\n    from modal import App, enter, web_endpoint\n    \n    app = App(\"web-endpoint-cls\")\n    \n    @app.cls()\n    class Model:\n        @enter()\n        def run_this_on_container_startup(self):\n            self.model = pickle.load(open(\"model.pickle\"))\n    \n        @web_endpoint()\n        def predict(self, request: Request):\n            ...\n\nCopy\n\n[Container lifecycle hooks and parameters](#container-lifecycle-hooks-and-parameters)\n [@enter](#enter)\n [@exit](#exit)\n [@build](#build)\n [Parametrized functions](#parametrized-functions)\n [Looking up a parametrized function](#looking-up-a-parametrized-function)\n [Lifecycle hooks for web endpoints](#lifecycle-hooks-for-web-endpoints)",
    "markdown": "* * *\n\nContainer lifecycle hooks and parameters\n========================================\n\nSince Modal [reuses the same container for multiple inputs](/docs/guide/cold-start#keep-containers-warm-for-longer-with-container_idle_timeout)\n, sometimes you might want to run some code exactly once when the container starts or exits. In addition, you might want to pass some parameters to the startup function that do not change between invocations (e.g. the name of a model that’s slow to load).\n\nTo accomplish any of these things, you need to use Modal’s class syntax and the [`@app.cls`](/docs/reference/modal.App#cls)\n decorator. Specifically, you’ll need to:\n\n1.  Convert your function to a method by making it a member of a class.\n2.  Decorate the class with `@app.cls(...)` with same arguments you previously had for `@app.xyz(...)`.\n3.  Instead of the `@app.function` decorator on the original method, use `@method` or the appropriate decorator for a [web endpoint](#lifecycle-hooks-for-web-endpoints)\n    .\n4.  Add the correct method “hooks” to your class based on your need:\n    *   `@enter` for one-time initialization (remote)\n    *   `@exit` for one-time cleanup (remote)\n    *   `@build` to run the function during image build and snapshot the results\n    *   A constructor (`__init__`) for [parametrized container pools](/docs/guide/lifecycle-functions#parametrized-functions)\n        \n\n`@enter`\n--------\n\nThe container entry handler is called when a new container is started. This is useful for doing one-time initialization, such as loading model weights or importing packages that are only present in that image.\n\nTo use, make your function a member of a class, and apply the `@enter()` decorator to one or more class methods:\n\n    from modal import App, enter, method\n    \n    app = App()\n    \n    @app.cls(cpu=8)\n    class Model:\n        @enter()\n        def run_this_on_container_startup(self):\n            self.model = pickle.load(open(\"model.pickle\"))\n    \n        @method()\n        def predict(self, x):\n            return self.model.predict(x)\n    \n    \n    @app.local_entrypoint()\n    def main():\n        Model().predict.remote(x)\n\nCopy\n\nWhen working with an [asynchronous Modal](/docs/guide/async)\n app, you may use an async method instead:\n\n    from modal import App, enter, method\n    \n    app = App()\n    \n    @app.cls(memory=1024)\n    class Processor:\n        @enter()\n        async def my_enter_method(self):\n            self.cache = await load_cache()\n    \n        @method()\n        async def run(self, x):\n            return await do_some_async_stuff(x, self.cache)\n    \n    \n    @app.local_entrypoint()\n    async def main():\n        await Processor().run.remote(x)\n\nCopy\n\nNote: The `@enter()` decorator replaces the earlier `__enter__` syntax, which has been deprecated.\n\n`@exit`\n-------\n\nThe container exit handler is called when a container is about to exit. It is useful for doing one-time cleanup, such as closing a database connection or saving intermediate results. To use, make your function a member of a class, and apply the `@exit()` decorator:\n\n    from modal import App, enter, exit, method\n    \n    app = App()\n    \n    @app.cls()\n    class ETLPipeline:\n        @enter()\n        def open_connection(self):\n            import psycopg2\n            self.connection = psycopg2.connect(os.environ[\"DATABASE_URI\"])\n    \n        @method()\n        def run(self):\n            # Run some queries\n            pass\n    \n        @exit()\n        def close_connection(self):\n            self.connection.close()\n    \n    \n    @app.local_entrypoint()\n    def main():\n        ETLPipeline().run.remote()\n\nCopy\n\nNote that the exit handler is given a grace period of 30 seconds to exit, and it will be killed if it takes longer than that to complete.\n\nNote: The `@exit()` decorator replaces the earlier `__exit__` syntax, which has been deprecated. Like `__exit__`, the method decorated by `@exit` previously needed to accept arguments containing exception information, but this is no longer supported.\n\n`@build`\n--------\n\nThe `@build()` decorator lets us define code that runs as a part of building the container image. This might be useful for downloading model weights and storing it as a part of the image:\n\n    from modal import App, build, enter, method\n    \n    app = App()\n    \n    @app.cls()\n    class Model:\n        @build()\n        def download_model(self):\n            download_model_to_disk()\n    \n        @enter()\n        def load_model(self):\n            load_model_from_disk()\n    \n        @method()\n        def predict(self, x):\n            ...\n\nCopy\n\nThe `@build` and `@enter` decorators can be stacked. This can be useful with tools like `tranformers` which lets you download model weights over the network but caches the weights locally. By making the initialization method run during image build, we make sure the model weights are cached in the image, which makes containers start faster.\n\n    from modal import App, build, enter, method\n    \n    app = App()\n    \n    @app.cls()\n    class Model:\n        @build()\n        @enter()\n        def load_model(self):\n            load_model_from_network(local_cache_dir=\"/\")\n    \n        @method()\n        def predict(self, x):\n            ...\n\nCopy\n\nParametrized functions\n----------------------\n\nImagine this scenario: you want to run different variants of a model based on some argument (say the size of the model), but still share the same code for all of these variants.\n\nIn other words, instead of defining a single Modal function, you want to define a family of functions parametrized by a set of arguments.\n\nTo do this, you can define an `__init__` (constructor) method on your class that accepts some arguments and performs the necessary initialization:\n\n    from modal import App, method\n    \n    app = App()\n    \n    @app.cls(gpu=\"A100\")\n    class Model():\n        def __init__(self, model_name: str, size: int) -> None:\n            self.model = load_model(model_name, size)\n    \n        @method()\n        def generate(self):\n            self.model.generate(...)\n\nCopy\n\nThen, you can construct a remote object with the desired parameters, and call the method on it:\n\n    @app.local_entrypoint()\n    def main():\n        m1 = Model(\"hedgehog\", size=7)\n        m1.generate.remote()\n    \n        m2 = Model(\"fox\", size=13)\n        m2.generate.remote()\n\nCopy\n\nEach variant of the model will behave like an independent Modal function. In addition, each pool is uniquely identified by a hash of the parameters. This means that if you constructed a `Model` with the same parameters in a different context, the calls to `generate` would be routed to the same set of containers as before.\n\nNote that any method annotated with `@enter` will still run remotely after `__init__`.\n\nArguments to `__init__` have a maximum size limit of 16 KiB.\n\n### Looking up a parametrized function\n\nIf you want to call your parametrized function from a Python script running anywhere, you can use `Cls.lookup`:\n\n    from modal import Cls\n    \n    Model = Cls.lookup(\"cls-app\", \"Model\")  # returns a class-like object\n    m = Model(\"snake\", size=12)\n    m.generate.remote()\n\nCopy\n\nWeb endpoints for parametrized functions is not supported at this point.\n\nLifecycle hooks for web endpoints\n---------------------------------\n\nModal `@function`s that are [web endpoints](/docs/guide/webhooks)\n can be converted to the class syntax as well. Instead of `@modal.method`, simply use whichever of the web endpoint decorators (`@modal.web_endpoint`, `@modal.asgi_app` or `@modal.wsgi_app`) you were using before.\n\n    from fastapi import Request\n    from modal import App, enter, web_endpoint\n    \n    app = App(\"web-endpoint-cls\")\n    \n    @app.cls()\n    class Model:\n        @enter()\n        def run_this_on_container_startup(self):\n            self.model = pickle.load(open(\"model.pickle\"))\n    \n        @web_endpoint()\n        def predict(self, request: Request):\n            ...\n\nCopy\n\n[Container lifecycle hooks and parameters](#container-lifecycle-hooks-and-parameters)\n [@enter](#enter)\n [@exit](#exit)\n [@build](#build)\n [Parametrized functions](#parametrized-functions)\n [Looking up a parametrized function](#looking-up-a-parametrized-function)\n [Lifecycle hooks for web endpoints](#lifecycle-hooks-for-web-endpoints)",
    "metadata": {
      "title": "Container lifecycle hooks and parameters | Modal Docs",
      "description": "Since Modal reuses the same container for multiple inputs, sometimes you might want to run some code exactly once when the container starts or exits. In addition, you might want to pass some parameters to the startup function that do not change between invocations (e.g. the name of a model that’s slow to load).",
      "ogTitle": "Container lifecycle hooks and parameters",
      "ogDescription": "Since Modal reuses the same container for multiple inputs, sometimes you might want to run some code exactly once when the container starts or exits. In addition, you might want to pass some parameters to the startup function that do not change between invocations (e.g. the name of a model that’s slow to load).",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/lifecycle-functions",
      "pageStatusCode": 200
    }
  },
  {
    "content": "* * *\n\nS3 Gateway endpoints\n====================\n\nWhen running workloads in AWS, our system automatically uses a corresponding [S3 Gateway endpoint](https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html)\n to ensure low costs, optimal performance, and network reliability between Modal and S3.\n\nWorkloads running on Modal should not incur egress or ingress fees associated with S3 operations. No configuration is needed in order for your app to use S3 Gateway endpoints. S3 Gateway endpoints are automatically used when your app runs on AWS.\n\nEndpoint configuration\n----------------------\n\nOnly use the region-specific endpoint (`s3.<region>.amazonaws.com`) or the global AWS endpoint (`s3.amazonaws.com`). Using an S3 endpoint from one region in another **will not use the S3 Gateway Endpoint incurring networking costs**.\n\nAvoid specifying regional endpoints manually, as this can lead to unexpected cost or performance degradation.\n\nInter-region costs\n------------------\n\nS3 Gateway endpoints guarantee no costs for network traffic within the same AWS region. However, if your Modal Function runs in one region but your bucket resides in a different region you will be billed for inter-region traffic.\n\nYou can prevent this by scheduling your Modal App in the same region of your S3 bucket with [Region selection](https://modal.com/docs/guide/region-selection#region-selection)\n.\n\n[S3 Gateway endpoints](#s3-gateway-endpoints)\n [Endpoint configuration](#endpoint-configuration)\n [Inter-region costs](#inter-region-costs)",
    "markdown": "* * *\n\nS3 Gateway endpoints\n====================\n\nWhen running workloads in AWS, our system automatically uses a corresponding [S3 Gateway endpoint](https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html)\n to ensure low costs, optimal performance, and network reliability between Modal and S3.\n\nWorkloads running on Modal should not incur egress or ingress fees associated with S3 operations. No configuration is needed in order for your app to use S3 Gateway endpoints. S3 Gateway endpoints are automatically used when your app runs on AWS.\n\nEndpoint configuration\n----------------------\n\nOnly use the region-specific endpoint (`s3.<region>.amazonaws.com`) or the global AWS endpoint (`s3.amazonaws.com`). Using an S3 endpoint from one region in another **will not use the S3 Gateway Endpoint incurring networking costs**.\n\nAvoid specifying regional endpoints manually, as this can lead to unexpected cost or performance degradation.\n\nInter-region costs\n------------------\n\nS3 Gateway endpoints guarantee no costs for network traffic within the same AWS region. However, if your Modal Function runs in one region but your bucket resides in a different region you will be billed for inter-region traffic.\n\nYou can prevent this by scheduling your Modal App in the same region of your S3 bucket with [Region selection](https://modal.com/docs/guide/region-selection#region-selection)\n.\n\n[S3 Gateway endpoints](#s3-gateway-endpoints)\n [Endpoint configuration](#endpoint-configuration)\n [Inter-region costs](#inter-region-costs)",
    "metadata": {
      "title": "S3 Gateway endpoints | Modal Docs",
      "description": "When running workloads in AWS, our system automatically uses a corresponding S3 Gateway endpoint to ensure low costs, optimal performance, and network reliability between Modal and S3.",
      "ogTitle": "S3 Gateway endpoints",
      "ogDescription": "When running workloads in AWS, our system automatically uses a corresponding S3 Gateway endpoint to ensure low costs, optimal performance, and network reliability between Modal and S3.",
      "ogImage": "https://modal.com/assets/social-image.jpg",
      "ogLocaleAlternate": [],
      "ogSiteName": "Modal",
      "sourceURL": "https://modal.com/docs/guide/s3-gateway-endpoints",
      "pageStatusCode": 200
    }
  }
]